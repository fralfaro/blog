[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English Espa√±ol\n\n\n\nHi üëã My name is Francisco!\n\n\n\n\nProfession: üìä Mathematical Engineer\nCurrent Work:\n\nüíª Head of Advanced Analytics (Grupo Security)\nüìñ Associate Lecturer (UTFSM)\n\n\n\nInterests\nüéÆ Gaming | üèÄ Basketball | üí° Learning\n‚úÖ Software Development  ‚úÖ Statistical Modelling, Time Series  ‚úÖ Machine/Deep Learning  ‚úÖ Cloud computing, Big Data"
  },
  {
    "objectID": "about-es.html",
    "href": "about-es.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English Espa√±ol\n\n\n\nHolaüëã Mi nombre es Francisco!\n\n\n\n\nProfesi√≥n: üìä Ingeniero Matem√°tico\nTrabajo Actual:\n\nüíª Jefe de Anal√≠tica Avanzada (Grupo Security)\nüìñ Profesor Asociado (UTFSM)\n\n\n\nIntereses\nüéÆ Videojuegos | üèÄ Baloncesto | üí° Aprendizaje\n‚úÖ Desarrollo de Software  ‚úÖ Modelado Estad√≠stico, Series Temporales  ‚úÖ Aprendizaje Autom√°tico/Profundo  ‚úÖ Computaci√≥n en la Nube, Big Data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Compartir datos\n\n\nC√≥mo Compartir Datos de Manera Efectiva (desde el punto de vista estad√≠stico) y no morir en el intento.\n\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n\n\n26 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nDocumentaci√≥n\n\n\nEnteneder los pasos para crear una buena documentaci√≥n en Python (m√°s algunas recomendaciones).\n\n\n\n\npython\n\n\ndocs\n\n\n\n\n\n\n\n\n\n\n\n7 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nGitlab PDF\n\n\nC√≥mo aprovechar GitLab CI/CD para generar archivos PDF utilizando los artefactos de un Pipeline.\n\n\n\n\nci-cd\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n\n\n1 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nCausal Impact\n\n\nCausalImpact creado por Google estima el impacto de una intervenci√≥n en una serie temporal.\n\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\n15 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nCollaborative Filtering\n\n\nUser-based collaborative filtering para realizar un mejor sistema de recomendaci√≥n de pel√≠culas.\n\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\n12 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nTest Driven Development\n\n\nC√≥mo abordar el desarrollo de software para Data Science usando Test Driven Development.\n\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nPolars\n\n\nPolars es una librer√≠a de DataFrames incre√≠blemente r√°pida y eficiente implementada en Rust.\n\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nImpact on Digital Learning\n\n\nCompetition Solution: LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\n\n\n\n\nkaggle\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n\n\n31 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nFastpages\n\n\nFastpages es una plataforma que te permite crear y alojar un blog con Jupyter Notebooks.\n\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n\n\n20 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nBuenas Pr√°cticas - Python\n\n\nConsejos que te ayudar√°n a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n\n\n15 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Book\n\n\nJupyter Book es una herramienta para crear documentos mediante Jupyter Notebooks y/o Markdown.\n\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n\n\n11 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nRISE\n\n\nRISE es una extensi√≥n a los Jupyter Notebooks que permite transformar tus notebooks en presentaciones interactivas.\n\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n\n\n5 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Noteboook\n\n\nJupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar c√≥digo en Python.\n\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n\n\n31 jul 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/2023/data_sharing.html",
    "href": "posts/2023/data_sharing.html",
    "title": "Compartir datos",
    "section": "",
    "text": "Esta gu√≠a est√° dise√±ada para todos aquellos que necesitan colaborar con un estad√≠stico o cient√≠fico de datos en el proceso de an√°lisis de datos. Nuestro p√∫blico objetivo abarca:\n\nColaboradores que requieren an√°lisis estad√≠sticos o de datos para sus proyectos.\nEstudiantes o posdoctorados en diversas disciplinas en busca de asesoramiento y consultor√≠a.\nEstudiantes de estad√≠stica junior que desempe√±an un papel crucial en la recopilaci√≥n, limpieza y preparaci√≥n de conjuntos de datos.\n\nEl prop√≥sito de esta gu√≠a es ofrecer pautas y recomendaciones para compartir datos de manera eficiente, evitando errores comunes y retrasos en la transici√≥n desde la recopilaci√≥n de datos hasta su an√°lisis.\nSostenemos firmemente la idea de que los estad√≠sticos deben ser capaces de trabajar con los datos en cualquier estado en el que se encuentren.\nEs esencial examinar los datos en su estado crudo, comprender los pasos involucrados en su procesamiento y poder identificar fuentes ocultas de variabilidad en el an√°lisis de datos. Sin embargo, para muchos tipos de datos, los pasos de procesamiento est√°n bien documentados y estandarizados. Por lo tanto, la labor de transformar los datos desde su forma original a una forma directamente analizable puede realizarse antes de involucrar a un estad√≠stico.\nEsta pr√°ctica puede acelerar significativamente el tiempo de respuesta, ya que el estad√≠stico no tendr√° que ocuparse de todos los pasos de preprocesamiento inicialmente.\n\n\nSi deseas asegurar un an√°lisis eficiente y oportuno, es fundamental proporcionar al estad√≠stico la siguiente informaci√≥n:\n\nDatos en su Estado Original (raw data): Esto incluye los datos sin procesar, tal como se recopilaron, sin ning√∫n tipo de transformaci√≥n o manipulaci√≥n. Al brindar los datos en su forma bruta, permites al estad√≠stico comprender la fuente y la calidad de la informaci√≥n.\nConjunto de Datos Ordenado (Tidy Data Set): Un conjunto de datos ordenado sigue los principios del ‚ÄúTidy Data‚Äù, lo que significa que se organiza de manera que cada variable se encuentre en una columna y cada observaci√≥n en una fila. Esta estructura facilita el an√°lisis y la interpretaci√≥n de los datos de manera efectiva.\nDetalles del Conjunto de Datos: Se debe describir minuciosamente cada variable presente en el conjunto de datos ordenado, incluyendo informaci√≥n sobre los valores que pueden tomar. Proporcionar un libro de c√≥digos es esencial para que el estad√≠stico comprenda la naturaleza de las variables y c√≥mo interpretarlas correctamente.\nLista de Instrucciones (script): Es crucial suministrar una descripci√≥n detallada de los pasos exactos que seguiste para transformar los datos desde su estado original hasta el conjunto de datos ordenado. Esto garantiza la reproducibilidad y una comprensi√≥n completa de tu proceso por parte del estad√≠stico.\n\n\n\nUno de los elementos fundamentales en cualquier an√°lisis de datos es la inclusi√≥n de los datos en su forma m√°s ‚Äúcruda‚Äù y original. Esto garantiza que la procedencia de los datos se mantenga intacta a lo largo de todo el proceso de trabajo. Aqu√≠ te proporcionamos ejemplos de lo que se entiende por datos en su estado m√°s ‚Äúcrudo‚Äù:\n\nUn enigm√°tico archivo binario generado por tu m√°quina de medici√≥n.\nUn archivo de Excel sin procesar con 10 hojas de trabajo que has recibido de la empresa con la que colaboras.\nComplejos datos en formato JSON que has extra√≠do al hacer web scraping de la API de Twitter.\nN√∫meros ingresados manualmente mientras observabas a trav√©s de un microscopio.\n\nPuedes considerar que los datos est√°n en su formato crudo si:\n\nNo se ha aplicado ning√∫n software a los datos.\nLos valores de los datos no han sido alterados.\nNo se ha eliminado ninguna parte de los datos del conjunto.\nLos datos no han sido resumidos o transformados de ninguna manera.\n\nSi realizaste alguna modificaci√≥n en los datos en bruto, estos ya no se consideran en su forma cruda. Reportar datos modificados como datos en bruto es una pr√°ctica com√∫n que puede ralentizar significativamente el proceso de an√°lisis, ya que el analista a menudo debe realizar un examen minucioso de tus datos para comprender por qu√© los datos en su forma cruda parecen inusuales. (Imagina tambi√©n c√≥mo impactar√≠a en futuros an√°lisis si llegan nuevos datos). Mantener la integridad de los datos en su forma cruda es esencial para un an√°lisis s√≥lido y confiable.\nClaro, te proporcionar√© un ejemplo pr√°ctico y aplicado en Python que ilustra los principios de datos ordenados (tidy data) de Hadley Wickham. En este ejemplo, trabajaremos con un conjunto de datos de resultados de ex√°menes m√©dicos para varios pacientes.\nEjemplo Pr√°ctico\nSupongamos que tenemos un conjunto de datos de resultados de ex√°menes m√©dicos para tres pacientes en una cl√≠nica. Los ex√°menes incluyen mediciones de presi√≥n arterial (sist√≥lica y diast√≥lica), nivel de glucosa y nivel de colesterol. El conjunto de datos se ve de la siguiente manera:\n\n\n\n\n\n\n\n\n\n\nPaciente\nPresion_Sistolica\nPresion_Diastolica\nGlucosa\nColesterol\n\n\n\n\n1\n120\n80\n95\n180\n\n\n2\n130\n85\n105\n190\n\n\n3\n115\n75\n90\n170\n\n\n\nEstos datos est√°n sin procesar, tal como se recopilar√≠an de los pacientes.\n\n\n\nLos principios generales de los datos ordenados (tidy data) son presentados por Hadley Wickham en este art√≠culo y este video. Aunque tanto el art√≠culo como el video describen los datos ordenados utilizando R, los principios son aplicables de manera m√°s general:\n\nCada variable que mides debe estar en una columna.\nCada observaci√≥n diferente de esa variable debe estar en una fila diferente.\nDebe haber una tabla para cada ‚Äútipo‚Äù de variable.\nSi tienes varias tablas, deben incluir una columna en la tabla que permita unirlas o combinarlas.\n\nLa idea detr√°s de los datos ordenados es que los datos se organicen de manera que sea f√°cil identificar y seleccionar variables, as√≠ como realizar an√°lisis de manera eficiente. Siguiendo estos principios, se logra una estructura de datos que es intuitiva y facilita el trabajo de los analistas y cient√≠ficos de datos, ya que no necesitan descifrar la estructura de los datos en cada proyecto. Tambi√©n se promueve el uso de nombres descriptivos para las variables, lo que mejora la comprensi√≥n de los datos por parte de otros colaboradores.\nEjemplo Pr√°ctico\nContinuando con el ejemplo, tenemos cuatro variables: Paciente, Presion_Sistolica, Presion_Diastolica, Glucosa y Colesterol. As√≠ que, creamos un conjunto de datos ordenado donde cada una de estas variables tiene su propia columna:\n\n\n\nPaciente\nVariable\nValor\n\n\n\n\n1\nPresion_Sistolica\n120\n\n\n1\nPresion_Diastolica\n80\n\n\n1\nGlucosa\n95\n\n\n1\nColesterol\n180\n\n\n2\nPresion_Sistolica\n130\n\n\n2\nPresion_Diastolica\n85\n\n\n2\nGlucosa\n105\n\n\n2\nColesterol\n190\n\n\n3\nPresion_Sistolica\n115\n\n\n3\nPresion_Diastolica\n75\n\n\n3\nGlucosa\n90\n\n\n3\nColesterol\n170\n\n\n\nEsto es un conjunto de datos ordenado porque cumple con las reglas mencionadas anteriormente.\n\n\n\nPara casi cualquier conjunto de datos, las mediciones que calculas necesitar√°n ser descritas con m√°s detalle de lo que puedes o debes incluir en la hoja de c√°lculo. El libro de c√≥digos contiene esta informaci√≥n. Como m√≠nimo, deber√≠a contener:\n\nInformaci√≥n sobre las variables (¬°incluyendo unidades!) en el conjunto de datos que no est√°n contenidas en los datos ordenados (tidy data).\nInformaci√≥n sobre las elecciones de resumen que hiciste.\nInformaci√≥n sobre el dise√±o experimental que utilizaste.\n\nEjemplo Pr√°ctico\nInformaci√≥n sobre las Variables:\nEn nuestro ejemplo de resultados de ex√°menes m√©dicos, tenemos los detalles adicionales sobre las variables:\n\nPresion_Sistolica: Presi√≥n arterial sist√≥lica medida en mmHg (mil√≠metros de mercurio).\nPresion_Diastolica: Presi√≥n arterial diast√≥lica medida en mmHg.\nGlucosa: Nivel de glucosa en sangre medido en mg/dL (miligramos por decilitro).\nColesterol: Nivel de colesterol en sangre medido en mg/dL.\n\nEstos detalles son importantes para que otros comprendan las unidades de medida y la naturaleza de las variables.\nInformaci√≥n sobre las Elecciones de Resumen:\nSi realizaste alguna operaci√≥n de resumen en los datos, como el c√°lculo de promedios o medianas, debes explicar estas elecciones en el libro de c√≥digos. Por ejemplo, si calculaste el promedio de la presi√≥n arterial sist√≥lica para resumir los datos, debes mencionarlo y proporcionar la f√≥rmula utilizada.\n\nPromedio de la presi√≥n sist√≥lica: Se calcul√≥ como la media de todas las mediciones de presi√≥n sist√≥lica en el conjunto de datos.\n\nPromedio de la presi√≥n sist√≥lica = (Suma de todas las mediciones de presi√≥n sist√≥lica) / (N√∫mero de pacientes)\n\nMediana de la presi√≥n diast√≥lica: Se calcul√≥ como la mediana de todas las mediciones de presi√≥n diast√≥lica en el conjunto de datos.\n\nMediana de la presi√≥n diast√≥lica = Valor central cuando las mediciones se ordenan de menor a mayor\n\n\nInformaci√≥n sobre el Dise√±o Experimental:\nSe debe incluir detalles sobre c√≥mo se recopilaron los datos y el dise√±o experimental subyacente. Por ejemplo, si los datos se recopilaron de pacientes en un estudio cl√≠nico, debes describir c√≥mo se seleccionaron los pacientes, si hubo aleatorizaci√≥n en los tratamientos, y cualquier otro detalle relevante sobre el dise√±o del estudio.\n\nSelecci√≥n de Pacientes: Los pacientes fueron seleccionados de un centro de atenci√≥n m√©dica en el que se atienden personas con afecciones cardiovasculares. La selecci√≥n de pacientes se realiz√≥ de manera no aleatoria, y se incluyeron aquellos que cumplieron con los siguientes criterios de inclusi√≥n: diagn√≥stico de enfermedad cardiovascular confirmado, disponibilidad de datos de presi√≥n arterial, niveles de glucosa y colesterol, y consentimiento para participar en el estudio.\nAleatorizaci√≥n en Tratamientos: En este estudio, no se aplic√≥ aleatorizaci√≥n en los tratamientos. Los pacientes recibieron el tratamiento m√©dico habitual seg√∫n las pautas cl√≠nicas establecidas por sus m√©dicos tratantes. Por lo tanto, no se realizaron intervenciones experimentales ni asignaciones aleatorias de tratamientos.\nRecopilaci√≥n de Datos: Los datos se recopilaron mediante la revisi√≥n de los registros m√©dicos electr√≥nicos de los pacientes. Se registraron las mediciones de presi√≥n arterial sist√≥lica y diast√≥lica, los niveles de glucosa en sangre y los niveles de colesterol en momentos espec√≠ficos de las consultas m√©dicas de los pacientes.\n\n\n\n\nEs posible que hayas escuchado esto antes, pero la reproducibilidad es un gran tema en la ciencia computacional. Esto significa que cuando env√≠es tu art√≠culo, los revisores y el resto del mundo deber√≠an poder replicar exactamente los an√°lisis desde los datos crudos hasta los resultados finales. Si est√°s tratando de ser eficiente, es probable que realices algunas etapas de resumen/an√°lisis de datos antes de que los datos se consideren ordenados.\nLo ideal que debes hacer al realizar el resumen es crear un script de computadora (en R, Python, u otro lenguaje) que tome los datos crudos como entrada y produzca los datos ordenados que est√°s compartiendo como salida. Puedes intentar ejecutar tu script algunas veces y ver si produce la misma salida.\nEn muchos casos, la persona que recopil√≥ los datos tiene incentivos para que sean ordenados para un estad√≠stico y acelerar el proceso de colaboraci√≥n. Es posible que no sepa c√≥mo programar en un lenguaje de script. En ese caso, lo que debes proporcionar al estad√≠stico es algo llamado pseudoc√≥digo. Deber√≠a verse algo como:\n\nPaso 1 - tomar el archivo crudo, ejecutar la versi√≥n 3.1.2 del software de resumen con par√°metros a=1, b=2, c=3\nPaso 2 - ejecutar el software por separado para cada muestra\nPaso 3 - tomar la tercera columna del archivo de salida para cada muestra y esa es la fila correspondiente en el conjunto de datos de salida\n\nTambi√©n debes incluir informaci√≥n sobre qu√© sistema (Mac/Windows/Linux) utilizaste para el software y si lo intentaste m√°s de una vez para confirmar que daba los mismos resultados. Idealmente, deber√≠as hacer que otro estudiante o compa√±ero de laboratorio confirme que puede obtener el mismo archivo de salida que t√∫.\nEjemplo Pr√°ctico\nEste c√≥digo se ejecut√≥ en Python 3.8.5 y utiliza las bibliotecas pandas y IPython.display. Las dependencias y la versi√≥n de Python est√°n indicadas en los comentarios para mayor claridad.\n# Importar las bibliotecas necesarias\nimport pandas as pd\nfrom IPython.display import display\n\n# Versi√≥n de Python utilizada: 3.8.5\n\n# Crear un DataFrame con los datos originales\ndata = {\n    'Paciente': [1, 2, 3],\n    'Presion_Sistolica': [120, 130, 115],\n    'Presion_Diastolica': [80, 85, 75],\n    'Glucosa': [95, 105, 90],\n    'Colesterol': [180, 190, 170]\n}\n\ndf_original = pd.DataFrame(data)\n\n# Crear un nuevo DataFrame en el formato deseado (unstack)\ndf_unstacked = df_original.set_index('Paciente').stack().reset_index()\ndf_unstacked.columns = ['Paciente', 'Variable', 'Valor']\n\n# Calcular diferentes estad√≠sticas\nestadisticas = df_unstacked.groupby('Variable')['Valor'].describe()\n\n# Mostrar el DataFrame en el formato antiguo   \nprint(\"Mostrar el DataFrame en el formato antiguo:\")\ndisplay(df_original)\n\n# Mostrar el DataFrame en el formato nuevo   \nprint(\"\\nMostrar el DataFrame en el formato nuevo:\")\ndisplay(df_unstacked)\n\n# Mostrar las estad√≠sticas b√°sicas\nprint(\"\\nMostrar las estad√≠sticas b√°sicas:\")\ndisplay(estadisticas)\n\n\n\nCuando entregas un conjunto de datos debidamente ordenados, disminuye dr√°sticamente la carga de trabajo del estad√≠stico. Entonces, con suerte, te responder√°n mucho m√°s r√°pido. Pero la mayor√≠a de los estad√≠sticos cuidadosos verificar√°n tu procedimiento, har√°n preguntas sobre las etapas que realizaste y tratar√°n de confirmar que pueden obtener los mismos datos ordenados que t√∫, al menos con verificaciones puntuales.\nDeber√≠as esperar del estad√≠stico:\n\nUn script de an√°lisis que realice cada uno de los an√°lisis (no solo instrucciones).\nEl c√≥digo de computadora exacto que utilizaron para ejecutar el an√°lisis.\nTodos los archivos de salida/figuras que generaron.\n\nEsta es la informaci√≥n que utilizar√°s para establecer la reproducibilidad y la precisi√≥n de tus resultados. Cada uno de los pasos en el an√°lisis debe estar claramente explicado y debes hacer preguntas cuando no entiendas lo que hizo el analista. Es responsabilidad tanto del estad√≠stico como del cient√≠fico comprender el an√°lisis estad√≠stico.\nEs posible que no puedas realizar los an√°lisis exactos sin el c√≥digo del estad√≠stico, pero deber√≠as poder explicar por qu√© el estad√≠stico realiz√≥ cada paso a un compa√±ero de laboratorio o a tu investigador principal."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html",
    "href": "posts/2023/gitlab_pdf.html",
    "title": "Gitlab PDF",
    "section": "",
    "text": "GitLab CI/CD es una potente herramienta que permite automatizar y gestionar el ciclo de vida de las aplicaciones de software.\nCI (Integraci√≥n Continua) y CD (Entrega Continua) son pr√°cticas esenciales en el desarrollo de software moderno que buscan mejorar la calidad del c√≥digo, aumentar la eficiencia y reducir los errores. GitLab CI/CD se integra de manera nativa en el flujo de trabajo de GitLab, lo que lo convierte en una opci√≥n atractiva para equipos de desarrollo.\n\nCI (Integraci√≥n Continua): Es el proceso de integrar cambios de c√≥digo frecuentes en un repositorio compartido. Esto implica la ejecuci√≥n autom√°tica de pruebas y an√°lisis de calidad cada vez que se env√≠a c√≥digo. El objetivo es identificar y corregir problemas de manera temprana en el ciclo de desarrollo.\nCD (Entrega Continua): Una vez que las pruebas de CI se han superado con √©xito, el c√≥digo se considera apto para su implementaci√≥n en entornos de producci√≥n o de pruebas. El objetivo es entregar de manera eficiente y confiable el software a los usuarios finales.\n\n\n\n\nAntes de profundizar en la generaci√≥n de archivos PDF, es importante comprender el concepto de ‚Äúartefactos‚Äù en GitLab CI/CD. Los artefactos son archivos o conjuntos de archivos generados como resultado de una ejecuci√≥n exitosa de un pipeline.\nEstos artefactos se almacenan en GitLab y se pueden utilizar posteriormente en otros trabajos o pipelines.\nEn el contexto de la generaci√≥n de archivos PDF, los artefactos son esenciales porque permiten que los archivos PDF generados en un trabajo se conserven y utilicen en otros trabajos o etapas del pipeline.\n\n\n\n\nLa generaci√≥n de archivos PDF como parte de su proceso de CI/CD puede ser √∫til en varios escenarios, como la creaci√≥n de informes automatizados, la generaci√≥n de documentaci√≥n t√©cnica o la producci√≥n de facturas en l√≠nea.\nA continuaci√≥n, detallaremos c√≥mo lograrlo utilizando GitLab CI/CD:\n\nNota: Tomaremos como referencia el siguiente repositorio.\n\n\n\nAntes de comenzar, aseg√∫rese de que su proyecto de GitLab est√© configurado correctamente y tenga acceso a GitLab CI/CD. Tambi√©n debe tener un archivo de c√≥digo fuente que desee convertir en un archivo PDF.\nAseg√∫rese de que cualquier dependencia necesaria est√© especificada en su archivo de configuraci√≥n de CI/CD.\n\n\n\n\nCree un script que sea capaz de generar el archivo PDF a partir de sus datos de entrada.\nEste script deber√≠a tomar los datos relevantes y formatearlos en un archivo PDF.\n\n\n\n\nEn su repositorio de GitLab, cree un archivo llamado .gitlab-ci.yml si a√∫n no lo ha hecho.\nEste archivo contiene la configuraci√≥n de su pipeline.\nAqu√≠ hay un ejemplo de c√≥mo podr√≠a verse:\nstages:\n  - pdf\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\n  artifacts:\n    paths:\n      - ./*.pdf\nEn este ejemplo:\nstages:\n  - pdf\n\nstages: Esta secci√≥n define las etapas (stages). En este caso, solo se define una etapa llamada ‚Äúpdf‚Äù. Las etapas son divisiones l√≥gicas en el pipeline que agrupan trabajos relacionados. En este caso, el pipeline tiene una sola etapa llamada ‚Äúpdf‚Äù.\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\ngenerate_pdf: Esta secci√≥n define un trabajo (job) llamado ‚Äúgenerate_pdf‚Äù. Un trabajo es una unidad de ejecuci√≥n en el pipeline. Aqu√≠ est√° el desglose de esta secci√≥n:\n\nstage: pdf: Esta l√≠nea especifica que este trabajo pertenece a la etapa ‚Äúpdf‚Äù definida previamente. En otras palabras, este trabajo se ejecutar√° en la etapa ‚Äúpdf‚Äù del pipeline.\nimage: aergus/latex: Esta l√≠nea especifica la imagen Docker que se utilizar√° para ejecutar este trabajo. En este caso, se utiliza la imagen ‚Äúaergus/latex‚Äù, que contiene un entorno LaTeX para compilar documentos PDF. Esta imagen es esencial para compilar archivos LaTeX en archivos PDF.\nscript: Aqu√≠ se definen los comandos que se ejecutar√°n en el trabajo. En este caso, se utiliza el comando ‚Äúlatexmk -pdf **/*.tex‚Äù. Este comando utiliza ‚Äúlatexmk‚Äù para compilar todos los archivos ‚Äú.tex‚Äù en el proyecto en archivos PDF. El uso de **/*.tex significa que buscar√° archivos ‚Äú.tex‚Äù en todos los subdirectorios del proyecto.\n\n\nartifacts:\n  paths:\n    - ./*.pdf\n\nartifacts: Esta secci√≥n especifica qu√© archivos deben considerarse artefactos y, por lo tanto, se conservar√°n despu√©s de una ejecuci√≥n exitosa del trabajo. Aqu√≠ est√° el desglose de esta secci√≥n:\n\npaths: ./*.pdf: Esta l√≠nea especifica que todos los archivos con extensi√≥n ‚Äú.pdf‚Äù en el directorio actual deben considerarse artefactos. Esto significa que los archivos PDF generados como resultado de la ejecuci√≥n de este trabajo se conservar√°n y estar√°n disponibles para su descarga despu√©s de el pipeline se haya ejecutado con √©xito.\n\n\n\n\n\nCada vez que realice un env√≠o de c√≥digo (push) o active manualmente el pipeline, GitLab ejecutar√° el trabajo de generaci√≥n de PDF. El script generar√° el archivo PDF y lo almacenar√° como un artefacto.\n\n\n\n\nUna vez que el pipeline se haya ejecutado con √©xito, puede acceder a los archivos PDF generados en GitLab.\nVaya a la p√°gina de su proyecto en GitLab, seleccione ‚ÄúCI/CD‚Äù y luego ‚ÄúArtefactos‚Äù.\nAqu√≠ encontrar√° el archivo PDF generado que puede descargar.\n\n\n\n\nGitLab CI/CD es una herramienta poderosa que puede ayudar en la automatizaci√≥n de una amplia variedad de tareas, incluida la generaci√≥n de archivos PDF.\nAl comprender c√≥mo utilizar artefactos en GitLab CI/CD y seguir los pasos mencionados anteriormente, puede integrar f√°cilmente la generaci√≥n de PDF en su flujo de trabajo de desarrollo, lo que ahorra tiempo y esfuerzo, y garantiza la consistencia y la calidad en la creaci√≥n de documentos PDF automatizados."
  },
  {
    "objectID": "posts/2023/art_docs.html",
    "href": "posts/2023/art_docs.html",
    "title": "Documentaci√≥n",
    "section": "",
    "text": "Esperamos que, si est√°s leyendo este tutorial, ya comprendas la importancia de documentar tu c√≥digo. Pero, por si acaso, perm√≠teme citar algo que Guido mencion√≥ en la reciente PyCon 2016:\n\n\n\n\n\n\nCita\n\n\n\n‚ÄúEl c√≥digo se lee m√°s a menudo de lo que se escribe.‚Äù ‚Äî Guido van Rossum\n\n\nCuando escribes c√≥digo, lo haces para dos audiencias principales: tus usuarios y tus desarrolladores (incluy√©ndote a ti mismo). Ambas audiencias son igualmente cruciales. Si eres como yo, es posible que hayas abierto antiguas bases de c√≥digo y te hayas preguntado: ‚Äú¬øEn qu√© estaba pensando?‚Äù. Si tienes dificultades para entender tu propio c√≥digo, imagina lo que tus usuarios u otros desarrolladores sienten cuando intentan utilizarlo o contribuir a tu c√≥digo.\nPor otro lado, es probable que hayas pasado por situaciones en las que deseabas realizar algo en Python y encontraste lo que parec√≠a ser una excelente biblioteca que podr√≠a hacer el trabajo. Sin embargo, al comenzar a usar la biblioteca, buscaste ejemplos, descripciones o incluso documentaci√≥n oficial sobre c√≥mo realizar una tarea espec√≠fica y no pudiste encontrar una soluci√≥n de inmediato.\nDespu√©s de buscar durante un tiempo, te das cuenta de que la documentaci√≥n es insuficiente o, peor a√∫n, est√° completamente ausente. Esta es una experiencia frustrante que te impide utilizar la biblioteca, sin importar cu√°n bueno o eficiente sea el c√≥digo. Daniele Procida resumi√≥ esta situaci√≥n de manera acertada:\n!!! quote ‚ÄúNo importa cu√°n bueno sea tu software, porque si la documentaci√≥n no es lo suficientemente buena, la gente no lo usar√°.‚Äù ‚Äî Daniele Procida\nEn esta gu√≠a, aprender√°s desde cero c√≥mo documentar adecuadamente tu c√≥digo en Python, desde los scripts m√°s peque√±os hasta los proyectos m√°s grandes de Python, para evitar que tus usuarios se sientan frustrados al usar o contribuir a tu proyecto.\n\n\n\nAntes de sumergirnos en el arte de documentar tu c√≥digo en Python, es crucial establecer una distinci√≥n fundamental: los comentarios y la documentaci√≥n desempe√±an roles distintos y est√°n dirigidos a audiencias diferentes.\nComentarios:\nEn t√©rminos generales, los comentarios est√°n dise√±ados para proporcionar informaci√≥n sobre tu c√≥digo a los desarrolladores.\nLa audiencia principal a la que se dirigen son aquellos que mantienen y trabajan en el c√≥digo Python. Cuando se combinan con un c√≥digo bien escrito, los comentarios act√∫an como gu√≠as que ayudan a los lectores a comprender mejor el c√≥digo, su prop√≥sito y su estructura. Esto se alinea perfectamente con la sabia observaci√≥n de Jeff Atwood,\n!!! quote ‚ÄúEl c√≥digo te dice c√≥mo; los comentarios te dicen por qu√©.‚Äù ‚Äî Jeff Atwood\nDocumentaci√≥n del C√≥digo:\nPor otro lado, la documentaci√≥n del c√≥digo se enfoca en describir el uso y la funcionalidad del c√≥digo a los usuarios. Aunque puede ser √∫til durante el proceso de desarrollo, su audiencia principal son los usuarios finales del software. La siguiente secci√≥n de este art√≠culo se adentrar√° en cu√°ndo y c√≥mo debes abordar la tarea de comentar tu c√≥digo en Python.\n\n\n\n\nEn Python, los comentarios son esenciales para proporcionar informaci√≥n adicional sobre tu c√≥digo.\nSe crean utilizando el s√≠mbolo de n√∫mero (#) y deben ser declaraciones breves, no m√°s largas que unas pocas frases. Aqu√≠ tienes un ejemplo simple:\ndef hello_world():    \n    # Un comentario simple antes de una simple declaraci√≥n de impresi√≥n\n    print(\"Hola Mundo\")\nDe acuerdo con las pautas de estilo de c√≥digo de Python (PEP 8), los comentarios deben tener una longitud m√°xima de 72 caracteres. Esto es v√°lido incluso si tu proyecto cambia la longitud m√°xima de l√≠nea recomendada para que sea mayor que los 80 caracteres. Si un comentario va a superar el l√≠mite de caracteres recomendado, es apropiado usar m√∫ltiples l√≠neas para el comentario:\ndef hello_long_world():     \n    # Una declaraci√≥n muy larga que sigue y sigue y sigue y sigue y sigue \n    # sin terminar hasta que alcance el l√≠mite de 80 caracteres\n    print(\"¬°Hola Mundoooooooooooooooooooooooooooooooooooooooooooooooooooooo!\")\nComentar tu c√≥digo sirve para varios prop√≥sitos, incluyendo:\n\nPlanificaci√≥n y Revisi√≥n: Durante el desarrollo de nuevas partes de tu c√≥digo, los comentarios pueden servir como una forma de planificar o esquematizar esa secci√≥n. Es importante recordar eliminar estos comentarios una vez que se haya implementado y revisado/testeado el c√≥digo real:\n# Primer paso\n# Segundo paso\n# Tercer paso\nDescripci√≥n del C√≥digo: Los comentarios se utilizan para explicar la intenci√≥n de secciones espec√≠ficas del c√≥digo:\n# Intentar una conexi√≥n basada en configuraciones anteriores. Si no tiene √©xito,\n# solicitar al usuario nuevas configuraciones.\nDescripci√≥n Algor√≠tmica: Al usar algoritmos, especialmente los complicados, es √∫til explicar c√≥mo funcionan o c√≥mo se implementan en tu c√≥digo. Tambi√©n es apropiado describir por qu√© seleccionaste un algoritmo espec√≠fico en lugar de otro:\n# Usar el ordenamiento r√°pido para obtener ganancias de rendimiento.\nEtiquetado: Puedes utilizar etiquetas para se√±alar secciones espec√≠ficas de c√≥digo donde se encuentran problemas conocidos o √°reas de mejora. Algunos ejemplos son BUG, FIXME y TODO:\n# TODO: Agregar condici√≥n para cuando 'val' sea None\n\nLos comentarios en tu c√≥digo deben ser breves y centrados. Evita comentarios largos cuando sea posible. Adem√°s, sigue las siguientes cuatro reglas esenciales sugeridas por Jeff Atwood:\n\nMant√©n los Comentarios Cerca del C√≥digo: Los comentarios deben estar lo m√°s cerca posible del c√≥digo que describen. Los comentarios distantes del c√≥digo descriptivo son frustrantes y pueden pasarse por alto f√°cilmente al realizar actualizaciones.\nEvita el Formato Complejo: No uses formatos complejos como tablas o figuras ASCII. Estos formatos pueden distraer y ser dif√≠ciles de mantener con el tiempo.\nEvita Informaci√≥n Redundante: Sup√≥n que el lector del c√≥digo tiene un entendimiento b√°sico de los principios de programaci√≥n y la sintaxis del lenguaje. No incluyas informaci√≥n redundante.\nDise√±a Tu C√≥digo para que se Comente por S√≠ Mismo: La forma m√°s f√°cil de entender el c√≥digo es ley√©ndolo. Cuando dise√±es tu c√≥digo utilizando conceptos claros y f√°ciles de entender, ayudar√°s al lector a comprender tu intenci√≥n de manera r√°pida y sencilla.\n\nRecuerda que los comentarios est√°n dise√±ados para los lectores, incluy√©ndote a ti mismo, para ayudarlos a comprender el prop√≥sito y dise√±o del software.\n\n\n\nEl Type Hinting es una caracter√≠stica que te permite indicar expl√≠citamente los tipos de datos que esperas en las funciones y m√©todos. Aunque Python es un lenguaje de programaci√≥n de tipado din√°mico, el Type Hinting no cambia esa naturaleza, pero proporciona informaci√≥n adicional a los desarrolladores y a las herramientas de an√°lisis est√°tico sobre c√≥mo deber√≠a funcionar el c√≥digo.\nEl Type Hinting no afecta el comportamiento en tiempo de ejecuci√≥n, por lo que no impide que el c√≥digo funcione si los tipos no coinciden.\nEn cambio, es una herramienta para ayudar a los desarrolladores a comprender y depurar el c√≥digo de manera m√°s eficiente y prevenir posibles errores.\nConsidera la siguiente funci√≥n hello_name:\ndef hello_name(name: str) -> str:\n    return f\"Hello {name}\"\nEn este ejemplo, hemos utilizado Type Hinting para especificar que el par√°metro name debe ser una cadena (str) y que la funci√≥n hello_name debe devolver una cadena (str). Esta informaci√≥n es √∫til para otros desarrolladores que utilicen esta funci√≥n porque ahora saben qu√© tipo de dato esperar como entrada y qu√© tipo de dato obtendr√°n como resultado.\n\n\n\n\n\nUna parte fundamental de la documentaci√≥n en Python son las docstrings, que son cadenas de texto utilizadas para describir funciones, clases, m√≥dulos y m√°s.\n\n\nLas docstrings son cadenas de documentaci√≥n que se encuentran dentro del c√≥digo fuente Python. Estas cadenas proporcionan informaci√≥n sobre el prop√≥sito y el funcionamiento de funciones, clases y otros elementos del c√≥digo.\nLas docstrings son especialmente valiosas para ayudar a los usuarios y desarrolladores a comprender c√≥mo utilizar y trabajar con tu c√≥digo.\n¬øC√≥mo Funcionan las Docstrings?\nCuando definimos una funci√≥n, clase o m√≥dulo en Python, podemos incluir una docstring justo debajo de la definici√≥n. Por ejemplo:\ndef saludar(nombre):\n    \"\"\"Esta funci√≥n imprime un saludo personalizado.\"\"\"\n    print(f\"Hola, {nombre}!\")\nLas docstrings se pueden acceder a trav√©s del atributo __doc__ del objeto. Por ejemplo:\nprint(saludar.__doc__)\nLa salida ser√≠a: ‚ÄúEsta funci√≥n imprime un saludo personalizado.‚Äù Las docstrings tambi√©n se utilizan en entornos de desarrollo interactivo y se muestran al utilizar la funci√≥n help().\nManipulaci√≥n de Docstrings\nEs importante destacar que puedes manipular directamente las docstrings. Sin embargo, existen restricciones para los objetos incorporados en Python. Por ejemplo, no puedes cambiar la docstring de un objeto str incorporado:\nstr.__doc__ = \"¬°Esto no funcionar√° para objetos incorporados!\"\nPero para funciones y objetos personalizados, puedes establecer o modificar sus docstrings de la siguiente manera:\ndef decir_hola(nombre):\n    \"\"\"Una funci√≥n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¬øsoy yo a quien est√°s buscando?\")\n\ndecir_hola.__doc__ = \"Una funci√≥n que saluda estilo Richie Rich.\"\nUbicaci√≥n Estrat√©gica de las Docstrings\nUna forma m√°s sencilla de definir docstrings es colocar una cadena literal justo debajo de la definici√≥n de la funci√≥n o clase. Python autom√°ticamente interpreta esta cadena como la docstring. Por ejemplo:\ndef decir_hola(nombre):\n    \"\"\"Una funci√≥n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, ¬øsoy yo a quien est√°s buscando?\")\n\n\n\nLas docstrings son elementos esenciales para documentar tu c√≥digo Python de manera clara y coherente. Siguen convenciones y pautas que se describen en PEP 257.\nEl prop√≥sito de las docstrings es proporcionar a los usuarios de tu c√≥digo un resumen conciso y √∫til del objeto, como una funci√≥n, clase, m√≥dulo o script. Deben ser lo suficientemente concisas como para ser f√°ciles de mantener, pero lo suficientemente detalladas como para que los nuevos usuarios comprendan su prop√≥sito y c√≥mo utilizar el objeto documentado.\n\n\nTodas las docstrings deben utilizar el formato de triple comilla doble (\"\"\") y deben colocarse justo debajo de la definici√≥n del objeto, ya sea en una sola l√≠nea o en varias l√≠neas:\nUna l√≠nea:\n\"\"\"Esta es una l√≠nea de resumen r√°pida utilizada como descripci√≥n del objeto.\"\"\"\nVarias l√≠neas:\n\"\"\"\nEsta es la l√≠nea de resumen\nEsta es la elaboraci√≥n adicional de la docstring. Dentro de esta secci√≥n, puedes proporcionar m√°s detalles seg√∫n sea apropiado para la situaci√≥n. Observa que el resumen y la elaboraci√≥n est√°n separados por una nueva l√≠nea en blanco.\n\"\"\"\nEs importante destacar que todas las docstrings de varias l√≠neas deben seguir un patr√≥n espec√≠fico:\n\nUna l√≠nea de resumen de una sola l√≠nea.\nUna l√≠nea en blanco despu√©s del resumen.\nCualquier elaboraci√≥n adicional de la docstring.\nOtra l√≠nea en blanco.\n\nAdem√°s, todas las docstrings deben tener una longitud m√°xima de caracteres que sigue las mismas pautas que los comentarios, que es de 72 caracteres.\n\n\n\nLas docstrings de clase se crean para la clase en s√≠, as√≠ como para cualquier m√©todo de clase. Las docstrings se colocan inmediatamente despu√©s de la clase o el m√©todo de clase, con un nivel de sangr√≠a:\nclass ClaseSimple:\n    \"\"\"Aqu√≠ van las docstrings de clase.\"\"\"\n    def decir_hola(self, nombre: str):\n        \"\"\"Aqu√≠ van las docstrings de m√©todo de clase.\"\"\"\n        print(f'Hola {nombre}')\nLas docstrings de clase deben contener la siguiente informaci√≥n:\n\nUn breve resumen de su prop√≥sito y comportamiento.\nCualquier m√©todo p√∫blico, junto con una breve descripci√≥n.\nCualquier propiedad de clase (atributos).\nCualquier cosa relacionada con la interfaz para los subclases, si la clase est√° destinada a ser subclaseada.\n\nLos par√°metros del constructor de clase deben documentarse dentro de la docstring del m√©todo __init__ de la clase. Los m√©todos individuales deben documentarse utilizando sus propias docstrings individuales. Las docstrings de m√©todo de clase deben contener lo siguiente:\n\nUna breve descripci√≥n de lo que hace el m√©todo y para qu√© se utiliza.\nCualquier argumento (tanto requerido como opcional) que se pase, incluidos los argumentos de palabras clave.\nEtiqueta para cualquier argumento que se considere opcional o tenga un valor predeterminado.\nCualquier efecto secundario que ocurra al ejecutar el m√©todo.\nCualquier excepci√≥n que se genere.\nCualquier restricci√≥n sobre cu√°ndo se puede llamar al m√©todo.\n\nEchemos un vistazo a un ejemplo simple de una clase de datos que representa un Animal. Esta clase contendr√° algunas propiedades de clase, propiedades de instancia, un __init__ y un √∫nico m√©todo de instancia:\nclass Animal:\n    \"\"\"Una clase utilizada para representar un Animal\n    \n    Attributes:\n        dice_str (str): una cadena formateada para imprimir lo que dice el animal\n        nombre (str): el nombre del animal\n        sonido (str): el sonido que hace el animal\n        num_patas (int): el n√∫mero de patas del animal (predeterminado 4)\n    \"\"\"\n    \n    dice_str = \"Un {nombre} dice {sonido}\"\n    \n    def __init__(self, nombre, sonido, num_patas=4):\n        \"\"\"Inicializa una nueva instancia de Animal\n        \n        Parameters:\n            nombre (str): El nombre del animal\n            sonido (str): El sonido que hace el animal\n            num_patas (int, opcional): El n√∫mero de patas del animal (predeterminado es 4)\n        \"\"\"\n        self.nombre = nombre\n        self.sonido = sonido\n        self.num_patas = num_patas\n        \n    def dice(self, sonido=None):\n        \"\"\"Imprime el nombre del animal y el sonido que hace.\n        \n        Si no se pasa el argumento `sonido`, se utiliza el sonido predeterminado del Animal.\n        \n        Parameters:\n            sonido (str, opcional): El sonido que hace el animal (predeterminado es None)\n        \n        Raises:\n            NotImplementedError: Si no se establece ning√∫n sonido para el animal o se pasa como par√°metro.\n        \"\"\"\n        if self.sonido is None and sonido is None:\n            raise NotImplementedError(\"¬°No se admiten animales silenciosos!\")\n        sonido_salida = self.sonido if sonido is None else sonido\n        print(self.dice_str.format(nombre=self.nombre, sonido=sonido_salida))\n\n\n\n\n\nExisten formatos espec√≠ficos de docstrings que pueden ser utilizados para ayudar a los analizadores de docstrings y a los usuarios a tener un formato familiar y reconocido.\nAlgunos de los formatos m√°s comunes son los siguientes:\n\n\n\nTipo de Formato\nDescripci√≥n\nCompatible con Sphinx\nEspecificaci√≥n Formal\n\n\n\n\nGoogle docstrings\nForma de documentaci√≥n recomendada por Google\nS√≠\nNo\n\n\nreStructuredText\nEst√°ndar oficial de documentaci√≥n de Python; no es amigable para principiantes pero rico en caracter√≠sticas\nS√≠\nS√≠\n\n\nNumPy/SciPy docstrings\nCombinaci√≥n de reStructuredText y Docstrings de Google utilizada por NumPy\nS√≠\nS√≠\n\n\nEpytext\nUna adaptaci√≥n de Epydoc para Python; ideal para desarrolladores de Java\nNo oficialmente\nS√≠\n\n\n\nLa elecci√≥n del formato de docstring depende de ti, pero debes mantener el mismo formato en todo tu documento o proyecto. A continuaci√≥n, se presentan ejemplos de cada tipo para darte una idea de c√≥mo se ve cada formato de documentaci√≥n.\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de c√°lculo\n\nArgs:\n    file_loc (str): La ubicaci√≥n del archivo de la hoja de c√°lculo.\n    print_cols (bool): Una bandera utilizada para imprimir las columnas en la consola\n        (el valor predeterminado es Falso)\n\nReturns:\n    list: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de c√°lculo\n\n:param file_loc: La ubicaci√≥n del archivo de la hoja de c√°lculo\n:type file_loc: str\n:param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n:type print_cols: bool\n\n:returns: una lista de cadenas que representan las columnas de encabezado\n:rtype: list\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de c√°lculo\n\nParameters\n----------\nfile_loc : str\n    La ubicaci√≥n del archivo de la hoja de c√°lculo\nprint_cols : bool, opcional\n    Una bandera utilizada para imprimir las columnas en la consola (el valor predeterminado es Falso)\n\nReturns\n-------\nlist\n    una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de c√°lculo\n\n@type file_loc: str\n@param file_loc: La ubicaci√≥n del archivo de la hoja de c√°lculo\n@type print_cols: bool\n@param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n@rtype: list\n@returns: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\nEstos ejemplos te proporcionan una idea de c√≥mo se estructuran y formatean las docstrings en diferentes estilos de documentaci√≥n.\nPuedes elegir el que mejor se adapte a tus preferencias y necesidades de documentaci√≥n, pero aseg√∫rate de mantener la coherencia en todo tu proyecto.\n\n\n\n\n\nLos proyectos de Python vienen en todo tipo de formas, tama√±os y prop√≥sitos. La forma en que documentas tu proyecto debe adaptarse a tu situaci√≥n espec√≠fica. Ten en cuenta qui√©nes ser√°n los usuarios de tu proyecto y ad√°ptate a sus necesidades. Dependiendo del tipo de proyecto, se recomiendan ciertos aspectos de la documentaci√≥n. La estructura general del proyecto y su documentaci√≥n debe ser la siguiente:\nproject_root/\n‚îÇ\n‚îú‚îÄ‚îÄ project/  # Project source code\n‚îú‚îÄ‚îÄ docs/\n‚îú‚îÄ‚îÄ README\n‚îú‚îÄ‚îÄ HOW_TO_CONTRIBUTE\n‚îú‚îÄ‚îÄ CODE_OF_CONDUCT\n‚îú‚îÄ‚îÄ examples.py\nEsta estructura de directorios es un dise√±o com√∫n para organizar un proyecto de software en Python. A continuaci√≥n, se explica en detalle cada elemento de esta estructura:\n\nproject_root (Directorio Ra√≠z del Proyecto): Este es el directorio principal que contiene todos los archivos y carpetas relacionados con tu proyecto. Es el punto de partida para tu proyecto.\nproject/ (Carpeta ‚Äúproject‚Äù): Esta carpeta suele contener el c√≥digo fuente principal de tu proyecto. Aqu√≠ se almacenan todos los archivos de Python que forman parte de tu proyecto. Puedes organizar estos archivos en subdirectorios seg√∫n la estructura de tu proyecto. Por ejemplo, puedes tener subdirectorios para m√≥dulos espec√≠ficos o componentes del proyecto.\ndocs/ (Carpeta ‚Äúdocs‚Äù): La carpeta ‚Äúdocs‚Äù se utiliza para almacenar la documentaci√≥n de tu proyecto. Aqu√≠ puedes incluir documentos explicativos, manuales de usuario, instrucciones de instalaci√≥n y cualquier otra documentaci√≥n relevante. Mantener una documentaci√≥n clara y organizada es esencial para que los usuarios comprendan y utilicen tu proyecto de manera efectiva.\nREADME: El archivo ‚ÄúREADME‚Äù es un documento importante que proporciona una breve descripci√≥n de tu proyecto y su prop√≥sito. Suele incluir informaci√≥n sobre c√≥mo instalar y utilizar el proyecto, as√≠ como otros detalles importantes. Los usuarios suelen consultar este archivo primero cuando exploran un proyecto.\nHOW_TO_CONTRIBUTE: Este archivo contiene instrucciones para las personas que deseen contribuir al desarrollo de tu proyecto. Incluye detalles sobre c√≥mo pueden colaborar, enviar correcciones, agregar nuevas funciones y seguir las pautas de contribuci√≥n.\nCODE_OF_CONDUCT: El archivo ‚ÄúCODE_OF_CONDUCT‚Äù establece las reglas y pautas de comportamiento que deben seguir los colaboradores y usuarios del proyecto. Define c√≥mo deben interactuar entre s√≠ de manera respetuosa y profesional. Tambi√©n puede indicar las consecuencias en caso de violaci√≥n del c√≥digo de conducta.\nexamples.py: Este archivo es un script de Python que contiene ejemplos simples de c√≥mo utilizar las funcionalidades de tu proyecto. Estos ejemplos pueden ayudar a los usuarios a comprender c√≥mo utilizar tu c√≥digo en situaciones reales y proporcionar ejemplos de uso pr√°ctico.\n\n\n\n\nLa documentaci√≥n es una parte fundamental de cualquier proyecto de desarrollo de software. Proporciona informaci√≥n crucial sobre c√≥mo utilizar, mantener y contribuir al c√≥digo. En el ecosistema de Python, existen varias bibliotecas y herramientas que facilitan la tarea de documentar el c√≥digo de manera efectiva. En este art√≠culo, exploraremos algunas de las principales bibliotecas de Python utilizadas para documentar c√≥digo.\n\n\n\nSphinx es una de las herramientas de documentaci√≥n m√°s populares en el mundo de Python. Fue originalmente desarrollada para documentar la propia documentaci√≥n de Python y se ha convertido en una elecci√≥n com√∫n para proyectos de c√≥digo abierto y proyectos internos. Algunas de sus caracter√≠sticas clave incluyen:\n\nGeneraci√≥n de documentaci√≥n en varios formatos, incluyendo HTML, PDF, ePub y m√°s.\nUtiliza reStructuredText como su formato de marcado predeterminado, que es altamente estructurado y permite documentar de manera eficiente los aspectos t√©cnicos.\nAmplia gama de extensiones y complementos que permiten personalizar y mejorar la documentaci√≥n.\nAdmite la generaci√≥n autom√°tica de documentaci√≥n a partir de docstrings en el c√≥digo Python.\nEs especialmente adecuado para documentar bibliotecas, API y proyectos t√©cnicos.\n\nSphinx es altamente configurable y puede generar documentaci√≥n de alta calidad y profesional. Sin embargo, puede requerir un tiempo de configuraci√≥n inicial y tiene una curva de aprendizaje empinada para los principiantes.\n\n\n\n\nMkDocs es una herramienta de generaci√≥n de documentaci√≥n que se centra en la simplicidad y la facilidad de uso. Est√° dise√±ada para crear documentaci√≥n de proyectos de una manera simple y r√°pida, principalmente enfocada en la generaci√≥n de sitios web de documentaci√≥n. Algunas de sus caracter√≠sticas clave incluyen:\n\nUtiliza Markdown como formato de marcado predeterminado, que es f√°cil de aprender y escribir.\nOfrece una interfaz de l√≠nea de comandos simple para iniciar y generar sitios de documentaci√≥n.\nProporciona temas y extensiones para personalizar el aspecto y la funcionalidad de la documentaci√≥n generada.\nIdeal para proyectos de c√≥digo abierto y documentaci√≥n de proyectos peque√±os a medianos.\n\nMkDocs es especialmente adecuado para proyectos con necesidades de documentaci√≥n simples. Es f√°cil de aprender y usar, lo que lo convierte en una excelente opci√≥n para principiantes. Sin embargo, puede ser limitado en funcionalidad en comparaci√≥n con Sphinx para proyectos t√©cnicos y complejos.\n\n\n\nMkDocs-Material es un tema personalizado para MkDocs, una popular herramienta de generaci√≥n de sitios web est√°ticos dise√±ada para crear documentaci√≥n de proyectos de manera sencilla y efectiva. Este tema, conocido como ‚ÄúMaterial for MkDocs‚Äù, se inspira en el elegante dise√±o de Material Design de Google y est√° dise√±ado para ofrecer una experiencia de documentaci√≥n moderna y atractiva.\nUna de las principales caracter√≠sticas de MkDocs-Material es su enfoque en la legibilidad y la facilidad de navegaci√≥n. Esto se logra mediante un dise√±o limpio y organizado que hace que la documentaci√≥n sea m√°s accesible para los usuarios. Adem√°s, el tema proporciona herramientas √∫tiles para mejorar la experiencia de los lectores, como una funci√≥n de b√∫squeda integrada que permite a los usuarios encontrar r√°pidamente la informaci√≥n que necesitan.\nOtra ventaja de MkDocs-Material es su navegaci√≥n intuitiva, que facilita a los usuarios la exploraci√≥n de la documentaci√≥n y la navegaci√≥n entre secciones y p√°ginas. Esto es fundamental para garantizar que los usuarios puedan acceder f√°cilmente a la informaci√≥n que est√°n buscando sin esfuerzo.\n\n\n\n\n\nLa documentaci√≥n de proyectos sigue una progresi√≥n simple:\n\nSin Documentaci√≥n\nAlguna Documentaci√≥n\nDocumentaci√≥n Completa\nBuena Documentaci√≥n\nExcelente Documentaci√≥n\n\nSi te sientes perdido acerca de por d√≥nde continuar con tu documentaci√≥n, observa en qu√© punto se encuentra tu proyecto en relaci√≥n con la progresi√≥n mencionada anteriormente. ¬øTienes alguna documentaci√≥n? Si no la tienes, comienza por ah√≠. Si ya tienes algo de documentaci√≥n pero te faltan algunos de los archivos clave del proyecto, comienza agreg√°ndolos.\nAl final, no te desanimes ni te sientas abrumado por la cantidad de trabajo necesario para documentar el c√≥digo.\nUna vez que comiences a documentar tu c√≥digo, te resultar√° m√°s f√°cil seguir adelante."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html",
    "href": "posts/2021/2021-08-11-jb.html",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter Book es un proyecto de c√≥digo abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.\nAlgunas caracter√≠sticas importantes del uso de Jupyter Book:\n\ncontenido con calidad de publicaci√≥n que incluya figuras, s√≠mbolos matem√°ticos, citas y referencias cruzadas!\nescribir contenido como Jupyter Notebooks, markdown o reStructuredText\nAgregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en l√≠nea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).\ngenerar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.\nuna interfaz de l√≠nea de comandos para crear libros r√°pidamente, por ejemplo, jupyter-book build mybook\n\nEn esta sesi√≥n, se muestra un ejemplo de c√≥mo crear un Jupyter Book desde cero y algunas de las caracter√≠sticas clave que ofrece Jupyter Book.\n\nNota: Puede encontrar los c√≥digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaci√≥n con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar Jupyter Book, necesitar√° usar la l√≠nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge jupyter-book\nDe lo contrario, puede instalar con pip:\npip install jupyter-book\n\n\n\nJupyter Book viene con una herramienta que le permite crear y construir libros r√°pidamente. Para crear el esqueleto del libro, escriba lo siguiente en la l√≠nea de comando:\njupyter-book create jupiter\n\nNota: Aqu√≠ llamamos al libro jupiter, pero puedes elegir llamar a tu libro como quieras.\n\nAhora tendr√°s un nuevo directorio llamado jupiter (o como quieras llamar a tu libro), con el siguiente contenido:\njupiter\n  ‚îú‚îÄ‚îÄ _config.yml\n  ‚îú‚îÄ‚îÄ _toc.yml\n  ‚îú‚îÄ‚îÄ content.md\n  ‚îú‚îÄ‚îÄ intro.md\n  ‚îú‚îÄ‚îÄ markdown.md\n  ‚îú‚îÄ‚îÄ notebooks.ipynb\n  ‚îú‚îÄ‚îÄ references.bib\n  ‚îî‚îÄ‚îÄ requirements.txt\nen donde: * _config.yml: archivo que contiene las configuraciones del proyecto. * _toc.yml: archivo que ordena los cap√≠tulos del libro. * content.md: archivo gen√©rico .md. * intro.md: archivo gen√©rico .md. * markdown.md: archivo gen√©rico .md. * notebooks.ipynb: archivo gen√©rico .ipynb. * references.bib: archivo para a√±adir las referencias. * requirements.txt: archivo que contiene las dependencias python) del proyecto.\n\n\n\nJupyter Book admite varios tipos de archivos:\n\nMarkdown (.md)\nnotebooks (.ipynb)\netc.\n\nComo Markdown y Jupyter Notebooks probablemente ser√°n los tipos de archivo m√°s comunes que usar√°, se mostrar√° un ejemplo de ello.\nLo primero ser√° eliminar los archivos de inicio en el directorio:\n\ncontent.md\nintro.md\nmarkdown.md\nnotebooks.ipynb\n\nAs√≠ que ejecutamos por l√≠nea de comando:\nrm content.md intro.md markdown.md notebooks.ipynb\nPor otro lado, nuestro proyecto estar√© conformado por tres archivos:\n\nindex.md\nIntroduction.md\ngreat_red_spot.ipynb\n\nLuego, debemos indicar c√≥mo ser√°n mostrados estos documentos en el archivo _toc.yml. La estructura ser√° la siguiente:\nformat: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\nEn este caso, root: index corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo index.md escribiremos:\n\n# Home\n\njupyter book example\n\n## Contenidos\n\n\n```{tableofcontents}\n```\n\n\n\n\n\n\nSe comienza por agregar un archivo de markdown. Con alg√∫n editor a elecci√≥n (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado Introduction.md.\nSe usa este archivo como demostraci√≥n de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.\n\n\nSe agrega un texto de Markdown simple a nuestro archivo. Si no est√° familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo Introduction.md.\n# Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n\n\n\nPuedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:\n```{figure} my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n\nSi bien la imagen puede estar contenida y referenciada desde el directorio ra√≠z, tambi√©n se puede incluir im√°genes a trav√©s de URL. Incluyamos una imagen del planeta J√∫piter en nuestro archivo Introduction.md usando lo siguiente:\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n\nLa raz√≥n por la que le damos un ‚Äúnombre‚Äù a nuestra imagen es para que podamos hacer referencia a ella f√°cilmente con la sintaxis:\n{numref}`jupiter-figure`\nSe agregar√° una oraci√≥n que incluya esta referencia. El archivo completo ahora deber√≠a verse as√≠:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\nEn este punto, probablemente se deber√≠a crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo _toc.yml. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay all√≠. Luego, simplemente agregue lo siguiente:\n- file: introduction\nAhora podemos construir nuestro libro desde la l√≠nea de comandos asegur√°ndonos de que estamos en el directorio ra√≠z de nuestro libro y luego usando:\njupyter-book build .\nUna vez finalizada la compilaci√≥n, tendr√° un nuevo subdirectorio llamado_build/html/ en la ra√≠z de su libro, navegue hasta esa ubicaci√≥n y abra _build/html/index.html. Deber√≠a verse algo como esto:\n\n\n\n\nJupyter Book usa MathJax para componer matem√°ticas, lo que le permite agregar matem√°ticas de estilo LaTeX a su libro. Puede agregar matem√°ticas en l√≠nea, bloques matem√°ticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matem√°ticas.\nLas matem√°ticas en l√≠nea se pueden definir usando $ de la siguiente manera:\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n\nJupiter has a mass of: \\(m_{j} \\approx 1.9 \\times 10^{27} kg\\)\nLos bloques matem√°ticos se pueden definir usando la notaci√≥n $$:\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n\n\\[m_{j} \\approx 1.9 \\times 10^{27} kg\\]\n\nNota: Si lo prefiere, los bloques matem√°ticos tambi√©n se pueden definir con \\begin{equation} en lugar de $$.\n\nLas ecuaciones numeradas se pueden definir as√≠ (este es el estilo que te recomiendo que uses con m√°s frecuencia):\n```{math}\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nAgreguemos m√°s contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo Introduction.md:\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```{math}\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```{math}\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nA continuaci√≥n, puede reconstruir su libro (jupyter-book build .) y abrir _build/html/index.html para asegurarse de que todo se est√© procesando como se esperaba.\n\n\n\n\nHay varias formas diferentes de controlar el dise√±o de las p√°ginas de su Jupyter Book. El cambio de dise√±o que utilizo con m√°s frecuencia es agregar contenido a un margen en la p√°gina. Puede agregar un margen usando la siguiente directiva:\n```{margin} An optional title\nSome margin content.\n```\n\nAgreguemos algo de contenido marginal al libro:\n```{margin} Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n\n\nHay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aqu√≠ en la documentaci√≥n de Jupyter Book. Las advertencias se crean con la sintaxis:\n```{note}\nI am a useful note!\n```\n\n\nNo dude en agregar la siguiente advertencia a Introduction.md:\n```{hint}\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\n\n\n\nEl √∫ltimo contenido corresponde a referencias y una bibliograf√≠a. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex Reference.bib que se encuentra en el directorio ra√≠z de su libro.\nPara incluir una cita en su libro, agregue una entrada bibtex a references.bib, por ejemplo:\n@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n\nNota: Consulte la documentaci√≥n de BibTex para obtener informaci√≥n sobre el estilo de referencia de BibTex. Google Scholar facilita la exportaci√≥n de un formato de cita bibtex.\n\nA continuaci√≥n, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:\n{cite}`mayor1995jupiter`\nO para m√∫ltiples citas:\n{cite}`mayor1995jupiter,guillot1999interiors`\nLuego puede crear una bibliograf√≠a a partir de reference.bib usando:\n```{bibliography} references.bib\n```\n\nPor ejemplo, intente agregar esto a su archivo Introduction.md:\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```{bibliography} references.bib\n```\n\nSu archivo final Introduction.md deber√≠a verse as√≠:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```{math}\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```{math}\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\n```{margin} Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n```{hint}\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```{bibliography} references.bib\n```\n\nY deber√≠a renderizarse as√≠: \n\n\n\nTodos los flujos de trabajo de formato y estilo que vimos en markdown tambi√©n se aplican a un Jupyter Notebook; simplemente agr√©guelos a una celda de markdown y listo.\nComencemos con lo siguiente:\n\nCree un nuevo notebook llamado great_red_spot.ipynb;\nAgregue este archivo a su _toc.yml;\nAgregue una celda de markdown con el siguiente contenido:\n\n# The Great Red Spot\n\nJupiter‚Äôs iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```{figure} https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n\n¬°Ahora intente construir su libro (jupyter-book build .) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva p√°gina ‚ÄúThe Great Red Spot‚Äù, que deber√≠a verse as√≠:\n\n¬°Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos gr√°ficos.\nCree una nueva celda de c√≥digo debajo de la celda de rebaja actual y agregue el siguiente c√≥digo para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n\nNota: Estamos imprimiendo la salida en la pantalla con el uso de df.head() y esto se mostrar√° en nuestro Jupyter Book renderizado.\n\nSi reconstruye su libro (jupyter-book build .) en este punto, ver√° algo como lo siguiente:\n\nAhora, podemos usar estos datos para crear algunos gr√°ficos.\nLas tramas en su Jupyter Book pueden ser est√°ticas (por ejemplo, matplotlib, seaborn) o interactivas (por ejemplo, altair, plotly, bokeh). Para este tutorial, crearemos algunos gr√°ficos de ejemplo usando Plotly (a trav√©s del backend de Pandas).\nPrimero creemos un diagrama de dispersi√≥n simple de nuestros datos. Cree una nueva celda de c√≥digo en su cuaderno y agregue el siguiente c√≥digo:\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\nYa que estamos en eso, creemos tambi√©n una trama animada. Cree otra celda de c√≥digo nueva y agregue el siguiente c√≥digo:\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n\nNota: Plotly tiene diferentes renderizadores disponibles para generar gr√°ficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que pio.renderers.default = \"notebook\" funciona con la versi√≥n actual de Jupyter Book.\n\n¬°Ahora, reconstruyamos nuestro libro y echemos un vistazo!\n\nEs posible que desee ocultar parte del c√≥digo en su libro, ¬°no hay problema! Eso tambi√©n se hace f√°cilmente con Jupyter Book.\nEl que nos interesa aqu√≠ es ocultar la entrada de c√≥digo. Podemos hacerlo f√°cilmente agregando la etiqueta hide-input a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuaci√≥n:\n\nContin√∫e y agregue las etiquetas hide-input a ambas celdas de trazado en su archivo great_red_spot.ipynb. Cuando reconstruyas el libro, ver√°s que la entrada del c√≥digo est√° oculta (pero se puede alternar con el √≠cono +):\n\n\nNota: Tambi√©n puede almacenar el contenido de la libreta como valores, gr√°ficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta glue.\n\n\n\n\n\n\nJupyter-Book - Documentation\nTutorial - Jupyter Book"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html",
    "href": "posts/2021/2021-07-31-jupyter.html",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar c√≥digo en Python (por defecto, aunque permite otros lenguajes) de manera din√°mica, a la vez que integrar en un mismo documento tanto bloques de c√≥digo como texto, gr√°ficas o im√°genes. Es un SaaS utilizado ampliamente en an√°lisis num√©rico, estad√≠stica y machine learning, entre otros campos de la inform√°tica y las matem√°ticas.\nPor otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz m√°s interesante para los usuarios. Eventualmente Jupyter Lab reemplazar√° a Jupyter Notebok.\nNos centraremos en comprender aspectos b√°sicos de c√≥mo trabajar un archivo en jupyter notebook (extensi√≥n .ipynb).\n\n\n\n\n\nPara instalar RISE, necesitar√° usar la l√≠nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge notebook\nDe lo contrario, puede instalar con pip:\npip install notebook\n\nNota: SI desea instalar JupyterLab, simplemente reemplaza notebook por jupyterlab.\n\n\n\n\n\n\n\nUna vez que haya instalado Jupyter Notebook en su computadora, estar√° listo para ejecutar el servidor de la computadora port√°til. Puede iniciar el servidor del port√°til desde la l√≠nea de comandos (usando Terminal en Mac/Linux, S√≠mbolo del sistema en Windows) ejecutando:\njupyter notebook\n\nEsto imprimir√° cierta informaci√≥n sobre el servidor en su terminal, incluida la URL de la aplicaci√≥n web (de forma predeterminada, http://localhost:8888):\n$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nA continuaci√≥n, abrir√° su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, ver√° el Panel, que mostrar√° una lista de notebooks, archivos y subdirectorios en el directorio donde se inici√≥ el servidor.\n\nLa parte superior de la lista de notebooks se muestran rutas de navegaci√≥n en las que se puede hacer clic del directorio actual.\nPara crear un nuevo notebook, haga clic en el bot√≥n New en la parte superior de la lista y seleccione el kernel del men√∫ desplegable (como se ve a continuaci√≥n). Los kernels que se enumeran dependen de lo que est√© instalado en el servidor.\n\nNota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opci√≥n para usted.\n\n\nUna vez seleccionado el kernel, se abrira nuestro primer notebook!.\n\n\n\n\nJupyter notebook nos ofrece el siguiente toolbox:\n\n\nFile: En √©l, puede crear un nuevo cuaderno o abrir uno preexistente. Aqu√≠ es tambi√©n a donde ir√≠a para cambiar el nombre de un Cuaderno. Creo que el elemento de men√∫ m√°s interesante es la opci√≥n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.\nEdit: Aqu√≠ puede cortar, copiar y pegar celdas. Aqu√≠ tambi√©n es donde ir√≠as si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aqu√≠ tambi√©n.\nView: es √∫til para alternar la visibilidad del encabezado y la barra de herramientas. Tambi√©n puede activar o desactivar los n√∫meros de l√≠nea dentro de las celdas. Aqu√≠ tambi√©n es donde ir√≠as si quieres meterte con la barra de herramientas de la celda.\nInsert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.\nCell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. Tambi√©n puede ir aqu√≠ para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es m√°s intuitiva para eso.\nKernel: es para trabajar con el kernel que se ejecuta en segundo plano. Aqu√≠ puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que est√° utilizando su computadora port√°til.\nWidgets: es para guardar y borrar el estado del widget. Los widgets son b√°sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido din√°mico utilizando Python (u otro Kernel).\nHelp: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia.\n\n\n\n\n\n\nJupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, t√≠tulos de distintos tama√±os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.\nLos lenguajes de markup son lenguajes ideados para procesar texto, algunos de los m√°s conocidos son HTML y \\(\\LaTeX\\). Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.\nLa cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que m√°s se utilizan.\n\nTexto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecer√° como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecer√° como palabra.\nListas: Las listas en Markdown se realizan indicando un asterisco o un n√∫mero seguido de un punto si se desean listas numeradas. Markdown organiza autom√°ticamente los items asign√°ndoles el n√∫mero correcto.\nInclusi√≥n de im√°genes: La sintaxis para incluir im√°genes en Markdown es ![nombre alternativo](direcci√≥n de la imagen) en donde el nombre alternativo aparecer√° en caso de que no se pueda cargar la im√°gen y la direcci√≥n puede referirse a una imagen local o un enlace en Internet.\nInclusi√≥n de c√≥digo HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el c√≥digo HTML.\nEnlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a p√°ginas en internet u otros archivos locales. Su sintaxis es [texto](direcci√≥n del enlace).\nF√≥rmulas matem√°ticas: Gracias al uso de MathJax, se puede incluir c√≥digo en \\(\\LaTeX\\) para mostrar todo tipo de f√≥rmulas y expresiones matem√°ticas. Las f√≥rmulas dentro de una l√≠nea de texto se escriben entre s√≠mbolos de d√≥lar $...$, mientras que las expresiones separadas del texto utilizan s√≠mbolos de d√≥lar dobles $$...$$. Los siguientes son ejemplos de f√≥rmulas matem√°ticas escritas en \\(\\LaTeX\\):\n\n\\[p(x) = 3x^2 + 5y^2 + x^2y^2\\]\n\\[e^{\\pi i} - 1 = 0\\]\n\\[\\lim_{x \\rightarrow \\infty} 3x+1\\]\n\\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\]\n\\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\]\n\n\n\n\nJupyter Notebook permite que escribamos c√≥digo dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.\nVeamos unos ejemplos sencillos de c√≥digo:\n\nimport math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n\nLa raiz cuadra de 16 es 4.0\n\n\nTambi√©n es posible visualizar tablas de datos con la librer√≠a pandas:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Title\n      US_Gross\n      Worldwide_Gross\n      US_DVD_Sales\n      Production_Budget\n      Release_Date\n      MPAA_Rating\n      Running_Time_min\n      Distributor\n      Source\n      Major_Genre\n      Creative_Type\n      Director\n      Rotten_Tomatoes_Rating\n      IMDB_Rating\n      IMDB_Votes\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      146083.0\n      NaN\n      8000000.0\n      Jun 12 1998\n      R\n      NaN\n      Gramercy\n      None\n      None\n      None\n      None\n      NaN\n      6.1\n      1071.0\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      10876.0\n      NaN\n      300000.0\n      Aug 07 1998\n      R\n      NaN\n      Strand\n      None\n      Drama\n      None\n      None\n      NaN\n      6.9\n      207.0\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      203134.0\n      NaN\n      250000.0\n      Aug 28 1998\n      None\n      NaN\n      Lionsgate\n      None\n      Comedy\n      None\n      None\n      NaN\n      6.8\n      865.0\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      373615.0\n      NaN\n      300000.0\n      Sep 11 1998\n      None\n      NaN\n      Fine Line\n      None\n      Comedy\n      None\n      None\n      13.0\n      NaN\n      NaN\n    \n    \n      4\n      Slam\n      1009819.0\n      1087521.0\n      NaN\n      1000000.0\n      Oct 09 1998\n      R\n      NaN\n      Trimark\n      Original Screenplay\n      Drama\n      Contemporary Fiction\n      None\n      62.0\n      3.4\n      165.0\n    \n  \n\n\n\n\nUnas de las cosas m√°s significativas de Jupyter notebook es poder trabajar con distintos tipos de gr√°ficos (imagen est√°tica o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.\n\n#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nLa completaci√≥n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que est√© tratando.\nSimplemente escriba object_name.<TAB> para ver los atributos del objeto. Adem√°s de los objetos y palabras clave de Python, la finalizaci√≥n de pesta√±as tambi√©n funciona en nombres de archivos y directorios.\nimport collections\ncollections. # aprete la tecla <ùëáùê¥ùêµ>\n\n\n\nEn caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una funci√≥n llamada help.\nEn resumen, ¬°suele ser m√°s importante saber como buscar informaci√≥n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaci√≥n ? luego del nombre del comando (y luego ejecutar la celda con la combinaci√≥n de teclas SHIFT + ENTER).\nimport numpy as np\nnp.sum?\n\n\n\nJupyter posee varias funciones m√°gicas predefinidas que sirven para simplificar tareas comunes.\nHay dos tipos de magias:\n\nMagias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma l√≠nea.\nMagias por celda (cell magics): son comandos que empiezan con los caracteres %%, y que reciben argumentos en la misma l√≠nea y en toda la celda.\n\nEn general solo se puede usar una sola m√°gias por celda en cada celda y debe ser escrita en la primer linea de la celda.\nUn buen ejemplo de m√°gia es %lsmagic que lista todas las magias disponibles:\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\nEn varias situaciones resulta necesario medir el tiempo de ejecuci√≥n de una porci√≥n de c√≥digo. Para ello podemos usar la magia %timeit. Esta magia est√° disponible tanto para l√≠nea como para celda:\n\n%%timeit \n1+1 # timeit repite (adaptativamente) la medici√≥n a fin de reducir el error.\n\n8.68 ns ¬± 0.387 ns per loop (mean ¬± std. dev. of 7 runs, 100000000 loops each)\n\n\nJupyter notebook permite tambi√©n mezclar varios lenguajes de programaci√≥n en una misma notebook. Por ejemplo, podr√≠amos escribir en bash lo siguiente:\n\n%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n\n3\n2\n1\nHola desde /usr/bin/bash\n\n\nTambi√©n, puede acceder a la l√≠nea de comandos, anteponiendo el s√≠mbolo de !. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: !pip install pandas).\nVeamos un ejemplo:\n\n!pwd\n\n/home/fralfaro/PycharmProjects/ds_blog/_notebooks\n\n\n\n\n\n\n\nNotebook Basics\nRunning the Notebook"
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html",
    "href": "posts/2021/2021-08-05-rise.html",
    "title": "RISE",
    "section": "",
    "text": "RISE es una extensi√≥n a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.\nToda las celdas pueden editarse y ejecutarse directamente, durante la presentaci√≥n. Esto es pr√°ctico si necesitas corregir un error en una celda de texto. M√°s importante a√∫n, puedes ejecutar c√≥digo directamente en el kernel. En una misma diapositiva puedes tener m√∫ltiples celdas y elegir cu√°l ejecutar, o corregir el texto y volver a ejecutar.\n\nAlgunas caracter√≠sticas importantes del uso de RISE:\n\nSimplifica la generaci√≥n de material.\nSe mantiene un archivo y no varios archivos para hablar de lo mismo.\nEs f√°cil de corregir, no se necesita mucho esfuerzo (similar a una PPT).\n\nEn esta sesi√≥n, se muestra un ejemplo de c√≥mo crear una presentaci√≥n con RISE.\n\nNota: Puede encontrar los c√≥digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaci√≥n con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar RISE, necesitar√° usar la l√≠nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge rise\nDe lo contrario, puede instalar con pip:\npip install RISE\n\nNota: No interactuar√°s directamente con RISE. En su lugar, podr√° acceder a √©l a trav√©s de Jupyter Notebooks.\n\n\n\n\n\nPara crear una presentaci√≥n, deber√° iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto despu√©s de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deber√° habilitar la presentaci√≥n de diapositivas. Puede hacer esto haciendo lo siguiente:\n\nHaga clic en ‚ÄúVer‚Äù en la barra de herramientas de Jupyter\nColoca el cursor sobre ‚ÄúBarra de herramientas de celda‚Äù en el men√∫ ‚ÄúVer‚Äù\nHaga clic en ‚ÄúPresentaci√≥n de diapositivas‚Äù en el men√∫ ‚ÄúBarra de herramientas de celda‚Äù\n\n\n\n\nEn este punto, deber√≠a tener una barra de herramientas de celda con un men√∫ desplegable en el lado derecho: \nDeber√≠a ver seis opciones aqu√≠. Este men√∫ desplegable y sus opciones determinan c√≥mo encaja cada celda en la presentaci√≥n. Las opciones y sus descripciones se encuentran a continuaci√≥n:\n\nslide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.\nsub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.\nfragment: indica que la celda seleccionada debe aparecer como una compilaci√≥n de la diapositiva anterior.\nskip: indica que la celda seleccionada debe omitirse y no ser parte de la presentaci√≥n de diapositivas.\nnotes: indica que la celda seleccionada debe ser solo notas del presentador.\n- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es √∫til cuando una celda de rebaja y una celda de c√≥digo deben aparecer simult√°neamente.\n\nCada una de estas opciones puede incluir c√≥digo Python o c√≥digo Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.\n\n\n\nUna vez que se han utilizado las celdas para crear material para la presentaci√≥n, la presentaci√≥n se puede ver directamente desde el notebook.\nHay dos opciones para ver la presentaci√≥n de diapositivas:\n\nUsar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentaci√≥n desde dentro de la computadora port√°til\nAl hacer clic en el bot√≥n ‚ÄúModo de presentaci√≥n‚Äù de la computadora port√°til, esto solo aparecer√° si ha instalado RISE.\n\n\nDespu√©s de ingresar al modo de presentaci√≥n, deber√≠a ver una pantalla similar a esta:\n\n\n\n\nSi bien puede ser tentador usar las teclas <- y -> para cambiar las diapositivas en la presentaci√≥n, esto no funcionar√° por completo: omitir√° las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentaci√≥n de diapositivas hacia adelante y MAY√öS + ESPACIO para mover la presentaci√≥n de diapositivas hacia atr√°s.\nHay muchos otros atajos de teclado a los que se puede acceder dentro de la presentaci√≥n haciendo clic en el signo de interrogaci√≥n (?) en la esquina inferior izquierda.\n\n\n\nUna de las mejores cosas de RISE es que funciona en una sesi√≥n de Python en vivo, lo que significa que puede editar y ejecutar c√≥digo mientras se ejecuta la presentaci√≥n.\n\n\n\n\nPuedes exportar tu presentaci√≥n desplegando la opci√≥n: File -> Download as.\n\nNota: Para poder descargar en formato .pdf, necesita tener instalado pandoc.\n\n\n\n\n\n\nRISE - Documentation\nCreating Interactive Slideshows in Jupyter Notebooks"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Main objective is understand of the best way the challenge LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\nThe steps to follow are:\n\nOverview of the Dataset: Understanding the datasets available.\nPreprocessing: Preprocessing of the datasets available.\nEDA: Exploratory data analysis using visualization tools in Python.\n\n\nNote: My analysis is inspired by several of the notebooks that different profiles have uploaded to the challenge, so some graphics or images belong to these authors. The most important ones will be found in the references. On the other hand, my project is available in Jupyter Book, click in the following link.\n\n\n\nThe objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\nFile: File name (.csv).\nShape: Dimensionality of datasets.\nDescription: Basic description of the dataset.\nTop 5 rows: Show first 5 rows + explanation for some columns.\nSummary: Summary of datasets.\n\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T21:59:13.410181Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T21:59:13.316042Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T21:59:41.807446Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T21:59:41.806324Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:48:09.998997Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:28.514116,‚Äúend_time‚Äù:‚Äú2021-08-24T21:59:41.807642‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T21:59:13.293526‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=1}\n# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n:::\n\n\n\nFile: districts_info.csv.\nShape: \\(233\\) rows \\(\\times\\) \\(7\\) columns.\nDescription: file contains information about each school district.\nTop 5 rows::\n\n\n\nSummary:\n\n\n\n\n\n\nFile: products_info.csv\nShape: \\(372\\) rows \\(\\times\\) \\(6\\) columns.\nDescription: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nFile: engagement_data/*.csv.\nShape: \\(22324190\\) rows \\(\\times\\) \\(5\\) columns.\nDescription: file contains information about each school district. The files can be joined by the key columns district_id and lp_id.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nPreprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.\n\none-hot encoding the product sectors\nsplitting up the primary essential function into main and sub category\n\n\nNote: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.\n\n\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T21:59:42.315509Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T21:59:42.314913Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:00.669027Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:00.669504Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:48:44.902736Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:18.384236,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:00.669700‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T21:59:42.285464‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=2}\n# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n:::\n\n\n\nExploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\nVisualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy Grays scale next to the technique: dark text on a light background.\n\nNote: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.\n\n\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:00.788039Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:00.787339Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:00.944794Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:00.944141Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:15.148181Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.179153,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:00.944932‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:00.765779‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=3}\n# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\n\n                                                \n\n:::\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:01.006598Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:01.005538Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:01.460650Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:01.461172Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:15.338218Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.494452,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:01.461333‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:00.966881‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=4}\n# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n\n\n\n:::\n\n\n\nLocales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).\nFor the pct_black/hispanic variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\nFor pctfree/reduced and pp_total_raw indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:01.557765Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:01.556848Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:03.160447Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:03.160919Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:15.865498Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:1.630377,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:03.161103‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:01.530726‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=5}\n# heatmap: districts -> locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n\n\n\n:::\n\n\n\nSectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categor√≠a mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the primary_function_main variable, all sectors are focused on theLC category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:03.301514Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:03.292826Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:03.796850Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:03.797314Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:17.574316Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.551066,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:03.797481‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:03.246415‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=6}\nplt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n\n\n\n:::\n\n\n\nContinuing the analysis of the primary_function_main variable, it was observed that most of these are in theLC category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is Sites, Resources & Reference (101).\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:03.924145Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:03.923467Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:04.076527Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:04.077022Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:18.054662Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.190305,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:04.077195‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:03.886890‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=7}\n# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n\n\n\n:::\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:04.155626Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:04.153843Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:04.430687Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:04.431156Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:18.238340Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.322608,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:04.431319‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:04.108711‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=8}\n# pieplot: products -> subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n\n\n\n:::\n\n\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n258 providers have 1 occurrences.\n18 providers have 2 occurrences.\n9 providers have 3 occurrences.\n2 providers have 4 occurrences.\n2 providers have 6 occurrences.\n1 provider have 30 occurrences.\n\nBased on this, only the top 15 providers will be displayed.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:04.610677Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:04.586018Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:04.945261Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:04.945715Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:18.515334Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:0.414758,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:04.945874‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:04.531116‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=9}\ndct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n:::\nWith regard to products, there are about 372 different products.\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the product_name variable.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:05.106485Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:05.104433Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:06.068287Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:06.068956Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:18.935625Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:1.012728,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:06.069135‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:05.056407‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=10}\ncloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n\n\n\n:::\nTo understand more in detail the use of these products, we will analyze the use of these products with respect to the variable engagement_index. The first graph is related to the average engagement_index (per student) for the year 2020, where the first 15 products will be displayed.\nAn important fact is that 362 products have an average of less than 1!.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:06.276148Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:06.275229Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:07.642119Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:07.642568Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:19.888566Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:1.424686,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:07.642750‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:06.218064‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=11}\ngroup_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']<1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n:::\nLet‚Äôs study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\nNote: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.\n\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:08.316803Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:08.316106Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:13.858922Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:13.859377Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:21.992666Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:6.060783,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:13.859531‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:07.798748‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=12}\ncol = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n\n\n\n:::\nNow, we can understand the engagement index for the most important tools about districts, where the districts of * Wisconsin ,  Missouri * and * Virginia * have the highest engagement index among the three most used tools.\n::: {.cell _kg_hide-input=‚Äòtrue‚Äô execution=‚Äò{‚Äúiopub.execute_input‚Äù:‚Äú2021-08-24T22:00:14.637147Z‚Äù,‚Äúiopub.status.busy‚Äù:‚Äú2021-08-24T22:00:14.636457Z‚Äù,‚Äúiopub.status.idle‚Äù:‚Äú2021-08-24T22:00:17.454954Z‚Äù,‚Äúshell.execute_reply‚Äù:‚Äú2021-08-24T22:00:17.455400Z‚Äù,‚Äúshell.execute_reply.started‚Äù:‚Äú2021-08-24T21:49:29.563655Z‚Äù}‚Äô papermill=‚Äò{‚Äúduration‚Äù:3.421243,‚Äúend_time‚Äù:‚Äú2021-08-24T22:00:17.455572‚Äù,‚Äúexception‚Äù:false,‚Äústart_time‚Äù:‚Äú2021-08-24T22:00:14.034329‚Äù,‚Äústatus‚Äù:‚Äúcompleted‚Äù}‚Äô tags=‚Äò[‚Äúhide-input‚Äù]‚Äô execution_count=13}\ngroup_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']>0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n\n\n\n:::\n\n\n\n\n\nDepending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\nWhen looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning\n\n\n\n\n\nDiverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland\nKaggle Competitions:\n\nEnthusiast to Data Professional - What changes?\nHow To Approach Analytics Challenges\nMost popular tools in 2020 Digital Learning"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html",
    "href": "posts/2021/2021-08-20-fastpages.html",
    "title": "Fastpages",
    "section": "",
    "text": "Fastpages es una plataforma que te permite crear y alojar un blog de forma gratuita, sin anuncios y con muchas funciones √∫tiles, como:\n\nCree publicaciones que contengan c√≥digo, salidas de c√≥digo (que pueden ser interactivas), texto formateado, etc. directamente desde Jupyter Notebooks. Las publicaciones en notebooks admiten funciones como:\n\nLas visualizaciones interactivas realizadas con Altair siguen siendo interactivas.\nOcultar o mostrar la entrada y salida de la celda.\nCeldas de c√≥digo contra√≠bles que est√°n abiertas o cerradas de forma predeterminada.\nDefina el t√≠tulo, el resumen y otros metadatos a trav√©s de celdas de rebajas especiales\nPosibilidad de agregar enlaces a Colab y GitHub autom√°ticamente.\n\nCree publicaciones, incluido el formato y las im√°genes, directamente desde documentos de Microsoft Word.\nCree y edite publicaciones de Markdown completamente en l√≠nea usando el editor de Markdown incorporado de GitHub.\nInserta tarjetas de Twitter y videos de YouTube.\nCategorizaci√≥n de publicaciones de blog por etiquetas proporcionadas por el usuario para mayor visibilidad.\n\nEn esta secci√≥n se ense√±ar√° los pasos b√°sicos para poder crear su propio blog con fastpages.\n\n\n\n\n\n¬°El proceso de configuraci√≥n de p√°ginas r√°pidas tambi√©n est√° automatizado con GitHub Actions! Al crear un repositorio a partir de la plantilla de p√°ginas r√°pidas, se abrir√° autom√°ticamente una solicitud de extracci√≥n (despu√©s de ~ 30 segundos) configurando su blog para que pueda comenzar a funcionar. La solicitud de extracci√≥n automatizada lo recibir√° con instrucciones como esta: \n¬°Todo lo que tienes que hacer es seguir estas instrucciones (en el PR que recibes) y tu nuevo sitio de blogs estar√° en funcionamiento!\n\nNote: Si tienes dudas con la instalaci√≥n, te recomiendo ver el siguiente video.\n\n\n\n\n\nEl repositorio de fastpages esta compuesto de la siguiente forma:\n‚îú‚îÄ‚îÄ .github\n‚îú‚îÄ‚îÄ _action_files\n‚îú‚îÄ‚îÄ _fastpages_docs\n‚îú‚îÄ‚îÄ images\n‚îú‚îÄ‚îÄ _includes\n‚îú‚îÄ‚îÄ _layouts\n‚îú‚îÄ‚îÄ _notebooks\n‚îú‚îÄ‚îÄ _pages\n‚îú‚îÄ‚îÄ _plugins\n‚îú‚îÄ‚îÄ _posts\n‚îú‚îÄ‚îÄ _sass\n‚îî‚îÄ‚îÄ _word\n‚îî‚îÄ‚îÄ .devcontainer.json\n‚îî‚îÄ‚îÄ Makefile\n‚îî‚îÄ‚îÄ index.html\n‚îî‚îÄ‚îÄ Gemfile\n‚îî‚îÄ‚îÄ _config.yml\n‚îî‚îÄ‚îÄ LICENSE\n‚îî‚îÄ‚îÄ docker-compose.yml\n‚îî‚îÄ‚îÄ Gemfile.lock\n‚îî‚îÄ‚îÄ README.md\n‚îî‚îÄ‚îÄ .gitattributes\n‚îî‚îÄ‚îÄ .gitignore\nDe momento nos vamos a centrar en algunos de estos archivos:\n\n_config.yml: es el archivo que funciona como el motor del proyecto. En ente archivo, podemos poner el nombre a nuestro blog, el logo, informaci√≥n personal (github, linkedin, etc), entre otras cosas.\nindex.html: Corresponde a la primera p√°gina cuando se despliega nuestro blog. por lo que es importante escribir alg√∫n mensaje para especificar la motivaci√≥n de hacer un blog.\n/_notebooks: Lugar donde se deben guardar los notebooks (.ipynb) con la convenci√≥n de nomenclatura **YYYY-MM-DD-*.ipynb**.\n/_posts: Lugar donde se deben guardar los archivos markdown (.md) con la convenci√≥n de nomenclatura **YYYY-MM-DD-*.md**.\n/_word: Lugar donde se deben guardar los archivos word (.docx) con la convenci√≥n de nomenclatura **YYYY-MM-DD-*.docx**.\n\n\nNote: fastpages usa nbdev para impulsar el proceso de conversi√≥n de Jupyter Notebooks en publicaciones de blog. Cuando guardas un notebook en la carpeta /_notebooks de tu repositorio, GitHub Actions aplica nbdev a esos notebooks autom√°ticamente. El mismo proceso ocurre cuando guarda documentos de Word o markdown en el directorio _word o_posts, respectivamente.\n\n\n\n\nEn esta parte, se muestran caracter√≠sticas especiales que fastpages proporciona para los notebooks. Tambi√©n puede escribir las publicaciones de su blog con documentos de Word o makdown.\n\n\nLa primera celda de su Jupyter Notebook o markdown contiene informaci√≥n preliminar. El tema principal son los metadatos que pueden activar/desactivar opciones en su Notebook. Tiene el formato siguiente:\n# Title\n> Awesome summary\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\nTodas las configuraciones anteriores est√°n habilitadas en esta publicaci√≥n, ¬°para que pueda ver c√≥mo se ven!\n\nEl campo de resumen (precedido por >) se mostrar√° debajo de su t√≠tulo y tambi√©n lo utilizar√°n las redes sociales para mostrar la descripci√≥n de su p√°gina.\ntoc: establecer esto en true generar√° autom√°ticamente una tabla de contenido\nbadges: establecer esto en true mostrar√° los enlaces de Google Colab y GitHub en la publicaci√≥n de su blog.\ncomments: establecer esto en true habilitar√° los comentarios. Consulte estas instrucciones para obtener m√°s detalles.\nautor: esto mostrar√° los nombres de los autores.\ncategories: permitir√° que su publicaci√≥n sea categorizada en una p√°gina de ‚ÄúEtiquetas‚Äù, donde los lectores pueden navegar por su publicaci√≥n por categor√≠as.\n\nMarkdown Front Matters tiene un formato similar al de los notebooks. Las diferencias entre los dos se puede ver en el siguiente link.\n\n\n\ncoloque una marca # collapse-hide en la parte superior de cualquier celda si desea ocultar esa celda de forma predeterminada, pero d√©le al lector la opci√≥n de mostrarla:\n\n#hide\n!pip install pandas altair\n\nRequirement already satisfied: pandas in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: altair in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: pytz>=2017.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy>=1.17.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (1.21.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six>=1.5 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\nRequirement already satisfied: jinja2 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.0.1)\nRequirement already satisfied: toolz in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: jsonschema in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jinja2->altair) (2.0.1)\nRequirement already satisfied: attrs>=17.4.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema->altair) (21.2.0)\nRequirement already satisfied: setuptools in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema->altair) (57.1.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema->altair) (0.18.0)\nWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\nYou should consider upgrading via the '/home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/bin/python -m pip install --upgrade pip' command.\n\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\ncoloque una marca # collapse-show en la parte superior de cualquier celda si desea mostrar esa celda de forma predeterminada, pero d√©le al lector la opci√≥n de ocultarla:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nSi desea ocultar las celdas por completo (no solo contraerlas), lea estas instrucciones.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n\n\nPuede mostrar tablas de la forma habitual en su blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'IMDB_Rating']].head()\n\n\n\n\n\n  \n    \n      \n      Title\n      Worldwide_Gross\n      Production_Budget\n      IMDB_Rating\n    \n  \n  \n    \n      0\n      The Land Girls\n      146083.0\n      8000000.0\n      6.1\n    \n    \n      1\n      First Love, Last Rites\n      10876.0\n      300000.0\n      6.9\n    \n    \n      2\n      I Married a Strange Person\n      203134.0\n      250000.0\n      6.8\n    \n    \n      3\n      Let's Talk About Sex\n      373615.0\n      300000.0\n      NaN\n    \n    \n      4\n      Slam\n      1087521.0\n      1000000.0\n      3.4\n    \n  \n\n\n\n\n\n\n\n¬°Las visualizaciones interactivas realizadas con Altair siguen siendo interactivas!\nDejamos esta celda de abajo sin ocultar para que pueda disfrutar de una vista previa del resaltado de sintaxis en p√°ginas r√°pidas, que utiliza el tema de dr√°cula.\n\n#collapse-hide\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n\nAl escribir Le doy a esta publicaci√≥n dos :+1:! Se mostrar√° esto:\nLe doy a esta publicaci√≥n dos :+1:!\n\n\nPuede incluir im√°genes de rebajas con leyenda (caption) como este:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")\n\nPor supuesto, la leyenda es opcional.\n\n\n\nSi escribe > twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 mostrar√° esto:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20\n\n\n\n\nSi escribe > youtube: https://youtu.be/XfoYk_Z5AkI mostrar√° esto:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI\n\n\n\n\nPuedes ejecutar algunas cajas con mensajes de nota, informaci√≥n o warning!. A continuaci√≥n se muestran algunos ejemplos:\n> Warning: There will be no second warning! > Warning: There will be no second warning!\n> Important: Pay attention! It's important.\n\nImportant: Pay attention! It‚Äôs important.\n\n> Tip: This is my tip.\n\nTip: This is my tip.\n\n> Note: Take note of this.\n\nNote: Take note of this.\n\n\n\n\n\n\nRepositorio: fastpages\nnbdev: notebooks a posts\nFastai: foro"
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html",
    "href": "posts/2021/2021-08-31-buenas_practicas.html",
    "title": "Buenas Pr√°cticas - Python",
    "section": "",
    "text": "Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cu√°l es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programaci√≥n, con el fin de hacer el c√≥digo m√°s legible, sencillo de entender y ayudar a encontrar posibles errores.\nEn esta secci√≥n se mostrar√° algunos conceptos sencillos que te ayudar√°n a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\nEl PEP8 es un estilo de codificaci√≥n que proporciona convenciones de codificaci√≥n para el c√≥digo Python que comprende la biblioteca est√°ndar en la distribuci√≥n principal de Python.\nAlgunos aspectos importantes:\n\nEl PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la Gu√≠a de estilo Python de Guido, con algunas adiciones de la gu√≠a de estilo de Barry.\nEsta gu√≠a de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.\nMuchos proyectos tienen sus propias pautas de estilo de codificaci√≥n. En caso de conflicto, dichas gu√≠as espec√≠ficas del proyecto tienen prioridad para ese proyecto.\n\nBasados en el PEP8 y algunas buenas pr√°cticas del dise√±o de software, veamos ejemplo para poder escribir de mejor forma nuestros c√≥digos.\n\n\nCuando sea posible, define variables con nombres que tengan alg√∫n sentido o que puedas identificar f√°cilmente, no importa que sean m√°s largas. Por ejemplo, en un programa podr√≠amos escribir:\n\na = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\npero, ¬øqu√© significan a y b? lo sabemos por el comentario (bien hecho), pero si m√°s adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:\n\naltura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\n\n\n\nLas l√≠neas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una l√≠nea larga, se puede cortar con una barra invertida (\\) y continuar en la siguiente l√≠nea:\n\nprint(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la l√≠nea inferior.\")\n\nEsta es una frase muy larga, se puede cortar con un        y seguir en la l√≠nea inferior.\n\n\n\n\n\nLos comentarios son muy importantes al escribir un programa. Describen lo que est√° sucediendo dentro de un programa, para que una persona que mira el c√≥digo fuente no tenga dificultades para descifrarlo.\n\n#\n# esto es un comentario\nprint('Hola')\n\nHola\n\n\nTambi√©n podemos tener comentarios multil√≠neas:\n\n#\n# Este es un comentario largo\n# y se extiende\n# a varias l√≠neas\n\n\n\n\nLas importaciones generalmente deben estar en l√≠neas separadas:\n\n#\n# no:\nimport sys, os\n\n\n#\n# si:\nimport os\nimport sys\n\n\n\n\nExisten varias formas de hacer comparaciones de objetos (principalmente en el uso del bucle if), ac√° se dejan alguna recomendaciones:\n# no\nif greeting == True:\n\n# no\nif greeting is True:\n# si\nif greeting:\n\n\n\nDentro de par√©ntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:\n\n#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n\n\n#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n\nAunque en Python se pueden hacer varias declaraciones en una l√≠nea, se recomienda hacer s√≥lo una en cada l√≠nea:\n\n#\n# no\na = 10; b = 20\n\n\n#\n# si\na = 10\nb = 20  \n\nCuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada l√≠nea sus argumentos.\n\n#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]  \n\n\n#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n\nLo anterior se puede extender para funciones con muchos argumentos\n\n#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n\n\n#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n\n\n\n\nUn tema interesante es corresponde a la identaci√≥n respecto a los operadores binarios, ac√° se muestra la forma correcta de hacerlo:\n# no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n# si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n\n\n\nAunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay m√©todos espec√≠ficos m√°s eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:\n\n#\n# Seleccionar los n√∫meros positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i > 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\nAunque t√©cnicamente es correcto, es m√°s eficiente hacer List Comprehension:\n\n#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i > 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\n\n\n\nCuando se ocupa try/except, es necesario especificar el tipo de error que se est√° cometiendo.\n\n#\n# importar librerias\nimport sys\n\n\n#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un <class 'ZeroDivisionError'>\n\n\n\n#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un <class 'ZeroDivisionError'>\n\n\n\n\n\nSiempre es mejor definir las variables dentro de una funci√≥n y no dejar variables globales.\n\n#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n\n\nfuncion_01(2)\n\n9\n\n\n\n#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n\n\nfuncion_01(2,5)\n\n9\n\n\n\n\n\nCon Python 3 se puede especificar el tipo de par√°metro y el tipo de retorno de una funci√≥n (usando la notaci√≥n PEP484 y PEP526. Se definen dos conceptos claves:\n\nEscritura din√°mica: no se especifican los atributos de los inputs ni de los ouputs\nEscritura est√°tica: se especifican los atributos de los inputs y los ouputs\n\n\n#\n# escritura din√°mica\ndef suma(x,y):\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\n\n#\n# escritura estatica\ndef suma(x:float,\n         y:float)->float:\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\nPara la escritura est√°tica, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la funci√≥n puede recibir otros tipos de atributos.\n\nprint(suma(\"hola\",\" mundo\"))\n\nhola mundo\n\n\nPara validar los tipos de datos son los correctos, se deben ocupar librer√≠as especializadas en la validaci√≥n de datos (por ejemplo: pydantic).\n\n\n\nExisten librer√≠as que pueden ayudar a corregir errores de escrituras en t√∫ c√≥digo (tambi√©n conocido como An√°lisis Est√°tico), por ejemplo:\n\nblack: El formateador de c√≥digo inflexible.\nflake8: La herramienta para aplicar la gu√≠a de estilo PEP8.\nmypy: Mypy es un verificador de tipo est√°tico para Python 3.\n\n\n\n\n\nCasi tan importante como la escritura de c√≥digo, es su correcta documentaci√≥n, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el c√≥digo explicando c√≥mo funciona, el elemento b√°sico de documentaci√≥n de Python es el Docstring o cadena de documentaci√≥n, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo despu√©s de la definici√≥n de funci√≥n o clase que sirve de documentaci√≥n a ese elemento.\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n\n\n# Acceso a la documentaci√≥n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n    '\n\n\n\n# Acceso a la documentaci√≥n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n\n\nLo correcto es detallar lo mejor posible en el Docstring qu√© hace y c√≥mo se usa la funci√≥n o clase y los par√°metros que necesita. Se recomienda usar el estilo de documentaci√≥n del software de documentaci√≥n sphinx, que emplea reStructuredText como lenguaje de marcado.\nVeamos un ejemplo de una funci√≥n bien documentada:\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    >>> potencia(2, 1)\n    2\n    >>> potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n\n\n# Acceso a la documentaci√≥n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    >>> potencia(2, 1)\\n    2\\n    >>> potencia(3, 2)\\n    9\\n    '\n\n\n\n# Acceso a la documentaci√≥n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    >>> potencia(2, 1)\n    2\n    >>> potencia(3, 2)\n    9\n\n\n\nExisten varias formas de documentar tus funciones, las principales encontradas en la literatura son: * Google docstrings: forma de documentaci√≥n recomendada por Google.. * reStructured Text: est√°ndar oficial de documentaci√≥n de Python; No es apto para principiantes, pero tiene muchas funciones. * NumPy/SciPy docstrings: combinaci√≥n de NumPy de reStructured y Google Docstrings.\n\n\n\nEl Zen de Python te dar√° la gu√≠a para decidir sobre que hacer con tu c√≥digo, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.\nPrincipios importantes:\n\nExpl√≠cito es mejor que impl√≠cito: Que no se asuma nada, aseg√∫rate que las cosas sean.\nSimple es mejor que complejo: Evita c√≥digo complejo, c√≥digo espagueti o que hace mas cosas para poder hacer una simple tarea.\nPlano es mejor que anidado: Si tu c√≥digo tiene mas de 3 niveles de identaci√≥n, deber√≠as mover parte de ese c√≥digo a una funci√≥n.\nLos errores nunca deber√≠an pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que impl√≠cito.\nSi la implementaci√≥n es dif√≠cil de explicar, es mala idea.\n\nTambi√©n, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n\n\nLos consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programaci√≥n al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.\n\n\nPython al ser multiparadigma, nos da una amplia gama de posibilidades de dise√±ar nuestros c√≥digos. Dentro de estos se destacan:\n\nProgramaci√≥n orientada a objetos (OOP)\nProgramaci√≥n funcional\n\nCu√°ndo ocupar uno o la otra, va a depender de c√≥mo queremos abordar una determinada problem√°tica, puesto que en la mayor√≠a de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).\n\n\n\nEn ingenier√≠a de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acr√≥nimo mnem√≥nico introducido por Robert C. Martin a comienzos de la d√©cada del 2000 que representa cinco principios b√°sicos de la programaci√≥n orientada a objetos y el dise√±o. Cuando estos principios se aplican en conjunto es m√°s probable que un desarrollador cree un sistema que sea f√°cil de mantener y ampliar con el tiempo.\nEn el siguiente link se deja una gu√≠a para poder entender estos conceptos en python.\n\n\n\nLos patrones de dise√±o son la base para la b√∫squeda de soluciones a problemas comunes en el desarrollo de software y otros √°mbitos referentes al dise√±o de interacci√≥n o interfaces.\n\nUn patr√≥n de dise√±o es una soluci√≥n a un problema de dise√±o.\n\nSe destacan tres tipos de patrones de dise√±os:\n\nComportamiento\nCreacionales\nEstructurales\n\nEn el siguiente link se deja una gu√≠a para poder entender estos conceptos en python.\n\n\n\n\n\nThe Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)\nClean Code: A Handbook of Agile Software - Robert C. Martin (2009).\nWorking effectively with legacy code Michael C. Feathers (2004)\nRefactoring Martin Fowler (1999)\nThe Pragmatic Programmer Thomas Hunt (1999)"
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html",
    "href": "posts/2022/2021-07-15-tdd.html",
    "title": "Test Driven Development",
    "section": "",
    "text": "Esta secci√≥n busca dar se√±ales de c√≥mo abordar el desarrollo de software para Data Science usando Test Driven Development, una t√©cnica ampliamente usada en otros rubros de la programaci√≥n.\n\n\nEn palabras simples, el desarrollo guiado por pruebas pone las pruebas en el coraz√≥n de nuestro trabajo. En su forma m√°s simple consiste en un proceso iterativo de 3 fases:\n\n\nRed: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla\nGreen: Escribe el c√≥digo m√≠nimo necesario para pasar ese test\nRefactor: Refactoriza de ser necesario\n\n\n\n\nA modo de ejemplo, vamos a testear la funci√≥n paridad, que determina si un n√∫mero natural es par o no.\nLo primero que se debe hacer es crear el test, para ello se ocupar√° la librer√≠a pytest.\n\nNota: No es necesario conocer previamente la librer√≠a pytest para entender el ejemplo.\n\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\nEl test nos dice que si el input es el n√∫mero 2, la funci√≥n paridad devuelve el output 'par'. C√≥mo a√∫n no hemos escrito la funci√≥n, el test fallar√° (fase red).\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\nAhora, se escribe la funci√≥n paridad (fase green):\ndef paridad(n:int)->str:\n    \"\"\"\n    Determina si un numero natural es par o no.\n    \n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\nVolvemos a correr el test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\nHemos cometido un descuido a proposito, no hemos testeado el caso si el n√∫mero fuese impar, por lo cual reescribimos el test (fase refactor)\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\ny corremos nuevamente los test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\nListo, nuestra funci√≥n paridad ha sido testeado correctamente!.\n\n\n\n\nExisten varias razones por las que uno deber√≠a usar TDD. Entre ellas podemos encontrar: - Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los l√≠mites del problema y c√≥mo podemos resolverlo. Con el tiempo esto ayuda a obtener un dise√±o modular y reusable del c√≥digo. - Escribir tests ayuda la forma en que escribimos c√≥digo, haci√©ndolo m√°s legible a otros. Sin embargo, no es un acto de altruismo, la mayor√≠a de las veces ese otro es tu futuro yo. - Verifica que el c√≥digo funciona de la manera que se espera, y lo hace de forma autom√°tica. - Te permite realizar refactoring con la certeza de que no has roto nada. - Los tests escritos sirven como documentaci√≥n para otros desarrolladores. - Es una pr√°ctica requerida en metodolog√≠as de desarrollo de software agile.\n\n\n\nEl 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de c√≥digo hasta las 155k. Estas son parte de sus conclusiones:\n\nTodos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.\n\nComo todo en la vida, nada es gratis:\n\nIncremento del tiempo de desarrollo var√≠a entre un 15% a 35%.\n\nSin embargo\n\nDesde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenci√≥n reducidos debido al incremento en calidad.\n\nAdem√°s, es importante escribir tests junto con la implementaci√≥n en peque√±as iteraciones. George y Williams encontraron que escribir tests despu√©s de que la aplicaci√≥n est√° mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y adem√°s la aplicaci√≥n se vuelve menos testeable. Otra conclusi√≥n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un dise√±o m√°s simple.\n\n\n\nNo, pero puedes usarlo casi siempre. El an√°lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaci√≥n v√≠a testing.\n\n\n\nAc√° listamos algunas librer√≠as de TDD en Python: - unittest: M√≥dulo dentro de la librer√≠a est√°ndar de Python. Permite realizar tests unitarios, de integraci√≥n y end to end. - doctest: Permite realizar test de la documentaci√≥n del c√≥digo (ejemplos: Numpy o Pandas). - pytest: Librer√≠a de testing ampliamente usada en proyectos nuevos de Python. - nose: Librer√≠a que extiende unittest para hacerlo m√°s simple. - coverage: Herramienta para medir la cobertura de c√≥digo de los proyectos. - tox: Herramienta para facilitar el test de una librer√≠a en diferentes versiones e int√©rpretes de Python. - hypothesis: Librer√≠a para escribir tests v√≠a reglas que ayuda a encontrar casos borde. - behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD.\n\n\n\n\nRealizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.\nGoogle Testing Blog: Poseen varios art√≠culos sobre c√≥mo abordar problemas tipo, buenas pr√°cticas de dise√±o para generar c√≥digo testeable, entre otros. En particular destaca la serie Testing on the Toilet.\nCualquier art√≠culo de Martin Fowler sobre testing, empezando por √©ste\nDesign Patterns: Los patrones de dise√±o de software tienen en consideraci√≥n que el c√≥digo sea testeable."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html",
    "href": "posts/2022/2022-10-12-causal_impact.html",
    "title": "Causal Impact",
    "section": "",
    "text": "El paquete CausalImpact creado por Google estima el impacto de una intervenci√≥n en una serie temporal. Por ejemplo, ¬øc√≥mo afecta una nueva funci√≥n en una aplicaci√≥n el tiempo de los usuarios en la aplicaci√≥n?\nEn este tutorial, hablaremos sobre c√≥mo usar el paquete de Python CausalImpact para hacer inferencias causales de series de tiempo. Aprender√°s: * ¬øC√≥mo establecer los per√≠odos previo y posterior para el an√°lisis de impacto causal? * ¬øC√≥mo realizar inferencias causales sobre datos de series temporales? * ¬øC√≥mo resumir los resultados del an√°lisis de causalidad y crear un informe? * ¬øCu√°les son las diferencias entre los paquetes python y R para CausalImpact?\n\n\n\nEn primer lugar, instalemos pycausalimpac para el an√°lisis causal de series de tiempo.\n\n# Install python version of causal impact\n#!pip install pycausalimpact\n\nUna vez completada la instalaci√≥n, podemos importar las bibliotecas. * pandas, numpy y datetime se importan para el procesamiento de datos. * ArmaProcess se importa para la creaci√≥n de datos de series temporales sint√©ticas. * matplotlib y seaborn son para visualizaci√≥n. * CausalImpact es para la estimaci√≥n de los efectos del tratamiento de series de tiempo.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact\n\n\n\n\nCrearemos un conjunto de datos de series de tiempo sint√©tico para el an√°lisis de impacto causal. El beneficio de usar un conjunto de datos sint√©tico es que podemos validar la precisi√≥n de los resultados del modelo.\nEl paquete CausalImpact requiere dos tipos de series temporales: * Una serie temporal de respuesta que se ve directamente afectada por la intervenci√≥n. * Y una o m√°s series temporales de control que no se ven afectadas por la intervenci√≥n.\nLa idea es construir un modelo de serie de tiempo para predecir el resultado contraf√°ctico. En otras palabras, el modelo utilizar√° la serie temporal de control para predecir cu√°l habr√≠a sido el resultado de la serie temporal de respuesta si no hubiera habido intervenci√≥n.\nEn este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control. * Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del c√≥digo. * Luego se crea un proceso de promedio m√≥vil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media m√≥vil (MA) tiene dos coeficientes 0,6 y 0,3. * Despu√©s de crear el proceso de media m√≥vil autorregresiva (ARMA), se generan 500 muestras a partir del proceso. * La variable de serie temporal de control X se crea a√±adiendo un valor constante de 10 a los valores generados. * La variable de serie temporal de respuesta y es una funci√≥n de la variable de serie temporal de control X. Es igual a 2 veces X m√°s un valor aleatorio. * La intervenci√≥n ocurre en el √≠ndice de 300, y el verdadero impacto causal es 10.\n\n# Set up a seed for reproducibility\nnp.random.seed(42)\n\n# Autoregressive coefficients\narparams = np.array([.95, .05])\n\n# Moving average coefficients\nmaparams = np.array([.6, .3])\n\n# Create a ARMA process\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Create the control time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Create the response time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Add the true causal impact\ny[300:] += 10\n\nUna serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la funci√≥n pandas date_range, lo que indica que el conjunto de datos tiene datos diarios.\nDespu√©s de eso, se crea un marco de datos de pandas con la variable de control X, la variable de respuesta es y y las dates como √≠ndice.\n\n# Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n  \n    \n      \n      y\n      X\n    \n    \n      dates\n      \n      \n    \n  \n  \n    \n      2021-01-01\n      21.919606\n      10.496714\n    \n    \n      2021-01-02\n      23.172702\n      10.631643\n    \n    \n      2021-01-03\n      21.278713\n      11.338640\n    \n    \n      2021-01-04\n      26.909878\n      13.173454\n    \n    \n      2021-01-05\n      27.260727\n      13.955685\n    \n  \n\n\n\n\n\n\n\nEstableceremos los periodos de pre y post intervenci√≥n. Usando df.index, podemos ver que la fecha de inicio de la serie temporal es 2021-01-01, la fecha de finalizaci√≥n de la serie temporal es 2022-05-15 y la fecha de inicio del tratamiento es 2021- 10-28.\n\n# Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n\nThe time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n\n\nA continuaci√≥n, visualicemos los datos de la serie temporal.\n\n# Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n\n\n\n\nEn el gr√°fico, la l√≠nea azul es la serie temporal de control, la l√≠nea naranja es la serie temporal de respuesta y la l√≠nea vertical roja representa la fecha de inicio de la intervenci√≥n.\nPodemos ver que antes de la intervenci√≥n, las series temporales de control y respuesta tienen valores similares. Despu√©s de la intervenci√≥n, la serie de tiempo de respuesta tiene consistentemente valores m√°s altos que la serie de tiempo de control.\nEl paquete CausalImpact de python requiere las entradas de los per√≠odos anterior y posterior en un formato de lista. El primer elemento de la lista es el √≠ndice inicial y el √∫ltimo elemento de la lista es el √≠ndice final.\nLa fecha de inicio de la intervenci√≥n es 2021-10-28, por lo que el per√≠odo previo finaliza en 2021-10-27.\n\n# Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n\nThe pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']\n\n\n\n\n\nCalcularemos la diferencia bruta entre los per√≠odos previo y posterior.\nPodemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.\nSin an√°lisis de causalidad, sobreestimaremos el impacto causal.\n\n# Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n\nThe pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855.\n\n\n\n\n\nejecutaremos el an√°lisis de impacto causal sobre la serie temporal.\nEl an√°lisis de causalidad tiene dos supuestos: * Supuesto 1: Hay una o m√°s series temporales de control que est√°n altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervenci√≥n. La violaci√≥n de esta suposici√≥n puede dar lugar a conclusiones err√≥neas sobre la existencia, la direcci√≥n o la magnitud del efecto del tratamiento. * Supuesto 2: La correlaci√≥n entre el control y la serie temporal de respuesta es la misma para antes y despu√©s de la intervenci√≥n.\nLos datos de series de tiempo sint√©ticos que creamos satisfacen las dos suposiciones.\nEl paquete CausalImpact de python tiene una funci√≥n llamada CausalImpact que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas: * data toma el nombre del dataframe de python. * pre_period toma los valores de √≠ndice inicial y final para el per√≠odo previo a la intervenci√≥n. * post_period toma los valores de √≠ndice inicial y final para el per√≠odo posterior a la intervenci√≥n.\nDespu√©s de guardar el objeto de salida en una variable llamada impact, podemos ejecutar impact.plot() para visualizar los resultados.\n\n# Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\nLa visualizaci√≥n consta de tres gr√°ficos: * El primer gr√°fico traza los valores contrafactuales pronosticados y los valores reales para el per√≠odo posterior. * El segundo gr√°fico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al per√≠odo est√°n alrededor de 0, y los valores de los efectos de puntos posteriores al per√≠odo est√°n alrededor del impacto real de 10. * El tercer gr√°fico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo gr√°fico.\n\n\n\nResumiremos el impacto causal de la intervenci√≥n para la serie temporal.\nEl resumen de impact.summary() nos dice que: * El promedio posterior a la intervenci√≥n real es 50,08 y el promedio posterior a la intervenci√≥n pronosticado es 40,3. * El efecto causal absoluto es 10,06, que est√° muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7. * El efecto causal relativo es del 25,12%. * La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.\n\n# Causal impact summary\nprint(impact.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\nPodemos imprimir la versi√≥n del informe del resumen usando la opci√≥n output='report'.\n\n# Causal impact report\nprint(impact.summary(output='report'))\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant.\n\n\n\n\n\nHablaremos sobre las diferencias del paquete CausalImpact de Google entre Python y R.\nEl paquete python fue portado desde el paquete R, por lo que la mayor√≠a de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.\nLas diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimizaci√≥n y los algoritmos de implementaci√≥n.\nLa documentaci√≥n del paquete pycausalimpact recomienda encarecidamente establecer prior_level_sd en Ninguno, lo que permitir√° que statsmodel realice la optimizaci√≥n para el componente anterior en el nivel local.\nEn base a esta sugerencia, se crea una versi√≥n con la opci√≥n prior_level_sd=None.\n\n# Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\nPodemos ver que los valores de estimaci√≥n puntual son similares, pero las desviaciones est√°ndar son m√°s peque√±as para la estimaci√≥n.\n\n# Print out the summary\nprint(impact_no_prior_level_sd.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\n\n\n\nPara aprender a ajustar los hiperpar√°metros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python\n\n\n\n\nInferring causal impact using Bayesian structural time-series models.\nTime Series Causal Impact Analysis in Python | Machine Learning.\nInferring the effect of an event using CausalImpact by Kay Brodersen."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html",
    "href": "posts/2022/2022-10-12-implicit.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendaci√≥n que utiliza la similitud del usuario para hacer recomendaciones de productos.\nEn este tutorial, hablaremos sobre * ¬øQu√© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¬øC√≥mo crear una matriz usuario-producto? * ¬øC√≥mo procesar los datos para el filtrado colaborativo basado en el usuario? * ¬øC√≥mo identificar usuarios similares? * ¬øC√≥mo reducir el grupo de elementos? * ¬øC√≥mo clasificar los art√≠culos para la recomendaci√≥n? * ¬øC√≥mo predecir la puntuaci√≥n de calificaci√≥n?\n\nNota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial.\n\n\n\n\nEn primer lugar, comprendamos c√≥mo funciona el filtrado colaborativo basado en usuarios.\nEl filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposici√≥n detr√°s del algoritmo es que a usuarios similares les gustan productos similares.\nEl algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:\n\nEncuentre usuarios similares en funci√≥n de las interacciones con elementos comunes.\nIdentifique los elementos con una calificaci√≥n alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de inter√©s.\nCalcular la puntuaci√≥n media ponderada de cada elemento.\nClasifique los elementos seg√∫n la puntuaci√≥n y elija los n mejores elementos para recomendar.\n\n\n\n\nimage.png\n\n\nEste gr√°fico ilustra c√≥mo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado. * A la Sra. Blond le gustan las manzanas, las sand√≠as y las pi√±as. A la Sra. Black le gusta la sand√≠a y la pi√±a. A la Sra. P√∫rpura le gustan las sand√≠as y las uvas. * Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sand√≠as como las pi√±as, consideramos que las sand√≠as y las pi√±as son art√≠culos similares. * Dado que a la Sra. P√∫rpura le gustan las sand√≠as y a√∫n no ha estado expuesta a la pi√±a, el sistema de recomendaci√≥n recomienda la pi√±a a la Sra. P√∫rpura.\n\n\n\nEn el primer paso, importaremos las bibliotecas de Python pandas, numpy y scipy.stats. Estas tres bibliotecas son para procesamiento de datos y c√°lculos.\nTambi√©n importamos seaborn para la visualizaci√≥n y cosine_similarity para calcular el puntaje de similitud.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n\n\nEste tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de pel√≠culas.\nSeguiremos los pasos a continuaci√≥n para obtener los conjuntos de datos: 1. Vaya a https://grouplens.org/datasets/movielens/ 2. Descargue el conjunto de datos de 100k con el nombre de archivo ‚Äúml-latest-small.zip‚Äù 3. Descomprima ‚Äúml-latest-small.zip‚Äù 4. Copie la carpeta ‚Äúml-latest-small‚Äù en la carpeta de su proyecto\nHay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y pel√≠culas. Ahora vamos a leer en los datos de calificaci√≥n (ratings.csv).\n\n# Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n\n\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n    \n  \n  \n    \n      0\n      1\n      1\n      4.0\n      964982703\n    \n    \n      1\n      1\n      3\n      4.0\n      964981247\n    \n    \n      2\n      1\n      6\n      4.0\n      964982224\n    \n    \n      3\n      1\n      47\n      5.0\n      964983815\n    \n    \n      4\n      1\n      50\n      5.0\n      964982931\n    \n  \n\n\n\n\nHay cuatro columnas en el conjunto de datos de calificaciones:\n\nuserID\nmovieID\nrating\ntimestamp\n\nEl conjunto de datos tiene m√°s de 100 000 registros y no falta ning√∫n dato.\n\n# Get the dataset information\nratings.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n\n\nLas calificaciones de 100k son de 610 usuarios en 9724 pel√≠culas. La calificaci√≥n tiene diez valores √∫nicos de 0.5 a 5.\n\n# Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n\nThe ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\nA continuaci√≥n, leamos los datos de las pel√≠culas para obtener los nombres de las pel√≠culas (movies.csv).\nEl conjunto de datos de pel√≠culas tiene: * movieId * title * genres\n\n# Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n\n\n\n\n\n  \n    \n      \n      movieId\n      title\n      genres\n    \n  \n  \n    \n      0\n      1\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      1\n      2\n      Jumanji (1995)\n      Adventure|Children|Fantasy\n    \n    \n      2\n      3\n      Grumpier Old Men (1995)\n      Comedy|Romance\n    \n    \n      3\n      4\n      Waiting to Exhale (1995)\n      Comedy|Drama|Romance\n    \n    \n      4\n      5\n      Father of the Bride Part II (1995)\n      Comedy\n    \n  \n\n\n\n\nUsando movieID como clave coincidente, agregamos informaci√≥n de la pel√≠cula al conjunto de datos de calificaci√≥n y lo llamamos df. ¬°As√≠ que ahora tenemos el t√≠tulo de la pel√≠cula y la calificaci√≥n de la pel√≠cula en el mismo conjunto de datos!\n\n# Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n  \n    \n      \n      userId\n      movieId\n      rating\n      timestamp\n      title\n      genres\n    \n  \n  \n    \n      0\n      1\n      1\n      4.0\n      964982703\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      1\n      5\n      1\n      4.0\n      847434962\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      2\n      7\n      1\n      4.5\n      1106635946\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      3\n      15\n      1\n      2.5\n      1510577970\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n    \n      4\n      17\n      1\n      4.5\n      1305696483\n      Toy Story (1995)\n      Adventure|Animation|Children|Comedy|Fantasy\n    \n  \n\n\n\n\n\n\n\nDebemos filtrar las pel√≠culas y mantener solo aquellas con m√°s de 100 calificaciones para el an√°lisis. Esto es para que el c√°lculo sea manejable por la memoria de Google Colab.\nPara hacerlo, primero agrupamos las pel√≠culas por t√≠tulo, contamos el n√∫mero de calificaciones y mantenemos solo las pel√≠culas con m√°s de 100 calificaciones.\nLas calificaciones promedio de las pel√≠culas tambi√©n se calculan.\nDesde la salida .info(), podemos ver que quedan 134 pel√≠culas.\n\n# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']>100]\nagg_ratings_GT100.info()  \n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n\n\nVeamos cu√°les son las pel√≠culas m√°s populares y sus calificaciones.\n\n# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n\n\n\n\n\n  \n    \n      \n      title\n      mean_rating\n      number_of_ratings\n    \n  \n  \n    \n      3158\n      Forrest Gump (1994)\n      4.164134\n      329\n    \n    \n      7593\n      Shawshank Redemption, The (1994)\n      4.429022\n      317\n    \n    \n      6865\n      Pulp Fiction (1994)\n      4.197068\n      307\n    \n    \n      7680\n      Silence of the Lambs, The (1991)\n      4.161290\n      279\n    \n    \n      5512\n      Matrix, The (1999)\n      4.192446\n      278\n    \n  \n\n\n\n\nA continuaci√≥n, usemos un jointplot para verificar la correlaci√≥n entre la calificaci√≥n promedio y el n√∫mero de calificaciones.\nPodemos ver una tendencia ascendente en el diagrama de dispersi√≥n, que muestra que las pel√≠culas populares obtienen calificaciones m√°s altas.\nLa distribuci√≥n de calificaci√≥n promedio muestra que la mayor√≠a de las pel√≠culas en el conjunto de datos tienen una calificaci√≥n promedio de alrededor de 4.\nEl n√∫mero de distribuci√≥n de calificaciones muestra que la mayor√≠a de las pel√≠culas tienen menos de 150 calificaciones.\n\n# Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n\n<seaborn.axisgrid.JointGrid at 0x263dc2b6070>\n\n\n\n\n\nPara mantener solo las 134 pel√≠culas con m√°s de 100 calificaciones, debemos unir la pel√≠cula con el dataframe del nivel de calificaci√≥n del usuario.\nhow='inner' y on='title' aseguran que solo se incluyan las pel√≠culas con m√°s de 100 calificaciones.\n\n# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDespu√©s de filtrar las pel√≠culas con m√°s de 100 calificaciones, tenemos 597 usuarios que calificaron 134 pel√≠culas.\n\n# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n\nThe ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\n\n\n\nTransformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son pel√≠culas. El valor de la matriz es la calificaci√≥n de usuario de la pel√≠cula si hay una calificaci√≥n. De lo contrario, muestra NaN.\n\n# Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n\n\n\n\n\n  \n    \n      title\n      2001: A Space Odyssey (1968)\n      Ace Ventura: Pet Detective (1994)\n      Aladdin (1992)\n      Alien (1979)\n      Aliens (1986)\n      Amelie (Fabuleux destin d'Am√©lie Poulain, Le) (2001)\n      American Beauty (1999)\n      American History X (1998)\n      American Pie (1999)\n      Apocalypse Now (1979)\n      ...\n      True Lies (1994)\n      Truman Show, The (1998)\n      Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n      Twister (1996)\n      Up (2009)\n      Usual Suspects, The (1995)\n      WALL¬∑E (2008)\n      Waterworld (1995)\n      Willy Wonka & the Chocolate Factory (1971)\n      X-Men (2000)\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      5.0\n      5.0\n      NaN\n      4.0\n      ...\n      NaN\n      NaN\n      NaN\n      3.0\n      NaN\n      5.0\n      NaN\n      NaN\n      5.0\n      5.0\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      5.0\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n    \n    \n      5\n      NaN\n      3.0\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      2.0\n      NaN\n      NaN\n      NaN\n      NaN\n      4.0\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows √ó 134 columns\n\n\n\n\n\n\nDado que algunas personas tienden a dar una calificaci√≥n m√°s alta que otras, normalizamos la calificaci√≥n extrayendo la calificaci√≥n promedio de cada usuario.\nDespu√©s de la normalizaci√≥n, las pel√≠culas con una calificaci√≥n inferior a la calificaci√≥n promedio del usuario obtienen un valor negativo y las pel√≠culas con una calificaci√≥n superior a la calificaci√≥n promedio del usuario obtienen un valor positivo.\n\n# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n\n\n\n\n\n  \n    \n      title\n      2001: A Space Odyssey (1968)\n      Ace Ventura: Pet Detective (1994)\n      Aladdin (1992)\n      Alien (1979)\n      Aliens (1986)\n      Amelie (Fabuleux destin d'Am√©lie Poulain, Le) (2001)\n      American Beauty (1999)\n      American History X (1998)\n      American Pie (1999)\n      Apocalypse Now (1979)\n      ...\n      True Lies (1994)\n      Truman Show, The (1998)\n      Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n      Twister (1996)\n      Up (2009)\n      Usual Suspects, The (1995)\n      WALL¬∑E (2008)\n      Waterworld (1995)\n      Willy Wonka & the Chocolate Factory (1971)\n      X-Men (2000)\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      NaN\n      NaN\n      NaN\n      -0.392857\n      NaN\n      NaN\n      0.607143\n      0.607143\n      NaN\n      -0.392857\n      ...\n      NaN\n      NaN\n      NaN\n      -1.392857\n      NaN\n      0.607143\n      NaN\n      NaN\n      0.607143\n      0.607143\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      0.617647\n      NaN\n      NaN\n      NaN\n      1.617647\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      -1.382353\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.617647\n      NaN\n    \n    \n      5\n      NaN\n      -0.461538\n      0.538462\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      -1.461538\n      NaN\n      NaN\n      NaN\n      NaN\n      0.538462\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n5 rows √ó 134 columns\n\n\n\n\n\n\nHay diferentes formas de medir las similitudes. La correlaci√≥n de Pearson y la similitud del coseno son dos m√©todos ampliamente utilizados.\nEn este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlaci√≥n de Pearson.\n\n# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n\n\n\n\n\n  \n    \n      userId\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      601\n      602\n      603\n      604\n      605\n      606\n      607\n      608\n      609\n      610\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      1.000000\n      NaN\n      NaN\n      0.391797\n      0.180151\n      -0.439941\n      -0.029894\n      0.464277\n      1.0\n      -0.037987\n      ...\n      0.091574\n      0.254514\n      0.101482\n      -0.500000\n      0.780020\n      0.303854\n      -0.012077\n      0.242309\n      -0.175412\n      0.071553\n    \n    \n      2\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.000000\n      ...\n      -0.583333\n      NaN\n      -1.000000\n      NaN\n      NaN\n      0.583333\n      NaN\n      -0.229416\n      NaN\n      0.765641\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      0.391797\n      NaN\n      NaN\n      1.000000\n      -0.394823\n      0.421927\n      0.704669\n      0.055442\n      NaN\n      0.360399\n      ...\n      -0.239325\n      0.562500\n      0.162301\n      -0.158114\n      0.905134\n      0.021898\n      -0.020659\n      -0.286872\n      NaN\n      -0.050868\n    \n    \n      5\n      0.180151\n      NaN\n      NaN\n      -0.394823\n      1.000000\n      -0.006888\n      0.328889\n      0.030168\n      NaN\n      -0.777714\n      ...\n      0.000000\n      0.231642\n      0.131108\n      0.068621\n      -0.245026\n      0.377341\n      0.228218\n      0.263139\n      0.384111\n      0.040582\n    \n  \n\n5 rows √ó 597 columns\n\n\n\nAquellos que est√©n interesados en usar la similitud del coseno pueden consultar este c√≥digo. Dado que cosine_similarity no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del c√°lculo.\n\n# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n\narray([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])\n\n\nAhora usemos el ID de usuario 1 como ejemplo para ilustrar c√≥mo encontrar usuarios similares.\nPrimero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el n√∫mero de usuarios similares.\n\n# Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n\n\n\n\n\n  \n    \n      userId\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      601\n      602\n      603\n      604\n      605\n      606\n      607\n      608\n      609\n      610\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2\n      NaN\n      1.0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      1.000000\n      ...\n      -0.583333\n      NaN\n      -1.000000\n      NaN\n      NaN\n      0.583333\n      NaN\n      -0.229416\n      NaN\n      0.765641\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      0.391797\n      NaN\n      NaN\n      1.000000\n      -0.394823\n      0.421927\n      0.704669\n      0.055442\n      NaN\n      0.360399\n      ...\n      -0.239325\n      0.562500\n      0.162301\n      -0.158114\n      0.905134\n      0.021898\n      -0.020659\n      -0.286872\n      NaN\n      -0.050868\n    \n    \n      5\n      0.180151\n      NaN\n      NaN\n      -0.394823\n      1.000000\n      -0.006888\n      0.328889\n      0.030168\n      NaN\n      -0.777714\n      ...\n      0.000000\n      0.231642\n      0.131108\n      0.068621\n      -0.245026\n      0.377341\n      0.228218\n      0.263139\n      0.384111\n      0.040582\n    \n    \n      6\n      -0.439941\n      NaN\n      NaN\n      0.421927\n      -0.006888\n      1.000000\n      0.000000\n      -0.127385\n      NaN\n      0.957427\n      ...\n      -0.292770\n      -0.030599\n      -0.123983\n      -0.176327\n      0.063861\n      -0.468008\n      0.541386\n      -0.337129\n      0.158255\n      -0.030567\n    \n  \n\n5 rows √ó 597 columns\n\n\n\nEn la matriz de similitud del usuario, los valores var√≠an de -1 a 1, donde -1 significa la preferencia de pel√≠cula opuesta y 1 significa la misma preferencia de pel√≠cula.\nn = 10 significa que nos gustar√≠a elegir los 10 usuarios m√°s similares para el ID de usuario 1.\nEl filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. Aqu√≠ configuramos user_similarity_threshold en 0,3, lo que significa que un usuario debe tener un coeficiente de correlaci√≥n de Pearson de al menos 0,3 para ser considerado como un usuario similar.\nDespu√©s de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del m√°s alto al m√°s bajo, luego imprimimos la ID de los usuarios m√°s similares y el valor de correlaci√≥n de Pearson.\n\n# Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]>user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n\nThe similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64\n\n\n\n\n\nReduciremos el grupo de art√≠culos haciendo lo siguiente:\n\nElimine las pel√≠culas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).\nGuarde solo las pel√≠culas que otros usuarios similares hayan visto.\n\nPara eliminar las pel√≠culas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.\n\n# Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n\n\n\n\n\n  \n    \n      title\n      Alien (1979)\n      American Beauty (1999)\n      American History X (1998)\n      Apocalypse Now (1979)\n      Back to the Future (1985)\n      Batman (1989)\n      Big Lebowski, The (1998)\n      Braveheart (1995)\n      Clear and Present Danger (1994)\n      Clerks (1994)\n      ...\n      Star Wars: Episode IV - A New Hope (1977)\n      Star Wars: Episode V - The Empire Strikes Back (1980)\n      Star Wars: Episode VI - Return of the Jedi (1983)\n      Stargate (1994)\n      Terminator, The (1984)\n      Toy Story (1995)\n      Twister (1996)\n      Usual Suspects, The (1995)\n      Willy Wonka & the Chocolate Factory (1971)\n      X-Men (2000)\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      -0.392857\n      0.607143\n      0.607143\n      -0.392857\n      0.607143\n      -0.392857\n      0.607143\n      -0.392857\n      -0.392857\n      -1.392857\n      ...\n      0.607143\n      0.607143\n      0.607143\n      -1.392857\n      0.607143\n      -0.392857\n      -1.392857\n      0.607143\n      0.607143\n      0.607143\n    \n  \n\n1 rows √ó 56 columns\n\n\n\nPara mantener solo las pel√≠culas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la pel√≠cula con todos los valores faltantes. Todo valor faltante para una pel√≠cula significa que ninguno de los usuarios similares ha visto la pel√≠cula.\n\n# Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n\n\n\n\n\n  \n    \n      title\n      Aladdin (1992)\n      Alien (1979)\n      Amelie (Fabuleux destin d'Am√©lie Poulain, Le) (2001)\n      Back to the Future (1985)\n      Batman Begins (2005)\n      Beautiful Mind, A (2001)\n      Beauty and the Beast (1991)\n      Blade Runner (1982)\n      Bourne Identity, The (2002)\n      Braveheart (1995)\n      ...\n      Shrek (2001)\n      Silence of the Lambs, The (1991)\n      Spider-Man (2002)\n      Star Wars: Episode I - The Phantom Menace (1999)\n      Terminator 2: Judgment Day (1991)\n      Titanic (1997)\n      Toy Story (1995)\n      Up (2009)\n      Usual Suspects, The (1995)\n      WALL¬∑E (2008)\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      9\n      NaN\n      NaN\n      NaN\n      0.333333\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      108\n      NaN\n      NaN\n      0.466667\n      0.466667\n      NaN\n      0.466667\n      NaN\n      0.466667\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      0.466667\n      NaN\n      NaN\n      -0.533333\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      154\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.214286\n      NaN\n      NaN\n    \n    \n      366\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.205882\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.205882\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.205882\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      401\n      -0.382353\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.382353\n      NaN\n      NaN\n      NaN\n      ...\n      0.117647\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.117647\n      0.617647\n      NaN\n      0.617647\n    \n    \n      502\n      NaN\n      -0.375\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      511\n      NaN\n      NaN\n      -0.653846\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      -1.153846\n      -0.653846\n      NaN\n      NaN\n      NaN\n      -0.153846\n      NaN\n      NaN\n    \n    \n      550\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.277778\n      0.222222\n      NaN\n      -0.277778\n    \n    \n      595\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      -0.333333\n      NaN\n      NaN\n      NaN\n      NaN\n      0.666667\n      NaN\n    \n    \n      598\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.888889\n      NaN\n      ...\n      -2.111111\n      -2.611111\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n10 rows √ó 62 columns\n\n\n\nA continuaci√≥n, eliminaremos las pel√≠culas que el usuario ID 1 vio de la lista de pel√≠culas de usuarios similares. errors='ignore' elimina columnas si existen sin dar un mensaje de error.\n\n# Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n\n\n\n\n\n  \n    \n      title\n      Aladdin (1992)\n      Amelie (Fabuleux destin d'Am√©lie Poulain, Le) (2001)\n      Batman Begins (2005)\n      Beautiful Mind, A (2001)\n      Beauty and the Beast (1991)\n      Blade Runner (1982)\n      Bourne Identity, The (2002)\n      Breakfast Club, The (1985)\n      Catch Me If You Can (2002)\n      Dark Knight, The (2008)\n      ...\n      Monsters, Inc. (2001)\n      Ocean's Eleven (2001)\n      Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n      Shawshank Redemption, The (1994)\n      Shrek (2001)\n      Spider-Man (2002)\n      Terminator 2: Judgment Day (1991)\n      Titanic (1997)\n      Up (2009)\n      WALL¬∑E (2008)\n    \n    \n      userId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      9\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      108\n      NaN\n      0.466667\n      NaN\n      0.466667\n      NaN\n      0.466667\n      NaN\n      -0.533333\n      0.466667\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.466667\n      NaN\n      -0.533333\n      NaN\n      NaN\n    \n    \n      154\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.214286\n      NaN\n    \n    \n      366\n      NaN\n      NaN\n      -0.205882\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.205882\n      ...\n      NaN\n      NaN\n      -0.205882\n      NaN\n      NaN\n      NaN\n      -0.205882\n      NaN\n      NaN\n      NaN\n    \n    \n      401\n      -0.382353\n      NaN\n      NaN\n      NaN\n      -0.382353\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      0.117647\n      NaN\n      0.117647\n      NaN\n      0.117647\n      NaN\n      NaN\n      NaN\n      0.617647\n      0.617647\n    \n    \n      502\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      0.125000\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      511\n      NaN\n      -0.653846\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      0.346154\n      NaN\n      -1.153846\n      NaN\n      NaN\n      -0.153846\n      NaN\n    \n    \n      550\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      -0.277778\n      -0.277778\n      ...\n      NaN\n      NaN\n      NaN\n      0.222222\n      NaN\n      NaN\n      NaN\n      NaN\n      0.222222\n      -0.277778\n    \n    \n      595\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      598\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      0.888889\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      0.888889\n      NaN\n      NaN\n      -2.111111\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n10 rows √ó 38 columns\n\n\n\n\n\n\nDecidiremos qu√© pel√≠cula recomendar al usuario objetivo. Los elementos recomendados est√°n determinados por el promedio ponderado del puntaje de similitud del usuario y la calificaci√≥n de la pel√≠cula. Las calificaciones de las pel√≠culas est√°n ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderaci√≥n.\nEste c√≥digo recorre los elementos y los usuarios para obtener la puntuaci√≥n del elemento, clasificar la puntuaci√≥n de mayor a menor y elegir las 10 mejores pel√≠culas para recomendar al ID de usuario 1.\n\n# A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n\n\n\n\n\n  \n    \n      \n      movie\n      movie_score\n    \n  \n  \n    \n      16\n      Harry Potter and the Chamber of Secrets (2002)\n      1.888889\n    \n    \n      13\n      Eternal Sunshine of the Spotless Mind (2004)\n      1.888889\n    \n    \n      6\n      Bourne Identity, The (2002)\n      0.888889\n    \n    \n      29\n      Ocean's Eleven (2001)\n      0.888889\n    \n    \n      18\n      Inception (2010)\n      0.587491\n    \n    \n      3\n      Beautiful Mind, A (2001)\n      0.466667\n    \n    \n      5\n      Blade Runner (1982)\n      0.466667\n    \n    \n      12\n      Donnie Darko (2001)\n      0.466667\n    \n    \n      10\n      Departed, The (2006)\n      0.256727\n    \n    \n      31\n      Shawshank Redemption, The (1994)\n      0.222566\n    \n  \n\n\n\n\n\n\n\nSi el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificaci√≥n del usuario, debemos sumar la calificaci√≥n promedio de la pel√≠cula del usuario a la calificaci√≥n de la pel√≠cula.\n\n# Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n\nThe average movie rating for user 1 is 4.39\n\n\nLa calificaci√≥n promedio de la pel√≠cula para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificaci√≥n de la pel√≠cula.\n\n# Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n\n\n\n\n\n  \n    \n      \n      movie\n      movie_score\n      predicted_rating\n    \n  \n  \n    \n      16\n      Harry Potter and the Chamber of Secrets (2002)\n      1.888889\n      6.281746\n    \n    \n      13\n      Eternal Sunshine of the Spotless Mind (2004)\n      1.888889\n      6.281746\n    \n    \n      6\n      Bourne Identity, The (2002)\n      0.888889\n      5.281746\n    \n    \n      29\n      Ocean's Eleven (2001)\n      0.888889\n      5.281746\n    \n    \n      18\n      Inception (2010)\n      0.587491\n      4.980348\n    \n    \n      3\n      Beautiful Mind, A (2001)\n      0.466667\n      4.859524\n    \n    \n      5\n      Blade Runner (1982)\n      0.466667\n      4.859524\n    \n    \n      12\n      Donnie Darko (2001)\n      0.466667\n      4.859524\n    \n    \n      10\n      Departed, The (2006)\n      0.256727\n      4.649584\n    \n    \n      31\n      Shawshank Redemption, The (1994)\n      0.222566\n      4.615423\n    \n  \n\n\n\n\nPodemos ver que las 10 mejores pel√≠culas recomendadas tienen calificaciones pronosticadas superiores a 4.5.\n\n\n\nEn este tutorial, analizamos c√≥mo crear un sistema de recomendaci√≥n de filtrado colaborativo basado en el usuario. Aprendiste * ¬øQu√© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * ¬øC√≥mo crear una matriz usuario-producto? * ¬øC√≥mo procesar los datos para el filtrado colaborativo basado en el usuario? * ¬øC√≥mo identificar usuarios similares? * ¬øC√≥mo reducir el grupo de elementos? * ¬øC√≥mo clasificar los art√≠culos para la recomendaci√≥n? * ¬øC√≥mo predecir la puntuaci√≥n de calificaci√≥n?\n\n\n\n\nUser-Based Collaborative Filtering.\nUser-Based Collaborative Filtering In Python | Machine Learning.\nCollaborative Filtering : Data Science Concepts."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html",
    "href": "posts/2022/2022-03-16-polars.html",
    "title": "Polars",
    "section": "",
    "text": "Polars es una librer√≠a de DataFrames incre√≠blemente r√°pida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.\n\nLazy | eager execution\nMulti-threaded\nSIMD (Single Instruction, Multiple Data)\nQuery optimization\nPowerful expression API\nRust | Python | ‚Ä¶\n\nEsta secci√≥n tiene como objetivos presentarle Polars a trav√©s de ejemplos y compar√°ndolo con otras soluciones.\n\nNota: Si usted no esta familiarizado con la manipulaci√≥n de datos en Python, se recomienda partir leyendo sobre la librer√≠a de Pandas. Tambi√©n, se deja como referencia el curso de Manipulaci√≥n de Datos.\n\n\n\n\n\n\nPara instalar Polars, necesitar√° usar la l√≠nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge polars\nDe lo contrario, puede instalar con pip:\npip install polars\n\nNota: Todos los binarios est√°n preconstruidos para Python v3.6+.\n\n\n\n\n\nPolars es muy r√°pido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta p√°gina tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de c√≥digo abierto. Se ejecuta regularmente con las √∫ltimas versiones de estos paquetes y se actualiza autom√°ticamente.\nTambi√©n se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si est√° realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tama√±o de sus datos.\nA modo de ejemplo, veamos algunos ejemplos de performances de distintas librer√≠as para ejecutar distintos tipos de tareas sobre datasets con distintos tama√±os. Para el caso de tareas b√°sicas sobre un dataset de 50 GB, Polars supera a librer√≠as espacializadas en distribuci√≥n de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librer√≠as conocidas en Python como Pandas o Dask se tiene el problema de out of memory.\n\n\n\n\nPolars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de Fn(Series) -> Series, lo que significa que tienen Series como entrada y Series como salida. Al observar esta definici√≥n funcional, podemos ver que la salida de un Expr tambi√©n puede servir como entrada de un Expr.\nEso puede sonar un poco extra√±o, as√≠ que vamos a dar un ejemplo.\nLa siguiente es una expresi√≥n:\npl.col(\"foo\").sort().head(2)\nEl fragmento anterior dice seleccionar la columna \"foo\", luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresi√≥n produce una nueva expresi√≥n y que se pueden canalizar juntas. Puede ejecutar una expresi√≥n pas√°ndola en uno de los contextos de ejecuci√≥n polares. Aqu√≠ ejecutamos dos expresiones ejecutando df.select:\ndf.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\nTodas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresi√≥n puede haber m√°s paralelizaci√≥n).\n\n\nEn esta secci√≥n veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:\n\nimport polars as pl\nimport numpy as np\n\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n\nshape: (5, 4)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ nrs  ‚îÜ names ‚îÜ random   ‚îÜ groups ‚îÇ\n‚îÇ ---  ‚îÜ ---   ‚îÜ ---      ‚îÜ ---    ‚îÇ\n‚îÇ i64  ‚îÜ str   ‚îÜ f64      ‚îÜ str    ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1    ‚îÜ foo   ‚îÜ 0.154163 ‚îÜ A      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 2    ‚îÜ ham   ‚îÜ 0.74     ‚îÜ A      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 3    ‚îÜ spam  ‚îÜ 0.263315 ‚îÜ B      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ null ‚îÜ egg   ‚îÜ 0.533739 ‚îÜ C      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 5    ‚îÜ null  ‚îÜ 0.014575 ‚îÜ B      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\nPuedes hacer mucho con las expresiones, veamos algunos ejemplos:\n\n\nPodemos contar los valores √∫nicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresi√≥n de alias, que cambia el nombre de una expresi√≥n.\n\nout = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n\nshape: (1, 2)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ unique_names_1 ‚îÜ unique_names_2 ‚îÇ\n‚îÇ ---            ‚îÜ ---            ‚îÇ\n‚îÇ u32            ‚îÜ u32            ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 5              ‚îÜ 5              ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\nPodemos hacer varias agregaciones. A continuaci√≥n mostramos algunas de ellas, pero hay m√°s, como median, mean, first, etc.\n\nout = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n\nshape: (1, 6)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ sum      ‚îÜ min      ‚îÜ max  ‚îÜ other_max ‚îÜ std dev  ‚îÜ variance ‚îÇ\n‚îÇ ---      ‚îÜ ---      ‚îÜ ---  ‚îÜ ---       ‚îÜ ---      ‚îÜ ---      ‚îÇ\n‚îÇ f64      ‚îÜ f64      ‚îÜ f64  ‚îÜ f64       ‚îÜ f64      ‚îÜ f64      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1.705842 ‚îÜ 0.014575 ‚îÜ 0.74 ‚îÜ 0.74      ‚îÜ 0.293209 ‚îÜ 0.085971 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nTambi√©n podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena \"am\".\n\nout = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n\nshape: (1, 1)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ names ‚îÇ\n‚îÇ ---   ‚îÇ\n‚îÇ u32   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 2     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nEn el ejemplo a continuaci√≥n, usamos un condicional para crear una nueva expresi√≥n when -> then -> otherwise.\nLa funci√≥n when() requiere una expresi√≥n de predicado (y, por lo tanto, conduce a una serie booleana), luego espera una expresi√≥n que se usar√° en caso de que el predicado se eval√∫e como verdadero y, de lo contrario, espera una expresi√≥n que se usar√° en caso de que el predicado se eval√∫e.\nTenga en cuenta que puede pasar cualquier expresi√≥n, o simplemente expresiones base como pl.col(\"foo\"), pl.lit(3), pl.lit(\"bar\"), etc.\nFinalmente, multiplicamos esto con el resultado de una expresi√≥n de suma.\n\nout = df.select(\n    [\n        pl.when(pl.col(\"random\") > 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n\nshape: (5, 1)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ literal  ‚îÇ\n‚îÇ ---      ‚îÇ\n‚îÇ f64      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1.695791 ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 0.0      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 2.896465 ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 0.0      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 0.160325 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nUna expresi√≥n polar tambi√©n puede hacer un GROUPBY, AGGREGATION y JOIN impl√≠citos en una sola expresi√≥n.\nEn los ejemplos a continuaci√≥n, hacemos un GROUPBY sobre \"groups\" y AGREGATE SUM de \"random\", y en la siguiente expresi√≥n GROUPBY OVER \"names\" y AGREGATE una lista de \"random\". Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estad√≠sticas de grupo. Vea m√°s expresiones en el siguiente link.\n\nout = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n\nshape: (5, 6)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ nrs  ‚îÜ names ‚îÜ random   ‚îÜ groups ‚îÜ sum[random]/groups ‚îÜ random/name ‚îÇ\n‚îÇ ---  ‚îÜ ---   ‚îÜ ---      ‚îÜ ---    ‚îÜ ---                ‚îÜ ---         ‚îÇ\n‚îÇ i64  ‚îÜ str   ‚îÜ f64      ‚îÜ str    ‚îÜ f64                ‚îÜ list [f64]  ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ 1    ‚îÜ foo   ‚îÜ 0.154163 ‚îÜ A      ‚îÜ 0.894213           ‚îÜ [0.154163]  ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 2    ‚îÜ ham   ‚îÜ 0.74     ‚îÜ A      ‚îÜ 0.894213           ‚îÜ [0.74]      ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 3    ‚îÜ spam  ‚îÜ 0.263315 ‚îÜ B      ‚îÜ 0.2778             ‚îÜ [0.263315]  ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ null ‚îÜ egg   ‚îÜ 0.533739 ‚îÜ C      ‚îÜ 0.533739           ‚îÜ [0.533739]  ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ 5    ‚îÜ null  ‚îÜ 0.014575 ‚îÜ B      ‚îÜ 0.2778             ‚îÜ [0.014575]  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\n\n\nUna de las formas m√°s eficientes de procesar datos tabulares es paralelizar su procesamiento a trav√©s del enfoque ‚Äúdividir-aplicar-combinar‚Äù. Esta operaci√≥n es el n√∫cleo de la implementaci√≥n del agrupamiento de Polars, lo que le permite lograr operaciones ultrarr√°pidas. M√°s espec√≠ficamente, las fases de ‚Äúdivisi√≥n‚Äù y ‚Äúaplicaci√≥n‚Äù se ejecutan de forma multiproceso.\nUna operaci√≥n de agrupaci√≥n simple se toma a continuaci√≥n como ejemplo para ilustrar este enfoque:\n\nPara las operaciones hash realizadas durante la fase de ‚Äúdivisi√≥n‚Äù, Polars utiliza un enfoque sin bloqueo de subprocesos m√∫ltiples que se ilustra en el siguiente esquema:\n\n¬°Esta paralelizaci√≥n permite que las operaciones de agrupaci√≥n y uni√≥n (por ejemplo) sean incre√≠blemente r√°pidas!\n\n\n\nTodos hemos escuchado que Python es lento y ‚Äúno escala‚Äù. Adem√°s de la sobrecarga de ejecutar el c√≥digo de bytes ‚Äúlento‚Äù, Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operaci√≥n lambda o una funci√≥n de Python personalizada para aplicar durante una fase de paralelizaci√≥n, la velocidad de Polars se limita al ejecutar el c√≥digo de Python, lo que evita que varios subprocesos ejecuten la funci√≥n.\nTodo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos lambda en un paso .groupby(), por ejemplo. Este enfoque a√∫n es compatible con Polars, pero teniendo en cuenta el c√≥digo de bytes Y el precio GIL deben pagarse.\nPara mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su lazy, sino tambi√©n en su uso eager.\n\n\n\nEn la introducci√≥n de la p√°gina anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelizaci√≥n y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.\nComencemos con el conjunto de datos simple del congreso de EE. UU.\n\nimport polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n\nshape: (5, 34)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ last_name ‚îÜ first_name ‚îÜ middle_name ‚îÜ suffix ‚îÜ ... ‚îÜ ballotpedia_id ‚îÜ washington_post_id ‚îÜ icpsr_id ‚îÜ wikipedia_id   ‚îÇ\n‚îÇ ---       ‚îÜ ---        ‚îÜ ---         ‚îÜ ---    ‚îÜ     ‚îÜ ---            ‚îÜ ---                ‚îÜ ---      ‚îÜ ---            ‚îÇ\n‚îÇ str       ‚îÜ str        ‚îÜ str         ‚îÜ str    ‚îÜ     ‚îÜ str            ‚îÜ str                ‚îÜ i64      ‚îÜ str            ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ Brown     ‚îÜ Sherrod    ‚îÜ null        ‚îÜ null   ‚îÜ ... ‚îÜ Sherrod Brown  ‚îÜ null               ‚îÜ 29389    ‚îÜ Sherrod Brown  ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Cantwell  ‚îÜ Maria      ‚îÜ null        ‚îÜ null   ‚îÜ ... ‚îÜ Maria Cantwell ‚îÜ null               ‚îÜ 39310    ‚îÜ Maria Cantwell ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Cardin    ‚îÜ Benjamin   ‚îÜ L.          ‚îÜ null   ‚îÜ ... ‚îÜ Ben Cardin     ‚îÜ null               ‚îÜ 15408    ‚îÜ Ben Cardin     ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Carper    ‚îÜ Thomas     ‚îÜ Richard     ‚îÜ null   ‚îÜ ... ‚îÜ Tom Carper     ‚îÜ null               ‚îÜ 15015    ‚îÜ Tom Carper     ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Casey     ‚îÜ Robert     ‚îÜ P.          ‚îÜ Jr.    ‚îÜ ... ‚îÜ Bob Casey, Jr. ‚îÜ null               ‚îÜ 40703    ‚îÜ Bob Casey Jr.  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nPuede combinar f√°cilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un l√≠mite superior en el n√∫mero de agregaciones que puede hacer y puede hacer cualquier combinaci√≥n que desee. En el fragmento a continuaci√≥n, hacemos las siguientes agregaciones:\nPor grupo \"first_name\":\n\ncuente el n√∫mero de filas en el grupo:\n\nforma abreviada: pl.count(\"party\")\nforma completa: pl.col(\"party\").count()\n\nagregue el grupo de valores de g√©nero a una lista:\n\nforma completa: pl.col(\"gender\").list()\n\nobtenga el primer valor de la columna \"last_name\" en el grupo:\n\nforma abreviada: pl.primero(\"last_name\")\nforma completa: pl.col(\"last_name\").first()\n\n\nAdem√°s de la agregaci√≥n, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.\n\nq = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 4)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ first_name ‚îÜ count ‚îÜ gender              ‚îÜ last_name ‚îÇ\n‚îÇ ---        ‚îÜ ---   ‚îÜ ---                 ‚îÜ ---       ‚îÇ\n‚îÇ str        ‚îÜ u32   ‚îÜ list [str]          ‚îÜ str       ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ John       ‚îÜ 19    ‚îÜ [\"M\", \"M\", ... \"M\"] ‚îÜ Barrasso  ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Mike       ‚îÜ 13    ‚îÜ [\"M\", \"M\", ... \"M\"] ‚îÜ Kelly     ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ Michael    ‚îÜ 11    ‚îÜ [\"M\", \"M\", ... \"M\"] ‚îÜ Bennet    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ David      ‚îÜ 11    ‚îÜ [\"M\", \"M\", ... \"M\"] ‚îÜ Cicilline ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ James      ‚îÜ 9     ‚îÜ [\"M\", \"M\", ... \"M\"] ‚îÜ Inhofe    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nOk, eso fue bastante f√°cil, ¬øverdad? Subamos un nivel. Digamos que queremos saber cu√°ntos delegados de un ‚Äúestado‚Äù (state) son administraci√≥n ‚ÄúDemocrat‚Äù o ‚ÄúRepublican‚Äù. Podr√≠amos consultarlo directamente en la agregaci√≥n sin la necesidad de lambda o arreglar el DataFrame.\n\nq = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ state ‚îÜ demo ‚îÜ repu ‚îÇ\n‚îÇ ---   ‚îÜ ---  ‚îÜ ---  ‚îÇ\n‚îÇ str   ‚îÜ u32  ‚îÜ u32  ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ CA    ‚îÜ 44   ‚îÜ 10   ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ NY    ‚îÜ 21   ‚îÜ 8    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ IL    ‚îÜ 15   ‚îÜ 5    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ TX    ‚îÜ 13   ‚îÜ 25   ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ NJ    ‚îÜ 12   ‚îÜ 2    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\nPor supuesto, tambi√©n se podr√≠a hacer algo similar con un GROUPBY anidado, pero eso no me permitir√≠a mostrar estas caracter√≠sticas agradables. üòâ\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ state ‚îÜ party      ‚îÜ count ‚îÇ\n‚îÇ ---   ‚îÜ ---        ‚îÜ ---   ‚îÇ\n‚îÇ str   ‚îÜ str        ‚îÜ u32   ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ CA    ‚îÜ Democrat   ‚îÜ 44    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ TX    ‚îÜ Republican ‚îÜ 25    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ NY    ‚îÜ Democrat   ‚îÜ 21    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ FL    ‚îÜ Republican ‚îÜ 18    ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ IL    ‚îÜ Democrat   ‚îÜ 15    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nTambi√©n podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del DataFrame (porque necesitamos esas filas para otra agregaci√≥n).\nEn el siguiente ejemplo, mostramos c√≥mo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos Polars expression, no aplicamos una funci√≥n personalizada sobre Series durante el tiempo de ejecuci√≥n de la consulta.\n\nfrom datetime import date\n\ndef compute_age() -> pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -> pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 5)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ state ‚îÜ avg M birthday ‚îÜ avg F birthday ‚îÜ # male ‚îÜ # female ‚îÇ\n‚îÇ ---   ‚îÜ ---            ‚îÜ ---            ‚îÜ ---    ‚îÜ ---      ‚îÇ\n‚îÇ str   ‚îÜ f64            ‚îÜ f64            ‚îÜ u32    ‚îÜ u32      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ MS    ‚îÜ 60.0           ‚îÜ 62.0           ‚îÜ 5      ‚îÜ 1        ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ NV    ‚îÜ 55.5           ‚îÜ 61.75          ‚îÜ 2      ‚îÜ 4        ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ KS    ‚îÜ 54.2           ‚îÜ 41.0           ‚îÜ 5      ‚îÜ 1        ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ IN    ‚îÜ 55.0           ‚îÜ 50.5           ‚îÜ 9      ‚îÜ 2        ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ IL    ‚îÜ 60.923077      ‚îÜ 58.428571      ‚îÜ 13     ‚îÜ 7        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\nA menudo veo que se ordena un DataFrame con el √∫nico prop√≥sito de ordenar durante la operaci√≥n GROUPBY. Digamos que queremos obtener los nombres de los pol√≠ticos m√°s antiguos y m√°s j√≥venes (no es que todav√≠a est√©n vivos) por estado, podr√≠amos ORDENAR y AGRUPAR.\n\ndef get_person() -> pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n\nshape: (5, 3)\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ state ‚îÜ youngest                 ‚îÜ oldest                   ‚îÇ\n‚îÇ ---   ‚îÜ ---                      ‚îÜ ---                      ‚îÇ\n‚îÇ str   ‚îÜ str                      ‚îÜ str                      ‚îÇ\n‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n‚îÇ PR    ‚îÜ Jenniffer Gonz√°lez-Col√≥n ‚îÜ Jenniffer Gonz√°lez-Col√≥n ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ ND    ‚îÜ John Hoeven              ‚îÜ Kelly Armstrong          ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ KY    ‚îÜ Harold Rogers            ‚îÜ Garland Barr             ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ NM    ‚îÜ Teresa Leger Fernandez   ‚îÜ Melanie Stansbury        ‚îÇ\n‚îú‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚îº‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚ïå‚î§\n‚îÇ OR    ‚îÜ Peter DeFazio            ‚îÜ Jeff Merkley             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n\n\n\n\n\n\n\nPolars - User Guide\nPolars - Github"
  }
]