[
  {
    "objectID": "posts/2022/2022-03-16-polars.html",
    "href": "posts/2022/2022-03-16-polars.html",
    "title": "Polars",
    "section": "",
    "text": "Polars es una librerÃ­a de DataFrames increÃ­blemente rÃ¡pida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.\n\nLazy | eager execution\nMulti-threaded\nSIMD (Single Instruction, Multiple Data)\nQuery optimization\nPowerful expression API\nRust | Python | â€¦\n\nEsta secciÃ³n tiene como objetivos presentarle Polars a travÃ©s de ejemplos y comparÃ¡ndolo con otras soluciones.\n\nNota: Si usted no esta familiarizado con la manipulaciÃ³n de datos en Python, se recomienda partir leyendo sobre la librerÃ­a de Pandas. TambiÃ©n, se deja como referencia el curso de ManipulaciÃ³n de Datos.\n\n\n\n\n\n\nPara instalar Polars, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge polars\nDe lo contrario, puede instalar con pip:\npip install polars\n\nNota: Todos los binarios estÃ¡n preconstruidos para Python v3.6+.\n\n\n\n\n\nPolars es muy rÃ¡pido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta pÃ¡gina tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de cÃ³digo abierto. Se ejecuta regularmente con las Ãºltimas versiones de estos paquetes y se actualiza automÃ¡ticamente.\nTambiÃ©n se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si estÃ¡ realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tamaÃ±o de sus datos.\nA modo de ejemplo, veamos algunos ejemplos de performances de distintas librerÃ­as para ejecutar distintos tipos de tareas sobre datasets con distintos tamaÃ±os. Para el caso de tareas bÃ¡sicas sobre un dataset de 50 GB, Polars supera a librerÃ­as espacializadas en distribuciÃ³n de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librerÃ­as conocidas en Python como Pandas o Dask se tiene el problema de out of memory.\n\n\n\n\nPolars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de Fn(Series) -&gt; Series, lo que significa que tienen Series como entrada y Series como salida. Al observar esta definiciÃ³n funcional, podemos ver que la salida de un Expr tambiÃ©n puede servir como entrada de un Expr.\nEso puede sonar un poco extraÃ±o, asÃ­ que vamos a dar un ejemplo.\nLa siguiente es una expresiÃ³n:\npl.col(\"foo\").sort().head(2)\nEl fragmento anterior dice seleccionar la columna \"foo\", luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresiÃ³n produce una nueva expresiÃ³n y que se pueden canalizar juntas. Puede ejecutar una expresiÃ³n pasÃ¡ndola en uno de los contextos de ejecuciÃ³n polares. AquÃ­ ejecutamos dos expresiones ejecutando df.select:\ndf.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\nTodas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresiÃ³n puede haber mÃ¡s paralelizaciÃ³n).\n\n\nEn esta secciÃ³n veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:\n\nimport polars as pl\nimport numpy as np\n\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n\nshape: (5, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ nrs  â”† names â”† random   â”† groups â”‚\nâ”‚ ---  â”† ---   â”† ---      â”† ---    â”‚\nâ”‚ i64  â”† str   â”† f64      â”† str    â”‚\nâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\nâ”‚ 1    â”† foo   â”† 0.154163 â”† A      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2    â”† ham   â”† 0.74     â”† A      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 3    â”† spam  â”† 0.263315 â”† B      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ null â”† egg   â”† 0.533739 â”† C      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 5    â”† null  â”† 0.014575 â”† B      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPuedes hacer mucho con las expresiones, veamos algunos ejemplos:\n\n\nPodemos contar los valores Ãºnicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresiÃ³n de alias, que cambia el nombre de una expresiÃ³n.\n\nout = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n\nshape: (1, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ unique_names_1 â”† unique_names_2 â”‚\nâ”‚ ---            â”† ---            â”‚\nâ”‚ u32            â”† u32            â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 5              â”† 5              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\nPodemos hacer varias agregaciones. A continuaciÃ³n mostramos algunas de ellas, pero hay mÃ¡s, como median, mean, first, etc.\n\nout = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n\nshape: (1, 6)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ sum      â”† min      â”† max  â”† other_max â”† std dev  â”† variance â”‚\nâ”‚ ---      â”† ---      â”† ---  â”† ---       â”† ---      â”† ---      â”‚\nâ”‚ f64      â”† f64      â”† f64  â”† f64       â”† f64      â”† f64      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1.705842 â”† 0.014575 â”† 0.74 â”† 0.74      â”† 0.293209 â”† 0.085971 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nTambiÃ©n podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena \"am\".\n\nout = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n\nshape: (1, 1)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names â”‚\nâ”‚ ---   â”‚\nâ”‚ u32   â”‚\nâ•â•â•â•â•â•â•â•â•¡\nâ”‚ 2     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nEn el ejemplo a continuaciÃ³n, usamos un condicional para crear una nueva expresiÃ³n when -&gt; then -&gt; otherwise.\nLa funciÃ³n when() requiere una expresiÃ³n de predicado (y, por lo tanto, conduce a una serie booleana), luego espera una expresiÃ³n que se usarÃ¡ en caso de que el predicado se evalÃºe como verdadero y, de lo contrario, espera una expresiÃ³n que se usarÃ¡ en caso de que el predicado se evalÃºe.\nTenga en cuenta que puede pasar cualquier expresiÃ³n, o simplemente expresiones base como pl.col(\"foo\"), pl.lit(3), pl.lit(\"bar\"), etc.\nFinalmente, multiplicamos esto con el resultado de una expresiÃ³n de suma.\n\nout = df.select(\n    [\n        pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n\nshape: (5, 1)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ literal  â”‚\nâ”‚ ---      â”‚\nâ”‚ f64      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1.695791 â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.0      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2.896465 â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.0      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.160325 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nUna expresiÃ³n polar tambiÃ©n puede hacer un GROUPBY, AGGREGATION y JOIN implÃ­citos en una sola expresiÃ³n.\nEn los ejemplos a continuaciÃ³n, hacemos un GROUPBY sobre \"groups\" y AGREGATE SUM de \"random\", y en la siguiente expresiÃ³n GROUPBY OVER \"names\" y AGREGATE una lista de \"random\". Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estadÃ­sticas de grupo. Vea mÃ¡s expresiones en el siguiente link.\n\nout = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n\nshape: (5, 6)\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ nrs  â”† names â”† random   â”† groups â”† sum[random]/groups â”† random/name â”‚\nâ”‚ ---  â”† ---   â”† ---      â”† ---    â”† ---                â”† ---         â”‚\nâ”‚ i64  â”† str   â”† f64      â”† str    â”† f64                â”† list [f64]  â”‚\nâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1    â”† foo   â”† 0.154163 â”† A      â”† 0.894213           â”† [0.154163]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2    â”† ham   â”† 0.74     â”† A      â”† 0.894213           â”† [0.74]      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 3    â”† spam  â”† 0.263315 â”† B      â”† 0.2778             â”† [0.263315]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ null â”† egg   â”† 0.533739 â”† C      â”† 0.533739           â”† [0.533739]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 5    â”† null  â”† 0.014575 â”† B      â”† 0.2778             â”† [0.014575]  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\nUna de las formas mÃ¡s eficientes de procesar datos tabulares es paralelizar su procesamiento a travÃ©s del enfoque â€œdividir-aplicar-combinarâ€. Esta operaciÃ³n es el nÃºcleo de la implementaciÃ³n del agrupamiento de Polars, lo que le permite lograr operaciones ultrarrÃ¡pidas. MÃ¡s especÃ­ficamente, las fases de â€œdivisiÃ³nâ€ y â€œaplicaciÃ³nâ€ se ejecutan de forma multiproceso.\nUna operaciÃ³n de agrupaciÃ³n simple se toma a continuaciÃ³n como ejemplo para ilustrar este enfoque:\n\nPara las operaciones hash realizadas durante la fase de â€œdivisiÃ³nâ€, Polars utiliza un enfoque sin bloqueo de subprocesos mÃºltiples que se ilustra en el siguiente esquema:\n\nÂ¡Esta paralelizaciÃ³n permite que las operaciones de agrupaciÃ³n y uniÃ³n (por ejemplo) sean increÃ­blemente rÃ¡pidas!\n\n\n\nTodos hemos escuchado que Python es lento y â€œno escalaâ€. AdemÃ¡s de la sobrecarga de ejecutar el cÃ³digo de bytes â€œlentoâ€, Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operaciÃ³n lambda o una funciÃ³n de Python personalizada para aplicar durante una fase de paralelizaciÃ³n, la velocidad de Polars se limita al ejecutar el cÃ³digo de Python, lo que evita que varios subprocesos ejecuten la funciÃ³n.\nTodo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos lambda en un paso .groupby(), por ejemplo. Este enfoque aÃºn es compatible con Polars, pero teniendo en cuenta el cÃ³digo de bytes Y el precio GIL deben pagarse.\nPara mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su lazy, sino tambiÃ©n en su uso eager.\n\n\n\nEn la introducciÃ³n de la pÃ¡gina anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelizaciÃ³n y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.\nComencemos con el conjunto de datos simple del congreso de EE. UU.\n\nimport polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n\nshape: (5, 34)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ last_name â”† first_name â”† middle_name â”† suffix â”† ... â”† ballotpedia_id â”† washington_post_id â”† icpsr_id â”† wikipedia_id   â”‚\nâ”‚ ---       â”† ---        â”† ---         â”† ---    â”†     â”† ---            â”† ---                â”† ---      â”† ---            â”‚\nâ”‚ str       â”† str        â”† str         â”† str    â”†     â”† str            â”† str                â”† i64      â”† str            â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ Brown     â”† Sherrod    â”† null        â”† null   â”† ... â”† Sherrod Brown  â”† null               â”† 29389    â”† Sherrod Brown  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Cantwell  â”† Maria      â”† null        â”† null   â”† ... â”† Maria Cantwell â”† null               â”† 39310    â”† Maria Cantwell â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Cardin    â”† Benjamin   â”† L.          â”† null   â”† ... â”† Ben Cardin     â”† null               â”† 15408    â”† Ben Cardin     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Carper    â”† Thomas     â”† Richard     â”† null   â”† ... â”† Tom Carper     â”† null               â”† 15015    â”† Tom Carper     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Casey     â”† Robert     â”† P.          â”† Jr.    â”† ... â”† Bob Casey, Jr. â”† null               â”† 40703    â”† Bob Casey Jr.  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nPuede combinar fÃ¡cilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un lÃ­mite superior en el nÃºmero de agregaciones que puede hacer y puede hacer cualquier combinaciÃ³n que desee. En el fragmento a continuaciÃ³n, hacemos las siguientes agregaciones:\nPor grupo \"first_name\":\n\ncuente el nÃºmero de filas en el grupo:\n\nforma abreviada: pl.count(\"party\")\nforma completa: pl.col(\"party\").count()\n\nagregue el grupo de valores de gÃ©nero a una lista:\n\nforma completa: pl.col(\"gender\").list()\n\nobtenga el primer valor de la columna \"last_name\" en el grupo:\n\nforma abreviada: pl.primero(\"last_name\")\nforma completa: pl.col(\"last_name\").first()\n\n\nAdemÃ¡s de la agregaciÃ³n, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.\n\nq = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ first_name â”† count â”† gender              â”† last_name â”‚\nâ”‚ ---        â”† ---   â”† ---                 â”† ---       â”‚\nâ”‚ str        â”† u32   â”† list [str]          â”† str       â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ John       â”† 19    â”† [\"M\", \"M\", ... \"M\"] â”† Barrasso  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Mike       â”† 13    â”† [\"M\", \"M\", ... \"M\"] â”† Kelly     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Michael    â”† 11    â”† [\"M\", \"M\", ... \"M\"] â”† Bennet    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ David      â”† 11    â”† [\"M\", \"M\", ... \"M\"] â”† Cicilline â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ James      â”† 9     â”† [\"M\", \"M\", ... \"M\"] â”† Inhofe    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nOk, eso fue bastante fÃ¡cil, Â¿verdad? Subamos un nivel. Digamos que queremos saber cuÃ¡ntos delegados de un â€œestadoâ€ (state) son administraciÃ³n â€œDemocratâ€ o â€œRepublicanâ€. PodrÃ­amos consultarlo directamente en la agregaciÃ³n sin la necesidad de lambda o arreglar el DataFrame.\n\nq = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† demo â”† repu â”‚\nâ”‚ ---   â”† ---  â”† ---  â”‚\nâ”‚ str   â”† u32  â”† u32  â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡\nâ”‚ CA    â”† 44   â”† 10   â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NY    â”† 21   â”† 8    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† 15   â”† 5    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ TX    â”† 13   â”† 25   â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NJ    â”† 12   â”† 2    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n\n\nPor supuesto, tambiÃ©n se podrÃ­a hacer algo similar con un GROUPBY anidado, pero eso no me permitirÃ­a mostrar estas caracterÃ­sticas agradables. ğŸ˜‰\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† party      â”† count â”‚\nâ”‚ ---   â”† ---        â”† ---   â”‚\nâ”‚ str   â”† str        â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ CA    â”† Democrat   â”† 44    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ TX    â”† Republican â”† 25    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NY    â”† Democrat   â”† 21    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ FL    â”† Republican â”† 18    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† Democrat   â”† 15    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nTambiÃ©n podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del DataFrame (porque necesitamos esas filas para otra agregaciÃ³n).\nEn el siguiente ejemplo, mostramos cÃ³mo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos Polars expression, no aplicamos una funciÃ³n personalizada sobre Series durante el tiempo de ejecuciÃ³n de la consulta.\n\nfrom datetime import date\n\ndef compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 5)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† avg M birthday â”† avg F birthday â”† # male â”† # female â”‚\nâ”‚ ---   â”† ---            â”† ---            â”† ---    â”† ---      â”‚\nâ”‚ str   â”† f64            â”† f64            â”† u32    â”† u32      â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ MS    â”† 60.0           â”† 62.0           â”† 5      â”† 1        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NV    â”† 55.5           â”† 61.75          â”† 2      â”† 4        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ KS    â”† 54.2           â”† 41.0           â”† 5      â”† 1        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IN    â”† 55.0           â”† 50.5           â”† 9      â”† 2        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† 60.923077      â”† 58.428571      â”† 13     â”† 7        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nA menudo veo que se ordena un DataFrame con el Ãºnico propÃ³sito de ordenar durante la operaciÃ³n GROUPBY. Digamos que queremos obtener los nombres de los polÃ­ticos mÃ¡s antiguos y mÃ¡s jÃ³venes (no es que todavÃ­a estÃ©n vivos) por estado, podrÃ­amos ORDENAR y AGRUPAR.\n\ndef get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† youngest                 â”† oldest                   â”‚\nâ”‚ ---   â”† ---                      â”† ---                      â”‚\nâ”‚ str   â”† str                      â”† str                      â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ PR    â”† Jenniffer GonzÃ¡lez-ColÃ³n â”† Jenniffer GonzÃ¡lez-ColÃ³n â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ ND    â”† John Hoeven              â”† Kelly Armstrong          â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ KY    â”† Harold Rogers            â”† Garland Barr             â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NM    â”† Teresa Leger Fernandez   â”† Melanie Stansbury        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ OR    â”† Peter DeFazio            â”† Jeff Merkley             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nPolars - User Guide\nPolars - Github"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#introducciÃ³n",
    "href": "posts/2022/2022-03-16-polars.html#introducciÃ³n",
    "title": "Polars",
    "section": "",
    "text": "Polars es una librerÃ­a de DataFrames increÃ­blemente rÃ¡pida implementada en Rust utilizando Arrow Columnar Format de Apache como modelo de memoria.\n\nLazy | eager execution\nMulti-threaded\nSIMD (Single Instruction, Multiple Data)\nQuery optimization\nPowerful expression API\nRust | Python | â€¦\n\nEsta secciÃ³n tiene como objetivos presentarle Polars a travÃ©s de ejemplos y comparÃ¡ndolo con otras soluciones.\n\nNota: Si usted no esta familiarizado con la manipulaciÃ³n de datos en Python, se recomienda partir leyendo sobre la librerÃ­a de Pandas. TambiÃ©n, se deja como referencia el curso de ManipulaciÃ³n de Datos."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#primeros-pasos",
    "href": "posts/2022/2022-03-16-polars.html#primeros-pasos",
    "title": "Polars",
    "section": "",
    "text": "Para instalar Polars, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge polars\nDe lo contrario, puede instalar con pip:\npip install polars\n\nNota: Todos los binarios estÃ¡n preconstruidos para Python v3.6+."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#rendimiento",
    "href": "posts/2022/2022-03-16-polars.html#rendimiento",
    "title": "Polars",
    "section": "",
    "text": "Polars es muy rÃ¡pido y, de hecho, es una de las mejores soluciones disponibles. Tomemos como referencia db-benchmark de h2oai. Esta pÃ¡gina tiene como objetivo comparar varias herramientas similares a bases de datos populares en la ciencia de datos de cÃ³digo abierto. Se ejecuta regularmente con las Ãºltimas versiones de estos paquetes y se actualiza automÃ¡ticamente.\nTambiÃ©n se incluye la sintaxis que se cronometra junto con el tiempo. De esta manera, puede ver de inmediato si estÃ¡ realizando estas tareas o no, y si las diferencias de tiempo le importan o no. Una diferencia de 10x puede ser irrelevante si eso es solo 1s frente a 0,1s en el tamaÃ±o de sus datos.\nA modo de ejemplo, veamos algunos ejemplos de performances de distintas librerÃ­as para ejecutar distintos tipos de tareas sobre datasets con distintos tamaÃ±os. Para el caso de tareas bÃ¡sicas sobre un dataset de 50 GB, Polars supera a librerÃ­as espacializadas en distribuciÃ³n de Dataframes como Spark (143 segundos vs 568 segundos). Por otro lado, librerÃ­as conocidas en Python como Pandas o Dask se tiene el problema de out of memory."
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#expresiones-en-polars",
    "href": "posts/2022/2022-03-16-polars.html#expresiones-en-polars",
    "title": "Polars",
    "section": "",
    "text": "Polars tiene un poderoso concepto llamado expresiones. Las expresiones polares se pueden usar en varios contextos y son un mapeo funcional de Fn(Series) -&gt; Series, lo que significa que tienen Series como entrada y Series como salida. Al observar esta definiciÃ³n funcional, podemos ver que la salida de un Expr tambiÃ©n puede servir como entrada de un Expr.\nEso puede sonar un poco extraÃ±o, asÃ­ que vamos a dar un ejemplo.\nLa siguiente es una expresiÃ³n:\npl.col(\"foo\").sort().head(2)\nEl fragmento anterior dice seleccionar la columna \"foo\", luego ordenar esta columna y luego tomar los primeros 2 valores de la salida ordenada. El poder de las expresiones es que cada expresiÃ³n produce una nueva expresiÃ³n y que se pueden canalizar juntas. Puede ejecutar una expresiÃ³n pasÃ¡ndola en uno de los contextos de ejecuciÃ³n polares. AquÃ­ ejecutamos dos expresiones ejecutando df.select:\ndf.select([\n     pl.col(\"foo\").sort().head(2),\n     pl.col(\"barra\").filter(pl.col(\"foo\") == 1).sum()\n])\nTodas las expresiones se ejecutan en paralelo. (Tenga en cuenta que dentro de una expresiÃ³n puede haber mÃ¡s paralelizaciÃ³n).\n\n\nEn esta secciÃ³n veremos algunos ejemplos, pero primero vamos a crear un conjunto de datos:\n\nimport polars as pl\nimport numpy as np\n\n\nnp.random.seed(12)\n\ndf = pl.DataFrame(\n    {\n        \"nrs\": [1, 2, 3, None, 5],\n        \"names\": [\"foo\", \"ham\", \"spam\", \"egg\", None],\n        \"random\": np.random.rand(5),\n        \"groups\": [\"A\", \"A\", \"B\", \"C\", \"B\"],\n    }\n)\nprint(df)\n\nshape: (5, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ nrs  â”† names â”† random   â”† groups â”‚\nâ”‚ ---  â”† ---   â”† ---      â”† ---    â”‚\nâ”‚ i64  â”† str   â”† f64      â”† str    â”‚\nâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\nâ”‚ 1    â”† foo   â”† 0.154163 â”† A      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2    â”† ham   â”† 0.74     â”† A      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 3    â”† spam  â”† 0.263315 â”† B      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ null â”† egg   â”† 0.533739 â”† C      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 5    â”† null  â”† 0.014575 â”† B      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nPuedes hacer mucho con las expresiones, veamos algunos ejemplos:\n\n\nPodemos contar los valores Ãºnicos en una columna. Tenga en cuenta que estamos creando el mismo resultado de diferentes maneras. Para no tener nombres de columna duplicados en el DataFrame, usamos una expresiÃ³n de alias, que cambia el nombre de una expresiÃ³n.\n\nout = df.select(\n    [\n        pl.col(\"names\").n_unique().alias(\"unique_names_1\"),\n        pl.col(\"names\").unique().count().alias(\"unique_names_2\"),\n    ]\n)\nprint(out)\n\nshape: (1, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ unique_names_1 â”† unique_names_2 â”‚\nâ”‚ ---            â”† ---            â”‚\nâ”‚ u32            â”† u32            â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 5              â”† 5              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\nPodemos hacer varias agregaciones. A continuaciÃ³n mostramos algunas de ellas, pero hay mÃ¡s, como median, mean, first, etc.\n\nout = df.select(\n    [\n        pl.sum(\"random\").alias(\"sum\"),\n        pl.min(\"random\").alias(\"min\"),\n        pl.max(\"random\").alias(\"max\"),\n        pl.col(\"random\").max().alias(\"other_max\"),\n        pl.std(\"random\").alias(\"std dev\"),\n        pl.var(\"random\").alias(\"variance\"),\n    ]\n)\nprint(out)\n\nshape: (1, 6)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ sum      â”† min      â”† max  â”† other_max â”† std dev  â”† variance â”‚\nâ”‚ ---      â”† ---      â”† ---  â”† ---       â”† ---      â”† ---      â”‚\nâ”‚ f64      â”† f64      â”† f64  â”† f64       â”† f64      â”† f64      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1.705842 â”† 0.014575 â”† 0.74 â”† 0.74      â”† 0.293209 â”† 0.085971 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nTambiÃ©n podemos hacer cosas bastante complejas. En el siguiente fragmento, contamos todos los nombres que terminan con la cadena \"am\".\n\nout = df.select(\n    [\n        pl.col(\"names\").filter(pl.col(\"names\").str.contains(r\"am$\")).count(),\n    ]\n)\nprint(out)\n\nshape: (1, 1)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names â”‚\nâ”‚ ---   â”‚\nâ”‚ u32   â”‚\nâ•â•â•â•â•â•â•â•â•¡\nâ”‚ 2     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nEn el ejemplo a continuaciÃ³n, usamos un condicional para crear una nueva expresiÃ³n when -&gt; then -&gt; otherwise.\nLa funciÃ³n when() requiere una expresiÃ³n de predicado (y, por lo tanto, conduce a una serie booleana), luego espera una expresiÃ³n que se usarÃ¡ en caso de que el predicado se evalÃºe como verdadero y, de lo contrario, espera una expresiÃ³n que se usarÃ¡ en caso de que el predicado se evalÃºe.\nTenga en cuenta que puede pasar cualquier expresiÃ³n, o simplemente expresiones base como pl.col(\"foo\"), pl.lit(3), pl.lit(\"bar\"), etc.\nFinalmente, multiplicamos esto con el resultado de una expresiÃ³n de suma.\n\nout = df.select(\n    [\n        pl.when(pl.col(\"random\") &gt; 0.5).then(0).otherwise(pl.col(\"random\")) * pl.sum(\"nrs\"),\n    ]\n)\nprint(out)\n\nshape: (5, 1)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ literal  â”‚\nâ”‚ ---      â”‚\nâ”‚ f64      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1.695791 â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.0      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2.896465 â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.0      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 0.160325 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nUna expresiÃ³n polar tambiÃ©n puede hacer un GROUPBY, AGGREGATION y JOIN implÃ­citos en una sola expresiÃ³n.\nEn los ejemplos a continuaciÃ³n, hacemos un GROUPBY sobre \"groups\" y AGREGATE SUM de \"random\", y en la siguiente expresiÃ³n GROUPBY OVER \"names\" y AGREGATE una lista de \"random\". Estas funciones de ventana se pueden combinar con otras expresiones y son una forma eficaz de determinar estadÃ­sticas de grupo. Vea mÃ¡s expresiones en el siguiente link.\n\nout = df[\n    [\n        pl.col(\"*\"),  # select all\n        pl.col(\"random\").sum().over(\"groups\").alias(\"sum[random]/groups\"),\n        pl.col(\"random\").list().over(\"names\").alias(\"random/name\"),\n    ]\n]\nprint(out)\n\nshape: (5, 6)\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ nrs  â”† names â”† random   â”† groups â”† sum[random]/groups â”† random/name â”‚\nâ”‚ ---  â”† ---   â”† ---      â”† ---    â”† ---                â”† ---         â”‚\nâ”‚ i64  â”† str   â”† f64      â”† str    â”† f64                â”† list [f64]  â”‚\nâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1    â”† foo   â”† 0.154163 â”† A      â”† 0.894213           â”† [0.154163]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 2    â”† ham   â”† 0.74     â”† A      â”† 0.894213           â”† [0.74]      â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 3    â”† spam  â”† 0.263315 â”† B      â”† 0.2778             â”† [0.263315]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ null â”† egg   â”† 0.533739 â”† C      â”† 0.533739           â”† [0.533739]  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ 5    â”† null  â”† 0.014575 â”† B      â”† 0.2778             â”† [0.014575]  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#groupby",
    "href": "posts/2022/2022-03-16-polars.html#groupby",
    "title": "Polars",
    "section": "",
    "text": "Una de las formas mÃ¡s eficientes de procesar datos tabulares es paralelizar su procesamiento a travÃ©s del enfoque â€œdividir-aplicar-combinarâ€. Esta operaciÃ³n es el nÃºcleo de la implementaciÃ³n del agrupamiento de Polars, lo que le permite lograr operaciones ultrarrÃ¡pidas. MÃ¡s especÃ­ficamente, las fases de â€œdivisiÃ³nâ€ y â€œaplicaciÃ³nâ€ se ejecutan de forma multiproceso.\nUna operaciÃ³n de agrupaciÃ³n simple se toma a continuaciÃ³n como ejemplo para ilustrar este enfoque:\n\nPara las operaciones hash realizadas durante la fase de â€œdivisiÃ³nâ€, Polars utiliza un enfoque sin bloqueo de subprocesos mÃºltiples que se ilustra en el siguiente esquema:\n\nÂ¡Esta paralelizaciÃ³n permite que las operaciones de agrupaciÃ³n y uniÃ³n (por ejemplo) sean increÃ­blemente rÃ¡pidas!\n\n\n\nTodos hemos escuchado que Python es lento y â€œno escalaâ€. AdemÃ¡s de la sobrecarga de ejecutar el cÃ³digo de bytes â€œlentoâ€, Python debe permanecer dentro de las restricciones del Global interpreter lock (GIL). Esto significa que si se usa la operaciÃ³n lambda o una funciÃ³n de Python personalizada para aplicar durante una fase de paralelizaciÃ³n, la velocidad de Polars se limita al ejecutar el cÃ³digo de Python, lo que evita que varios subprocesos ejecuten la funciÃ³n.\nTodo esto se siente terriblemente limitante, especialmente porque a menudo necesitamos esos lambda en un paso .groupby(), por ejemplo. Este enfoque aÃºn es compatible con Polars, pero teniendo en cuenta el cÃ³digo de bytes Y el precio GIL deben pagarse.\nPara mitigar esto, Polars implementa una poderosa sintaxis definida no solo en su lazy, sino tambiÃ©n en su uso eager.\n\n\n\nEn la introducciÃ³n de la pÃ¡gina anterior, discutimos que el uso de funciones personalizadas de Python eliminaba la paralelizaciÃ³n y que podemos usar las expresiones de la API diferida para mitigar esto. Echemos un vistazo a lo que eso significa.\nComencemos con el conjunto de datos simple del congreso de EE. UU.\n\nimport polars as pl\n\ndataset = pl.read_csv(\"legislators-current.csv\")\ndataset = dataset.with_column(pl.col(\"birthday\").str.strptime(pl.Date))\nprint(dataset.head())\n\nshape: (5, 34)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ last_name â”† first_name â”† middle_name â”† suffix â”† ... â”† ballotpedia_id â”† washington_post_id â”† icpsr_id â”† wikipedia_id   â”‚\nâ”‚ ---       â”† ---        â”† ---         â”† ---    â”†     â”† ---            â”† ---                â”† ---      â”† ---            â”‚\nâ”‚ str       â”† str        â”† str         â”† str    â”†     â”† str            â”† str                â”† i64      â”† str            â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ Brown     â”† Sherrod    â”† null        â”† null   â”† ... â”† Sherrod Brown  â”† null               â”† 29389    â”† Sherrod Brown  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Cantwell  â”† Maria      â”† null        â”† null   â”† ... â”† Maria Cantwell â”† null               â”† 39310    â”† Maria Cantwell â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Cardin    â”† Benjamin   â”† L.          â”† null   â”† ... â”† Ben Cardin     â”† null               â”† 15408    â”† Ben Cardin     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Carper    â”† Thomas     â”† Richard     â”† null   â”† ... â”† Tom Carper     â”† null               â”† 15015    â”† Tom Carper     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Casey     â”† Robert     â”† P.          â”† Jr.    â”† ... â”† Bob Casey, Jr. â”† null               â”† 40703    â”† Bob Casey Jr.  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nPuede combinar fÃ¡cilmente diferentes agregaciones agregando varias expresiones en una lista. No hay un lÃ­mite superior en el nÃºmero de agregaciones que puede hacer y puede hacer cualquier combinaciÃ³n que desee. En el fragmento a continuaciÃ³n, hacemos las siguientes agregaciones:\nPor grupo \"first_name\":\n\ncuente el nÃºmero de filas en el grupo:\n\nforma abreviada: pl.count(\"party\")\nforma completa: pl.col(\"party\").count()\n\nagregue el grupo de valores de gÃ©nero a una lista:\n\nforma completa: pl.col(\"gender\").list()\n\nobtenga el primer valor de la columna \"last_name\" en el grupo:\n\nforma abreviada: pl.primero(\"last_name\")\nforma completa: pl.col(\"last_name\").first()\n\n\nAdemÃ¡s de la agregaciÃ³n, clasificamos inmediatamente el resultado y lo limitamos a los 5 principales para que tengamos un buen resumen general.\n\nq = (\n    dataset.lazy()\n    .groupby(\"first_name\")\n    .agg(\n        [\n            pl.count(),\n            pl.col(\"gender\").list(),\n            pl.first(\"last_name\"),\n        ]\n    )\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ first_name â”† count â”† gender              â”† last_name â”‚\nâ”‚ ---        â”† ---   â”† ---                 â”† ---       â”‚\nâ”‚ str        â”† u32   â”† list [str]          â”† str       â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ John       â”† 19    â”† [\"M\", \"M\", ... \"M\"] â”† Barrasso  â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Mike       â”† 13    â”† [\"M\", \"M\", ... \"M\"] â”† Kelly     â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ Michael    â”† 11    â”† [\"M\", \"M\", ... \"M\"] â”† Bennet    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ David      â”† 11    â”† [\"M\", \"M\", ... \"M\"] â”† Cicilline â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ James      â”† 9     â”† [\"M\", \"M\", ... \"M\"] â”† Inhofe    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nOk, eso fue bastante fÃ¡cil, Â¿verdad? Subamos un nivel. Digamos que queremos saber cuÃ¡ntos delegados de un â€œestadoâ€ (state) son administraciÃ³n â€œDemocratâ€ o â€œRepublicanâ€. PodrÃ­amos consultarlo directamente en la agregaciÃ³n sin la necesidad de lambda o arreglar el DataFrame.\n\nq = (\n    dataset.lazy()\n    .groupby(\"state\")\n    .agg(\n        [\n            (pl.col(\"party\") == \"Democrat\").sum().alias(\"demo\"),\n            (pl.col(\"party\") == \"Republican\").sum().alias(\"repu\"),\n        ]\n    )\n    .sort(\"demo\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† demo â”† repu â”‚\nâ”‚ ---   â”† ---  â”† ---  â”‚\nâ”‚ str   â”† u32  â”† u32  â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡\nâ”‚ CA    â”† 44   â”† 10   â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NY    â”† 21   â”† 8    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† 15   â”† 5    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ TX    â”† 13   â”† 25   â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NJ    â”† 12   â”† 2    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\n\n\nPor supuesto, tambiÃ©n se podrÃ­a hacer algo similar con un GROUPBY anidado, pero eso no me permitirÃ­a mostrar estas caracterÃ­sticas agradables. ğŸ˜‰\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\", \"party\"])\n    .agg([pl.count(\"party\").alias(\"count\")])\n    .filter((pl.col(\"party\") == \"Democrat\") | (pl.col(\"party\") == \"Republican\"))\n    .sort(\"count\", reverse=True)\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† party      â”† count â”‚\nâ”‚ ---   â”† ---        â”† ---   â”‚\nâ”‚ str   â”† str        â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ CA    â”† Democrat   â”† 44    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ TX    â”† Republican â”† 25    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NY    â”† Democrat   â”† 21    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ FL    â”† Republican â”† 18    â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† Democrat   â”† 15    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nTambiÃ©n podemos filtrar los grupos. Digamos que queremos calcular una media por grupo, pero no queremos incluir todos los valores de ese grupo y tampoco queremos filtrar las filas del DataFrame (porque necesitamos esas filas para otra agregaciÃ³n).\nEn el siguiente ejemplo, mostramos cÃ³mo se puede hacer eso. Tenga en cuenta que podemos hacer funciones de Python para mayor claridad. Estas funciones no nos cuestan nada. Esto se debe a que solo creamos Polars expression, no aplicamos una funciÃ³n personalizada sobre Series durante el tiempo de ejecuciÃ³n de la consulta.\n\nfrom datetime import date\n\ndef compute_age() -&gt; pl.Expr:\n    return date(2021, 1, 1).year - pl.col(\"birthday\").dt.year()\n\n\ndef avg_birthday(gender: str) -&gt; pl.Expr:\n    return compute_age().filter(pl.col(\"gender\") == gender).mean().alias(f\"avg {gender} birthday\")\n\n\nq = (\n    dataset.lazy()\n    .groupby([\"state\"])\n    .agg(\n        [\n            avg_birthday(\"M\"),\n            avg_birthday(\"F\"),\n            (pl.col(\"gender\") == \"M\").sum().alias(\"# male\"),\n            (pl.col(\"gender\") == \"F\").sum().alias(\"# female\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\nprint(df)\n\nshape: (5, 5)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† avg M birthday â”† avg F birthday â”† # male â”† # female â”‚\nâ”‚ ---   â”† ---            â”† ---            â”† ---    â”† ---      â”‚\nâ”‚ str   â”† f64            â”† f64            â”† u32    â”† u32      â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ MS    â”† 60.0           â”† 62.0           â”† 5      â”† 1        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NV    â”† 55.5           â”† 61.75          â”† 2      â”† 4        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ KS    â”† 54.2           â”† 41.0           â”† 5      â”† 1        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IN    â”† 55.0           â”† 50.5           â”† 9      â”† 2        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ IL    â”† 60.923077      â”† 58.428571      â”† 13     â”† 7        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\nA menudo veo que se ordena un DataFrame con el Ãºnico propÃ³sito de ordenar durante la operaciÃ³n GROUPBY. Digamos que queremos obtener los nombres de los polÃ­ticos mÃ¡s antiguos y mÃ¡s jÃ³venes (no es que todavÃ­a estÃ©n vivos) por estado, podrÃ­amos ORDENAR y AGRUPAR.\n\ndef get_person() -&gt; pl.Expr:\n    return pl.col(\"first_name\") + pl.lit(\" \") + pl.col(\"last_name\")\n\n\nq = (\n    dataset.lazy()\n    .sort(\"birthday\")\n    .groupby([\"state\"])\n    .agg(\n        [\n            get_person().first().alias(\"youngest\"),\n            get_person().last().alias(\"oldest\"),\n        ]\n    )\n    .limit(5)\n)\n\ndf = q.collect()\n\nprint(df)\n\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ state â”† youngest                 â”† oldest                   â”‚\nâ”‚ ---   â”† ---                      â”† ---                      â”‚\nâ”‚ str   â”† str                      â”† str                      â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ PR    â”† Jenniffer GonzÃ¡lez-ColÃ³n â”† Jenniffer GonzÃ¡lez-ColÃ³n â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ ND    â”† John Hoeven              â”† Kelly Armstrong          â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ KY    â”† Harold Rogers            â”† Garland Barr             â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ NM    â”† Teresa Leger Fernandez   â”† Melanie Stansbury        â”‚\nâ”œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¼â•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ•Œâ”¤\nâ”‚ OR    â”† Peter DeFazio            â”† Jeff Merkley             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
  },
  {
    "objectID": "posts/2022/2022-03-16-polars.html#referencias",
    "href": "posts/2022/2022-03-16-polars.html#referencias",
    "title": "Polars",
    "section": "",
    "text": "Polars - User Guide\nPolars - Github"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html",
    "href": "posts/2022/2022-10-12-causal_impact.html",
    "title": "Causal Impact",
    "section": "",
    "text": "El paquete CausalImpact creado por Google estima el impacto de una intervenciÃ³n en una serie temporal. Por ejemplo, Â¿cÃ³mo afecta una nueva funciÃ³n en una aplicaciÃ³n el tiempo de los usuarios en la aplicaciÃ³n?\nEn este tutorial, hablaremos sobre cÃ³mo usar el paquete de Python CausalImpact para hacer inferencias causales de series de tiempo. AprenderÃ¡s: * Â¿CÃ³mo establecer los perÃ­odos previo y posterior para el anÃ¡lisis de impacto causal? * Â¿CÃ³mo realizar inferencias causales sobre datos de series temporales? * Â¿CÃ³mo resumir los resultados del anÃ¡lisis de causalidad y crear un informe? * Â¿CuÃ¡les son las diferencias entre los paquetes python y R para CausalImpact?\n\n\n\nEn primer lugar, instalemos pycausalimpac para el anÃ¡lisis causal de series de tiempo.\n\n# Install python version of causal impact\n#!pip install pycausalimpact\n\nUna vez completada la instalaciÃ³n, podemos importar las bibliotecas. * pandas, numpy y datetime se importan para el procesamiento de datos. * ArmaProcess se importa para la creaciÃ³n de datos de series temporales sintÃ©ticas. * matplotlib y seaborn son para visualizaciÃ³n. * CausalImpact es para la estimaciÃ³n de los efectos del tratamiento de series de tiempo.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact\n\n\n\n\nCrearemos un conjunto de datos de series de tiempo sintÃ©tico para el anÃ¡lisis de impacto causal. El beneficio de usar un conjunto de datos sintÃ©tico es que podemos validar la precisiÃ³n de los resultados del modelo.\nEl paquete CausalImpact requiere dos tipos de series temporales: * Una serie temporal de respuesta que se ve directamente afectada por la intervenciÃ³n. * Y una o mÃ¡s series temporales de control que no se ven afectadas por la intervenciÃ³n.\nLa idea es construir un modelo de serie de tiempo para predecir el resultado contrafÃ¡ctico. En otras palabras, el modelo utilizarÃ¡ la serie temporal de control para predecir cuÃ¡l habrÃ­a sido el resultado de la serie temporal de respuesta si no hubiera habido intervenciÃ³n.\nEn este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control. * Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del cÃ³digo. * Luego se crea un proceso de promedio mÃ³vil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media mÃ³vil (MA) tiene dos coeficientes 0,6 y 0,3. * DespuÃ©s de crear el proceso de media mÃ³vil autorregresiva (ARMA), se generan 500 muestras a partir del proceso. * La variable de serie temporal de control X se crea aÃ±adiendo un valor constante de 10 a los valores generados. * La variable de serie temporal de respuesta y es una funciÃ³n de la variable de serie temporal de control X. Es igual a 2 veces X mÃ¡s un valor aleatorio. * La intervenciÃ³n ocurre en el Ã­ndice de 300, y el verdadero impacto causal es 10.\n\n# Set up a seed for reproducibility\nnp.random.seed(42)\n\n# Autoregressive coefficients\narparams = np.array([.95, .05])\n\n# Moving average coefficients\nmaparams = np.array([.6, .3])\n\n# Create a ARMA process\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Create the control time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Create the response time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Add the true causal impact\ny[300:] += 10\n\nUna serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la funciÃ³n pandas date_range, lo que indica que el conjunto de datos tiene datos diarios.\nDespuÃ©s de eso, se crea un marco de datos de pandas con la variable de control X, la variable de respuesta es y y las dates como Ã­ndice.\n\n# Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\ny\nX\n\n\ndates\n\n\n\n\n\n\n2021-01-01\n21.919606\n10.496714\n\n\n2021-01-02\n23.172702\n10.631643\n\n\n2021-01-03\n21.278713\n11.338640\n\n\n2021-01-04\n26.909878\n13.173454\n\n\n2021-01-05\n27.260727\n13.955685\n\n\n\n\n\n\n\n\n\n\n\nEstableceremos los periodos de pre y post intervenciÃ³n. Usando df.index, podemos ver que la fecha de inicio de la serie temporal es 2021-01-01, la fecha de finalizaciÃ³n de la serie temporal es 2022-05-15 y la fecha de inicio del tratamiento es 2021- 10-28.\n\n# Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n\nThe time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n\n\nA continuaciÃ³n, visualicemos los datos de la serie temporal.\n\n# Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n\n\n\n\n\n\n\n\nEn el grÃ¡fico, la lÃ­nea azul es la serie temporal de control, la lÃ­nea naranja es la serie temporal de respuesta y la lÃ­nea vertical roja representa la fecha de inicio de la intervenciÃ³n.\nPodemos ver que antes de la intervenciÃ³n, las series temporales de control y respuesta tienen valores similares. DespuÃ©s de la intervenciÃ³n, la serie de tiempo de respuesta tiene consistentemente valores mÃ¡s altos que la serie de tiempo de control.\nEl paquete CausalImpact de python requiere las entradas de los perÃ­odos anterior y posterior en un formato de lista. El primer elemento de la lista es el Ã­ndice inicial y el Ãºltimo elemento de la lista es el Ã­ndice final.\nLa fecha de inicio de la intervenciÃ³n es 2021-10-28, por lo que el perÃ­odo previo finaliza en 2021-10-27.\n\n# Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n\nThe pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']\n\n\n\n\n\nCalcularemos la diferencia bruta entre los perÃ­odos previo y posterior.\nPodemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.\nSin anÃ¡lisis de causalidad, sobreestimaremos el impacto causal.\n\n# Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n\nThe pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855.\n\n\n\n\n\nejecutaremos el anÃ¡lisis de impacto causal sobre la serie temporal.\nEl anÃ¡lisis de causalidad tiene dos supuestos: * Supuesto 1: Hay una o mÃ¡s series temporales de control que estÃ¡n altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervenciÃ³n. La violaciÃ³n de esta suposiciÃ³n puede dar lugar a conclusiones errÃ³neas sobre la existencia, la direcciÃ³n o la magnitud del efecto del tratamiento. * Supuesto 2: La correlaciÃ³n entre el control y la serie temporal de respuesta es la misma para antes y despuÃ©s de la intervenciÃ³n.\nLos datos de series de tiempo sintÃ©ticos que creamos satisfacen las dos suposiciones.\nEl paquete CausalImpact de python tiene una funciÃ³n llamada CausalImpact que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas: * data toma el nombre del dataframe de python. * pre_period toma los valores de Ã­ndice inicial y final para el perÃ­odo previo a la intervenciÃ³n. * post_period toma los valores de Ã­ndice inicial y final para el perÃ­odo posterior a la intervenciÃ³n.\nDespuÃ©s de guardar el objeto de salida en una variable llamada impact, podemos ejecutar impact.plot() para visualizar los resultados.\n\n# Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nLa visualizaciÃ³n consta de tres grÃ¡ficos: * El primer grÃ¡fico traza los valores contrafactuales pronosticados y los valores reales para el perÃ­odo posterior. * El segundo grÃ¡fico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al perÃ­odo estÃ¡n alrededor de 0, y los valores de los efectos de puntos posteriores al perÃ­odo estÃ¡n alrededor del impacto real de 10. * El tercer grÃ¡fico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo grÃ¡fico.\n\n\n\nResumiremos el impacto causal de la intervenciÃ³n para la serie temporal.\nEl resumen de impact.summary() nos dice que: * El promedio posterior a la intervenciÃ³n real es 50,08 y el promedio posterior a la intervenciÃ³n pronosticado es 40,3. * El efecto causal absoluto es 10,06, que estÃ¡ muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7. * El efecto causal relativo es del 25,12%. * La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.\n\n# Causal impact summary\nprint(impact.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\nPodemos imprimir la versiÃ³n del informe del resumen usando la opciÃ³n output='report'.\n\n# Causal impact report\nprint(impact.summary(output='report'))\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant.\n\n\n\n\n\nHablaremos sobre las diferencias del paquete CausalImpact de Google entre Python y R.\nEl paquete python fue portado desde el paquete R, por lo que la mayorÃ­a de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.\nLas diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimizaciÃ³n y los algoritmos de implementaciÃ³n.\nLa documentaciÃ³n del paquete pycausalimpact recomienda encarecidamente establecer prior_level_sd en Ninguno, lo que permitirÃ¡ que statsmodel realice la optimizaciÃ³n para el componente anterior en el nivel local.\nEn base a esta sugerencia, se crea una versiÃ³n con la opciÃ³n prior_level_sd=None.\n\n# Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nPodemos ver que los valores de estimaciÃ³n puntual son similares, pero las desviaciones estÃ¡ndar son mÃ¡s pequeÃ±as para la estimaciÃ³n.\n\n# Print out the summary\nprint(impact_no_prior_level_sd.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\n\n\n\nPara aprender a ajustar los hiperparÃ¡metros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python\n\n\n\n\nInferring causal impact using Bayesian structural time-series models.\nTime Series Causal Impact Analysis in Python | Machine Learning.\nInferring the effect of an event using CausalImpact by Kay Brodersen."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#introducciÃ³n",
    "href": "posts/2022/2022-10-12-causal_impact.html#introducciÃ³n",
    "title": "Causal Impact",
    "section": "",
    "text": "El paquete CausalImpact creado por Google estima el impacto de una intervenciÃ³n en una serie temporal. Por ejemplo, Â¿cÃ³mo afecta una nueva funciÃ³n en una aplicaciÃ³n el tiempo de los usuarios en la aplicaciÃ³n?\nEn este tutorial, hablaremos sobre cÃ³mo usar el paquete de Python CausalImpact para hacer inferencias causales de series de tiempo. AprenderÃ¡s: * Â¿CÃ³mo establecer los perÃ­odos previo y posterior para el anÃ¡lisis de impacto causal? * Â¿CÃ³mo realizar inferencias causales sobre datos de series temporales? * Â¿CÃ³mo resumir los resultados del anÃ¡lisis de causalidad y crear un informe? * Â¿CuÃ¡les son las diferencias entre los paquetes python y R para CausalImpact?"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#importar-librerÃ­as",
    "href": "posts/2022/2022-10-12-causal_impact.html#importar-librerÃ­as",
    "title": "Causal Impact",
    "section": "",
    "text": "En primer lugar, instalemos pycausalimpac para el anÃ¡lisis causal de series de tiempo.\n\n# Install python version of causal impact\n#!pip install pycausalimpact\n\nUna vez completada la instalaciÃ³n, podemos importar las bibliotecas. * pandas, numpy y datetime se importan para el procesamiento de datos. * ArmaProcess se importa para la creaciÃ³n de datos de series temporales sintÃ©ticas. * matplotlib y seaborn son para visualizaciÃ³n. * CausalImpact es para la estimaciÃ³n de los efectos del tratamiento de series de tiempo.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\n# Create synthetic time-series data\nfrom statsmodels.tsa.arima_process import ArmaProcess\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Causal impact\nfrom causalimpact import CausalImpact"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#crear-el-conjunto-de-datos",
    "href": "posts/2022/2022-10-12-causal_impact.html#crear-el-conjunto-de-datos",
    "title": "Causal Impact",
    "section": "",
    "text": "Crearemos un conjunto de datos de series de tiempo sintÃ©tico para el anÃ¡lisis de impacto causal. El beneficio de usar un conjunto de datos sintÃ©tico es que podemos validar la precisiÃ³n de los resultados del modelo.\nEl paquete CausalImpact requiere dos tipos de series temporales: * Una serie temporal de respuesta que se ve directamente afectada por la intervenciÃ³n. * Y una o mÃ¡s series temporales de control que no se ven afectadas por la intervenciÃ³n.\nLa idea es construir un modelo de serie de tiempo para predecir el resultado contrafÃ¡ctico. En otras palabras, el modelo utilizarÃ¡ la serie temporal de control para predecir cuÃ¡l habrÃ­a sido el resultado de la serie temporal de respuesta si no hubiera habido intervenciÃ³n.\nEn este ejemplo, creamos una variable de serie temporal de respuesta y una variable de serie temporal de control. * Para que el conjunto de datos sea reproducible, se establece una semilla aleatoria al comienzo del cÃ³digo. * Luego se crea un proceso de promedio mÃ³vil autorregresivo (ARMA). La parte autorregresiva (AR) tiene dos coeficientes 0,95 y 0,05, y la parte de media mÃ³vil (MA) tiene dos coeficientes 0,6 y 0,3. * DespuÃ©s de crear el proceso de media mÃ³vil autorregresiva (ARMA), se generan 500 muestras a partir del proceso. * La variable de serie temporal de control X se crea aÃ±adiendo un valor constante de 10 a los valores generados. * La variable de serie temporal de respuesta y es una funciÃ³n de la variable de serie temporal de control X. Es igual a 2 veces X mÃ¡s un valor aleatorio. * La intervenciÃ³n ocurre en el Ã­ndice de 300, y el verdadero impacto causal es 10.\n\n# Set up a seed for reproducibility\nnp.random.seed(42)\n\n# Autoregressive coefficients\narparams = np.array([.95, .05])\n\n# Moving average coefficients\nmaparams = np.array([.6, .3])\n\n# Create a ARMA process\narma_process = ArmaProcess.from_coeffs(arparams, maparams)\n\n# Create the control time-series\nX = 10 + arma_process.generate_sample(nsample=500)\n\n# Create the response time-series\ny = 2 * X + np.random.normal(size=500)\n\n# Add the true causal impact\ny[300:] += 10\n\nUna serie de tiempo generalmente tiene una variable de tiempo que indica la frecuencia de los datos recopilados. Creamos 500 fechas a partir del 1 de enero de 2021 usando la funciÃ³n pandas date_range, lo que indica que el conjunto de datos tiene datos diarios.\nDespuÃ©s de eso, se crea un marco de datos de pandas con la variable de control X, la variable de respuesta es y y las dates como Ã­ndice.\n\n# Create dates\ndates = pd.date_range('2021-01-01', freq='D', periods=500)\n\n# Create dataframe\ndf = pd.DataFrame({'dates': dates, 'y': y, 'X': X}, columns=['dates', 'y', 'X'])\n\n# Set dates as index\ndf.set_index('dates', inplace=True)\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\ny\nX\n\n\ndates\n\n\n\n\n\n\n2021-01-01\n21.919606\n10.496714\n\n\n2021-01-02\n23.172702\n10.631643\n\n\n2021-01-03\n21.278713\n11.338640\n\n\n2021-01-04\n26.909878\n13.173454\n\n\n2021-01-05\n27.260727\n13.955685"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#perÃ­odos-anteriores-y-posteriores",
    "href": "posts/2022/2022-10-12-causal_impact.html#perÃ­odos-anteriores-y-posteriores",
    "title": "Causal Impact",
    "section": "",
    "text": "Estableceremos los periodos de pre y post intervenciÃ³n. Usando df.index, podemos ver que la fecha de inicio de la serie temporal es 2021-01-01, la fecha de finalizaciÃ³n de la serie temporal es 2022-05-15 y la fecha de inicio del tratamiento es 2021- 10-28.\n\n# Print out the time series start date\nprint(f'The time-series start date is :{df.index.min()}')\n\n# Print out the time series end date\nprint(f'The time-series end date is :{df.index.max()}')\n\n# Print out the intervention start date\nprint(f'The treatment start date is :{df.index[300]}')\n\nThe time-series start date is :2021-01-01 00:00:00\nThe time-series end date is :2022-05-15 00:00:00\nThe treatment start date is :2021-10-28 00:00:00\n\n\nA continuaciÃ³n, visualicemos los datos de la serie temporal.\n\n# Visualize data using seaborn\nsns.set(rc={'figure.figsize':(12,8)})\nsns.lineplot(x=df.index, y=df['X'])\nsns.lineplot(x=df.index, y=df['y'])\nplt.axvline(x= df.index[300], color='red')\nplt.legend(labels = ['X', 'y'])\nplt.show()\n\n\n\n\n\n\n\n\nEn el grÃ¡fico, la lÃ­nea azul es la serie temporal de control, la lÃ­nea naranja es la serie temporal de respuesta y la lÃ­nea vertical roja representa la fecha de inicio de la intervenciÃ³n.\nPodemos ver que antes de la intervenciÃ³n, las series temporales de control y respuesta tienen valores similares. DespuÃ©s de la intervenciÃ³n, la serie de tiempo de respuesta tiene consistentemente valores mÃ¡s altos que la serie de tiempo de control.\nEl paquete CausalImpact de python requiere las entradas de los perÃ­odos anterior y posterior en un formato de lista. El primer elemento de la lista es el Ã­ndice inicial y el Ãºltimo elemento de la lista es el Ã­ndice final.\nLa fecha de inicio de la intervenciÃ³n es 2021-10-28, por lo que el perÃ­odo previo finaliza en 2021-10-27.\n\n# Set pre-period\npre_period = [str(df.index.min())[:10], str(df.index[299])[:10]]\n\n# Set post-period\npost_period = [str(df.index[300])[:10], str(df.index.max())[:10]]\n\n# Print out the values\nprint(f'The pre-period is {pre_period}')\nprint(f'The post-period is {post_period}')\n\nThe pre-period is ['2021-01-01', '2021-10-27']\nThe post-period is ['2021-10-28', '2022-05-15']"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#diferencias-sin-procesar",
    "href": "posts/2022/2022-10-12-causal_impact.html#diferencias-sin-procesar",
    "title": "Causal Impact",
    "section": "",
    "text": "Calcularemos la diferencia bruta entre los perÃ­odos previo y posterior.\nPodemos ver que el promedio diario previo al tratamiento es -1,64, el promedio diario posterior al tratamiento es 50,08 y la diferencia bruta entre el tratamiento previo y posterior es 51,7, que es mucho mayor que el verdadero impacto causal de 10.\nSin anÃ¡lisis de causalidad, sobreestimaremos el impacto causal.\n\n# Calculate the pre-daily average\npre_daily_avg = df['y'][:300].mean()\n\n# Calculate the post-daily average\npost_daily_avg = df['y'][300:].mean()\n\n# Print out the results\nprint(f'The pre-treatment daily average is {pre_daily_avg}.')\nprint(f'The post-treatment daily average is {post_daily_avg}.')\nprint(f'The raw difference between the pre and the post treatment is {post_daily_avg - pre_daily_avg}.')\n\nThe pre-treatment daily average is -1.6403416947312546.\nThe post-treatment daily average is 50.08461262581729.\nThe raw difference between the pre and the post treatment is 51.72495432054855."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#causal-impact-en-series-de-tiempo",
    "href": "posts/2022/2022-10-12-causal_impact.html#causal-impact-en-series-de-tiempo",
    "title": "Causal Impact",
    "section": "",
    "text": "ejecutaremos el anÃ¡lisis de impacto causal sobre la serie temporal.\nEl anÃ¡lisis de causalidad tiene dos supuestos: * Supuesto 1: Hay una o mÃ¡s series temporales de control que estÃ¡n altamente correlacionadas con la variable de respuesta, pero que no se ven afectadas por la intervenciÃ³n. La violaciÃ³n de esta suposiciÃ³n puede dar lugar a conclusiones errÃ³neas sobre la existencia, la direcciÃ³n o la magnitud del efecto del tratamiento. * Supuesto 2: La correlaciÃ³n entre el control y la serie temporal de respuesta es la misma para antes y despuÃ©s de la intervenciÃ³n.\nLos datos de series de tiempo sintÃ©ticos que creamos satisfacen las dos suposiciones.\nEl paquete CausalImpact de python tiene una funciÃ³n llamada CausalImpact que implementa un modelo de serie de tiempo estructural bayesiano (BSTS) en el backend. Tiene tres entradas requeridas: * data toma el nombre del dataframe de python. * pre_period toma los valores de Ã­ndice inicial y final para el perÃ­odo previo a la intervenciÃ³n. * post_period toma los valores de Ã­ndice inicial y final para el perÃ­odo posterior a la intervenciÃ³n.\nDespuÃ©s de guardar el objeto de salida en una variable llamada impact, podemos ejecutar impact.plot() para visualizar los resultados.\n\n# Causal impact model\nimpact = CausalImpact(data=df, pre_period=pre_period, post_period=post_period)\n\n# Visualization\nimpact.plot()\n\nplt.show()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nLa visualizaciÃ³n consta de tres grÃ¡ficos: * El primer grÃ¡fico traza los valores contrafactuales pronosticados y los valores reales para el perÃ­odo posterior. * El segundo grÃ¡fico representa los efectos puntuales, que son las diferencias entre los valores reales y los previstos. Podemos ver que los valores de los efectos de puntos anteriores al perÃ­odo estÃ¡n alrededor de 0, y los valores de los efectos de puntos posteriores al perÃ­odo estÃ¡n alrededor del impacto real de 10. * El tercer grÃ¡fico traza el efecto acumulativo, que es la suma acumulativa de los efectos de puntos del segundo grÃ¡fico."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#resumen-causal-impact",
    "href": "posts/2022/2022-10-12-causal_impact.html#resumen-causal-impact",
    "title": "Causal Impact",
    "section": "",
    "text": "Resumiremos el impacto causal de la intervenciÃ³n para la serie temporal.\nEl resumen de impact.summary() nos dice que: * El promedio posterior a la intervenciÃ³n real es 50,08 y el promedio posterior a la intervenciÃ³n pronosticado es 40,3. * El efecto causal absoluto es 10,06, que estÃ¡ muy cerca del verdadero impacto de 10 y mucho mejor que la diferencia bruta de 51,7. * El efecto causal relativo es del 25,12%. * La probabilidad posterior de un efecto causal es del 100%, lo que demuestra que el modelo tiene mucha confianza en que existe el impacto causal.\n\n# Causal impact summary\nprint(impact.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         40.03 (1.2)        8005.58 (239.36)\n95% CI                    [37.64, 42.33]     [7527.6, 8465.89]\n\nAbsolute effect (s.d.)    10.06 (1.2)        2011.34 (239.36)\n95% CI                    [7.76, 12.45]      [1551.03, 2489.32]\n\nRelative effect (s.d.)    25.12% (2.99%)     25.12% (2.99%)\n95% CI                    [19.37%, 31.09%]   [19.37%, 31.09%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))\n\n\nPodemos imprimir la versiÃ³n del informe del resumen usando la opciÃ³n output='report'.\n\n# Causal impact report\nprint(impact.summary(output='report'))\n\nAnalysis report {CausalImpact}\n\n\nDuring the post-intervention period, the response variable had\nan average value of approx. 50.08. By contrast, in the absence of an\nintervention, we would have expected an average response of 40.03.\nThe 95% interval of this counterfactual prediction is [37.64, 42.33].\nSubtracting this prediction from the observed response yields\nan estimate of the causal effect the intervention had on the\nresponse variable. This effect is 10.06 with a 95% interval of\n[7.76, 12.45]. For a discussion of the significance of this effect,\nsee below.\n\n\nSumming up the individual data points during the post-intervention\nperiod (which can only sometimes be meaningfully interpreted), the\nresponse variable had an overall value of 10016.92.\nBy contrast, had the intervention not taken place, we would have expected\na sum of 8005.58. The 95% interval of this prediction is [7527.6, 8465.89].\n\n\nThe above results are given in terms of absolute numbers. In relative\nterms, the response variable showed an increase of +25.12%. The 95%\ninterval of this percentage is [19.37%, 31.09%].\n\n\nThis means that the positive effect observed during the intervention\nperiod is statistically significant and unlikely to be due to random\nfluctuations. It should be noted, however, that the question of whether\nthis increase also bears substantive significance can only be answered\nby comparing the absolute effect (10.06) to the original goal\nof the underlying intervention.\n\n\nThe probability of obtaining this effect by chance is very small\n(Bayesian one-sided tail-area probability p = 0.0).\nThis means the causal effect can be considered statistically\nsignificant."
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#causalimpact-python-vs-r",
    "href": "posts/2022/2022-10-12-causal_impact.html#causalimpact-python-vs-r",
    "title": "Causal Impact",
    "section": "",
    "text": "Hablaremos sobre las diferencias del paquete CausalImpact de Google entre Python y R.\nEl paquete python fue portado desde el paquete R, por lo que la mayorÃ­a de las veces los dos paquetes producen resultados similares, pero a veces producen resultados diferentes.\nLas diferencias son causadas por suposiciones para inicializaciones previas, el proceso de optimizaciÃ³n y los algoritmos de implementaciÃ³n.\nLa documentaciÃ³n del paquete pycausalimpact recomienda encarecidamente establecer prior_level_sd en Ninguno, lo que permitirÃ¡ que statsmodel realice la optimizaciÃ³n para el componente anterior en el nivel local.\nEn base a esta sugerencia, se crea una versiÃ³n con la opciÃ³n prior_level_sd=None.\n\n# Causal impact model without prior level sd\nimpact_no_prior_level_sd = CausalImpact(df, pre_period, post_period, prior_level_sd=None)\n\n# Plot the results\nimpact_no_prior_level_sd.plot()\n\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\base\\optimizer.py:17: FutureWarning: Keyword arguments have been passed to the optimizer that have no effect. The list of allowed keyword arguments for method lbfgs is: m, pgtol, factr, maxfun, epsilon, approx_grad, bounds, loglike_and_score. The list of unsupported keyword arguments passed include: nseasons, standardize, prior_level_sd. After release 0.14, this will raise.\n  warnings.warn(\nC:\\Users\\franc\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\basic-tools-a_KeXlN7-py3.8\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n\n\n\n\n\n\n\n\n\nPodemos ver que los valores de estimaciÃ³n puntual son similares, pero las desviaciones estÃ¡ndar son mÃ¡s pequeÃ±as para la estimaciÃ³n.\n\n# Print out the summary\nprint(impact_no_prior_level_sd.summary())\n\nPosterior Inference {Causal Impact}\n                          Average            Cumulative\nActual                    50.08              10016.92\nPrediction (s.d.)         39.84 (0.09)       7967.47 (18.41)\n95% CI                    [39.65, 40.01]     [7929.85, 8001.99]\n\nAbsolute effect (s.d.)    10.25 (0.09)       2049.45 (18.41)\n95% CI                    [10.07, 10.44]     [2014.93, 2087.08]\n\nRelative effect (s.d.)    25.72% (0.23%)     25.72% (0.23%)\n95% CI                    [25.29%, 26.19%]   [25.29%, 26.19%]\n\nPosterior tail-area probability p: 0.0\nPosterior prob. of a causal effect: 100.0%\n\nFor more details run the command: print(impact.summary('report'))"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#ajuste-de-hiperparÃ¡metros",
    "href": "posts/2022/2022-10-12-causal_impact.html#ajuste-de-hiperparÃ¡metros",
    "title": "Causal Impact",
    "section": "",
    "text": "Para aprender a ajustar los hiperparÃ¡metros del modelo de impacto causal de series temporales con el paquete CausalImpact de python, consulte eltutorial Hyperparameter Tuning for Time Series Causal Impact Analysis in Python"
  },
  {
    "objectID": "posts/2022/2022-10-12-causal_impact.html#referencias",
    "href": "posts/2022/2022-10-12-causal_impact.html#referencias",
    "title": "Causal Impact",
    "section": "",
    "text": "Inferring causal impact using Bayesian structural time-series models.\nTime Series Causal Impact Analysis in Python | Machine Learning.\nInferring the effect of an event using CausalImpact by Kay Brodersen."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html",
    "href": "posts/2021/2021-08-31-buenas_practicas.html",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cuÃ¡l es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programaciÃ³n, con el fin de hacer el cÃ³digo mÃ¡s legible, sencillo de entender y ayudar a encontrar posibles errores.\nEn esta secciÃ³n se mostrarÃ¡ algunos conceptos sencillos que te ayudarÃ¡n a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\nEl PEP8 es un estilo de codificaciÃ³n que proporciona convenciones de codificaciÃ³n para el cÃ³digo Python que comprende la biblioteca estÃ¡ndar en la distribuciÃ³n principal de Python.\nAlgunos aspectos importantes:\n\nEl PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la GuÃ­a de estilo Python de Guido, con algunas adiciones de la guÃ­a de estilo de Barry.\nEsta guÃ­a de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.\nMuchos proyectos tienen sus propias pautas de estilo de codificaciÃ³n. En caso de conflicto, dichas guÃ­as especÃ­ficas del proyecto tienen prioridad para ese proyecto.\n\nBasados en el PEP8 y algunas buenas prÃ¡cticas del diseÃ±o de software, veamos ejemplo para poder escribir de mejor forma nuestros cÃ³digos.\n\n\nCuando sea posible, define variables con nombres que tengan algÃºn sentido o que puedas identificar fÃ¡cilmente, no importa que sean mÃ¡s largas. Por ejemplo, en un programa podrÃ­amos escribir:\n\na = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\npero, Â¿quÃ© significan a y b? lo sabemos por el comentario (bien hecho), pero si mÃ¡s adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:\n\naltura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\n\n\n\nLas lÃ­neas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una lÃ­nea larga, se puede cortar con una barra invertida (\\) y continuar en la siguiente lÃ­nea:\n\nprint(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la lÃ­nea inferior.\")\n\nEsta es una frase muy larga, se puede cortar con un        y seguir en la lÃ­nea inferior.\n\n\n\n\n\nLos comentarios son muy importantes al escribir un programa. Describen lo que estÃ¡ sucediendo dentro de un programa, para que una persona que mira el cÃ³digo fuente no tenga dificultades para descifrarlo.\n\n#\n# esto es un comentario\nprint('Hola')\n\nHola\n\n\nTambiÃ©n podemos tener comentarios multilÃ­neas:\n\n#\n# Este es un comentario largo\n# y se extiende\n# a varias lÃ­neas\n\n\n\n\nLas importaciones generalmente deben estar en lÃ­neas separadas:\n\n#\n# no:\nimport sys, os\n\n\n#\n# si:\nimport os\nimport sys\n\n\n\n\nExisten varias formas de hacer comparaciones de objetos (principalmente en el uso del bucle if), acÃ¡ se dejan alguna recomendaciones:\n# no\nif greeting == True:\n\n# no\nif greeting is True:\n# si\nif greeting:\n\n\n\nDentro de parÃ©ntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:\n\n#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n\n\n#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n\nAunque en Python se pueden hacer varias declaraciones en una lÃ­nea, se recomienda hacer sÃ³lo una en cada lÃ­nea:\n\n#\n# no\na = 10; b = 20\n\n\n#\n# si\na = 10\nb = 20  \n\nCuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada lÃ­nea sus argumentos.\n\n#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]  \n\n\n#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n\nLo anterior se puede extender para funciones con muchos argumentos\n\n#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n\n\n#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n\n\n\n\nUn tema interesante es corresponde a la identaciÃ³n respecto a los operadores binarios, acÃ¡ se muestra la forma correcta de hacerlo:\n# no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n# si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n\n\n\nAunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay mÃ©todos especÃ­ficos mÃ¡s eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:\n\n#\n# Seleccionar los nÃºmeros positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i &gt; 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\nAunque tÃ©cnicamente es correcto, es mÃ¡s eficiente hacer List Comprehension:\n\n#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i &gt; 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\n\n\n\nCuando se ocupa try/except, es necesario especificar el tipo de error que se estÃ¡ cometiendo.\n\n#\n# importar librerias\nimport sys\n\n\n#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n\n\nSiempre es mejor definir las variables dentro de una funciÃ³n y no dejar variables globales.\n\n#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n\n\nfuncion_01(2)\n\n9\n\n\n\n#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n\n\nfuncion_01(2,5)\n\n9\n\n\n\n\n\nCon Python 3 se puede especificar el tipo de parÃ¡metro y el tipo de retorno de una funciÃ³n (usando la notaciÃ³n PEP484 y PEP526. Se definen dos conceptos claves:\n\nEscritura dinÃ¡mica: no se especifican los atributos de los inputs ni de los ouputs\nEscritura estÃ¡tica: se especifican los atributos de los inputs y los ouputs\n\n\n#\n# escritura dinÃ¡mica\ndef suma(x,y):\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\n\n#\n# escritura estatica\ndef suma(x:float,\n         y:float)-&gt;float:\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\nPara la escritura estÃ¡tica, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la funciÃ³n puede recibir otros tipos de atributos.\n\nprint(suma(\"hola\",\" mundo\"))\n\nhola mundo\n\n\nPara validar los tipos de datos son los correctos, se deben ocupar librerÃ­as especializadas en la validaciÃ³n de datos (por ejemplo: pydantic).\n\n\n\nExisten librerÃ­as que pueden ayudar a corregir errores de escrituras en tÃº cÃ³digo (tambiÃ©n conocido como AnÃ¡lisis EstÃ¡tico), por ejemplo:\n\nblack: El formateador de cÃ³digo inflexible.\nflake8: La herramienta para aplicar la guÃ­a de estilo PEP8.\nmypy: Mypy es un verificador de tipo estÃ¡tico para Python 3.\n\n\n\n\n\nCasi tan importante como la escritura de cÃ³digo, es su correcta documentaciÃ³n, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el cÃ³digo explicando cÃ³mo funciona, el elemento bÃ¡sico de documentaciÃ³n de Python es el Docstring o cadena de documentaciÃ³n, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo despuÃ©s de la definiciÃ³n de funciÃ³n o clase que sirve de documentaciÃ³n a ese elemento.\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n\n\n# Acceso a la documentaciÃ³n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n    '\n\n\n\n# Acceso a la documentaciÃ³n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n\n\nLo correcto es detallar lo mejor posible en el Docstring quÃ© hace y cÃ³mo se usa la funciÃ³n o clase y los parÃ¡metros que necesita. Se recomienda usar el estilo de documentaciÃ³n del software de documentaciÃ³n sphinx, que emplea reStructuredText como lenguaje de marcado.\nVeamos un ejemplo de una funciÃ³n bien documentada:\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n\n\n# Acceso a la documentaciÃ³n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    &gt;&gt;&gt; potencia(2, 1)\\n    2\\n    &gt;&gt;&gt; potencia(3, 2)\\n    9\\n    '\n\n\n\n# Acceso a la documentaciÃ³n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n\n\n\nExisten varias formas de documentar tus funciones, las principales encontradas en la literatura son: * Google docstrings: forma de documentaciÃ³n recomendada por Google.. * reStructured Text: estÃ¡ndar oficial de documentaciÃ³n de Python; No es apto para principiantes, pero tiene muchas funciones. * NumPy/SciPy docstrings: combinaciÃ³n de NumPy de reStructured y Google Docstrings.\n\n\n\nEl Zen de Python te darÃ¡ la guÃ­a para decidir sobre que hacer con tu cÃ³digo, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.\nPrincipios importantes:\n\nExplÃ­cito es mejor que implÃ­cito: Que no se asuma nada, asegÃºrate que las cosas sean.\nSimple es mejor que complejo: Evita cÃ³digo complejo, cÃ³digo espagueti o que hace mas cosas para poder hacer una simple tarea.\nPlano es mejor que anidado: Si tu cÃ³digo tiene mas de 3 niveles de identaciÃ³n, deberÃ­as mover parte de ese cÃ³digo a una funciÃ³n.\nLos errores nunca deberÃ­an pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que implÃ­cito.\nSi la implementaciÃ³n es difÃ­cil de explicar, es mala idea.\n\nTambiÃ©n, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\n\n\n\nLos consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programaciÃ³n al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.\n\n\nPython al ser multiparadigma, nos da una amplia gama de posibilidades de diseÃ±ar nuestros cÃ³digos. Dentro de estos se destacan:\n\nProgramaciÃ³n orientada a objetos (OOP)\nProgramaciÃ³n funcional\n\nCuÃ¡ndo ocupar uno o la otra, va a depender de cÃ³mo queremos abordar una determinada problemÃ¡tica, puesto que en la mayorÃ­a de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).\n\n\n\nEn ingenierÃ­a de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acrÃ³nimo mnemÃ³nico introducido por Robert C. Martin a comienzos de la dÃ©cada del 2000 que representa cinco principios bÃ¡sicos de la programaciÃ³n orientada a objetos y el diseÃ±o. Cuando estos principios se aplican en conjunto es mÃ¡s probable que un desarrollador cree un sistema que sea fÃ¡cil de mantener y ampliar con el tiempo.\nEn el siguiente link se deja una guÃ­a para poder entender estos conceptos en python.\n\n\n\nLos patrones de diseÃ±o son la base para la bÃºsqueda de soluciones a problemas comunes en el desarrollo de software y otros Ã¡mbitos referentes al diseÃ±o de interacciÃ³n o interfaces.\n\nUn patrÃ³n de diseÃ±o es una soluciÃ³n a un problema de diseÃ±o.\n\nSe destacan tres tipos de patrones de diseÃ±os:\n\nComportamiento\nCreacionales\nEstructurales\n\nEn el siguiente link se deja una guÃ­a para poder entender estos conceptos en python.\n\n\n\n\n\nThe Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)\nClean Code: A Handbook of Agile Software - Robert C. Martin (2009).\nWorking effectively with legacy code Michael C. Feathers (2004)\nRefactoring Martin Fowler (1999)\nThe Pragmatic Programmer Thomas Hunt (1999)"
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#introducciÃ³n",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#introducciÃ³n",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "Una pregunta que surgue a menudo cuando uno se encuentra programando es saber cuÃ¡l es la forma correcta de programar. La respuesta es que no existe la forma correcta de programar (ya sea en Python o cualquier otro lenguaje), sin embargo, existen estandares dentro del mundo de la programaciÃ³n, con el fin de hacer el cÃ³digo mÃ¡s legible, sencillo de entender y ayudar a encontrar posibles errores.\nEn esta secciÃ³n se mostrarÃ¡ algunos conceptos sencillos que te ayudarÃ¡n a mejorar tus skills en el desarrollo de software (con Python)."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#estilo-de-codificaciÃ³n-pep8",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#estilo-de-codificaciÃ³n-pep8",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "El PEP8 es un estilo de codificaciÃ³n que proporciona convenciones de codificaciÃ³n para el cÃ³digo Python que comprende la biblioteca estÃ¡ndar en la distribuciÃ³n principal de Python.\nAlgunos aspectos importantes:\n\nEl PEP8 y el PEP 257 (Docstring Conventions) fueron adaptados del ensayo original de la GuÃ­a de estilo Python de Guido, con algunas adiciones de la guÃ­a de estilo de Barry.\nEsta guÃ­a de estilo evoluciona con el tiempo a medida que se identifican convenciones adicionales y las convenciones pasadas se vuelven obsoletas debido a cambios en el propio lenguaje.\nMuchos proyectos tienen sus propias pautas de estilo de codificaciÃ³n. En caso de conflicto, dichas guÃ­as especÃ­ficas del proyecto tienen prioridad para ese proyecto.\n\nBasados en el PEP8 y algunas buenas prÃ¡cticas del diseÃ±o de software, veamos ejemplo para poder escribir de mejor forma nuestros cÃ³digos.\n\n\nCuando sea posible, define variables con nombres que tengan algÃºn sentido o que puedas identificar fÃ¡cilmente, no importa que sean mÃ¡s largas. Por ejemplo, en un programa podrÃ­amos escribir:\n\na = 10.  \nb = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\npero, Â¿quÃ© significan a y b? lo sabemos por el comentario (bien hecho), pero si mÃ¡s adelante nos encontramos con esas variables, tendremos que recordar cual es cual. Es mejor usar nombres con significado:\n\naltura = 10.  \nbase = 3.5 \nprint(f\"El area es {a*b}\" )\n\nEl area es 35.0\n\n\n\n\n\nLas lÃ­neas de codigo no deben ser muy largas, como mucho 72 caracteres. Si se tiene una lÃ­nea larga, se puede cortar con una barra invertida (\\) y continuar en la siguiente lÃ­nea:\n\nprint(\"Esta es una frase muy larga, se puede cortar con un \\\n       y seguir en la lÃ­nea inferior.\")\n\nEsta es una frase muy larga, se puede cortar con un        y seguir en la lÃ­nea inferior.\n\n\n\n\n\nLos comentarios son muy importantes al escribir un programa. Describen lo que estÃ¡ sucediendo dentro de un programa, para que una persona que mira el cÃ³digo fuente no tenga dificultades para descifrarlo.\n\n#\n# esto es un comentario\nprint('Hola')\n\nHola\n\n\nTambiÃ©n podemos tener comentarios multilÃ­neas:\n\n#\n# Este es un comentario largo\n# y se extiende\n# a varias lÃ­neas\n\n\n\n\nLas importaciones generalmente deben estar en lÃ­neas separadas:\n\n#\n# no:\nimport sys, os\n\n\n#\n# si:\nimport os\nimport sys\n\n\n\n\nExisten varias formas de hacer comparaciones de objetos (principalmente en el uso del bucle if), acÃ¡ se dejan alguna recomendaciones:\n# no\nif greeting == True:\n\n# no\nif greeting is True:\n# si\nif greeting:\n\n\n\nDentro de parÃ©ntesis, corchetes o llaves, no dejar espacios inmediatamente dentro de ellos:\n\n#\n# no\nlista_01 = [1, 2, 3,4, 5, 6,7, 8, 9,]\n\n\n#\n# si \nlista_01 = [\n    1, 2, 3,\n    4, 5, 6,\n    7, 8, 9, \n]\n\nAunque en Python se pueden hacer varias declaraciones en una lÃ­nea, se recomienda hacer sÃ³lo una en cada lÃ­nea:\n\n#\n# no\na = 10; b = 20\n\n\n#\n# si\na = 10\nb = 20  \n\nCuando se trabaja con lista, conjuntos y/o tuplas se recomienda poner en cada lÃ­nea sus argumentos.\n\n#\n# no\nlista = [(1, 'hola'),(2, 'mundo'),]  \n\n\n#\n# si\nlista = [\n    (1, 'hola'),\n    (2, 'mundo'),\n]\n\nLo anterior se puede extender para funciones con muchos argumentos\n\n#\n# no\ndef funcion_01(x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n    x1,x2,x3,x4):\n    print(x1,x2,x3,x4)\n\n\n#\n# si\ndef funcion_01(x1,x2,\n               x3,x4):\n    \n    print(x1,x2,x3,x4)\n    \ndef funcion_02(\n        x1,x2,\n        x3,x4):\n    \n    print(x1,x2,x3,x4)\n\n\n\n\nUn tema interesante es corresponde a la identaciÃ³n respecto a los operadores binarios, acÃ¡ se muestra la forma correcta de hacerlo:\n# no\nincome = (gross_wages +\n          taxable_interest +\n          (dividends - qualified_dividends) -\n          ira_deduction -\n          student_loan_interest)\n# si\nincome = (gross_wages\n          + taxable_interest\n          + (dividends - qualified_dividends)\n          - ira_deduction\n          - student_loan_interest)\n\n\n\nAunque combinar iterables con elementos de control de flujo para manipular listas es muy sencillo con Python, hay mÃ©todos especÃ­ficos mÃ¡s eficientes para hacer lo mismo. Pensemos el fitrado de datos de una lista:\n\n#\n# Seleccionar los nÃºmeros positivos\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = []\nfor i in numeros:\n    if i &gt; 0:\n        positivos.append(i)\n        \nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\nAunque tÃ©cnicamente es correcto, es mÃ¡s eficiente hacer List Comprehension:\n\n#\n# comprension de lista\nnumeros = [-3, 2, 1, -8, -2, 7]\npositivos = [i for i in numeros if i &gt; 0] # List Comprehension\nprint(f\"positivos: {positivos}\")\n\npositivos: [2, 1, 7]\n\n\n\n\n\nCuando se ocupa try/except, es necesario especificar el tipo de error que se estÃ¡ cometiendo.\n\n#\n# importar librerias\nimport sys\n\n\n#\n# no\ntry:\n    r = 1/0\nexcept:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n#\n# si\ntry:\n    r = 1/0\nexcept ZeroDivisionError:\n    print(\"Oops! ocurrio un\",sys.exc_info()[0])\n\nOops! ocurrio un &lt;class 'ZeroDivisionError'&gt;\n\n\n\n\n\nSiempre es mejor definir las variables dentro de una funciÃ³n y no dejar variables globales.\n\n#\n# no\nvalor = 5\n\ndef funcion_01(variable):\n    return 2*variable + valor\n\n\nfuncion_01(2)\n\n9\n\n\n\n#\n# si\ndef funcion_01(variable,valor):\n    return 2*variable + valor\n\n\nfuncion_01(2,5)\n\n9\n\n\n\n\n\nCon Python 3 se puede especificar el tipo de parÃ¡metro y el tipo de retorno de una funciÃ³n (usando la notaciÃ³n PEP484 y PEP526. Se definen dos conceptos claves:\n\nEscritura dinÃ¡mica: no se especifican los atributos de los inputs ni de los ouputs\nEscritura estÃ¡tica: se especifican los atributos de los inputs y los ouputs\n\n\n#\n# escritura dinÃ¡mica\ndef suma(x,y):\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\n\n#\n# escritura estatica\ndef suma(x:float,\n         y:float)-&gt;float:\n    return x+y\n\n\nprint(suma(1,2))\n\n3\n\n\nPara la escritura estÃ¡tica, si bien se especifica el tipo de atributo (tanto de los inputs o outputs), la funciÃ³n puede recibir otros tipos de atributos.\n\nprint(suma(\"hola\",\" mundo\"))\n\nhola mundo\n\n\nPara validar los tipos de datos son los correctos, se deben ocupar librerÃ­as especializadas en la validaciÃ³n de datos (por ejemplo: pydantic).\n\n\n\nExisten librerÃ­as que pueden ayudar a corregir errores de escrituras en tÃº cÃ³digo (tambiÃ©n conocido como AnÃ¡lisis EstÃ¡tico), por ejemplo:\n\nblack: El formateador de cÃ³digo inflexible.\nflake8: La herramienta para aplicar la guÃ­a de estilo PEP8.\nmypy: Mypy es un verificador de tipo estÃ¡tico para Python 3."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#documentaciÃ³n",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#documentaciÃ³n",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "Casi tan importante como la escritura de cÃ³digo, es su correcta documentaciÃ³n, una parte fundamental de cualquier programa que a menudo se infravalora o simplemente se ignora. Aparte de los comentarios entre el cÃ³digo explicando cÃ³mo funciona, el elemento bÃ¡sico de documentaciÃ³n de Python es el Docstring o cadena de documentaciÃ³n, que ya hemos visto. Simplemente es una cadena de texto con triple comillas que se coloca justo despuÃ©s de la definiciÃ³n de funciÃ³n o clase que sirve de documentaciÃ³n a ese elemento.\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n    \"\"\"\n    return x**y\n\n\n# Acceso a la documentaciÃ³n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n    '\n\n\n\n# Acceso a la documentaciÃ³n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n\n\n\nLo correcto es detallar lo mejor posible en el Docstring quÃ© hace y cÃ³mo se usa la funciÃ³n o clase y los parÃ¡metros que necesita. Se recomienda usar el estilo de documentaciÃ³n del software de documentaciÃ³n sphinx, que emplea reStructuredText como lenguaje de marcado.\nVeamos un ejemplo de una funciÃ³n bien documentada:\n\ndef potencia(x, y):\n    \"\"\"\n    Calcula la potencia arbitraria de un numero\n\n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n    \"\"\"\n\n    return x**y\n\n\n# Acceso a la documentaciÃ³n\npotencia.__doc__\n\n'\\n    Calcula la potencia arbitraria de un numero\\n\\n    :param x: base\\n    :param y: exponente\\n    :return:  potencia de un numero\\n    :ejemplos:\\n    \\n    &gt;&gt;&gt; potencia(2, 1)\\n    2\\n    &gt;&gt;&gt; potencia(3, 2)\\n    9\\n    '\n\n\n\n# Acceso a la documentaciÃ³n\nhelp(potencia)\n\nHelp on function potencia in module __main__:\n\npotencia(x, y)\n    Calcula la potencia arbitraria de un numero\n    \n    :param x: base\n    :param y: exponente\n    :return:  potencia de un numero\n    :ejemplos:\n    \n    &gt;&gt;&gt; potencia(2, 1)\n    2\n    &gt;&gt;&gt; potencia(3, 2)\n    9\n\n\n\nExisten varias formas de documentar tus funciones, las principales encontradas en la literatura son: * Google docstrings: forma de documentaciÃ³n recomendada por Google.. * reStructured Text: estÃ¡ndar oficial de documentaciÃ³n de Python; No es apto para principiantes, pero tiene muchas funciones. * NumPy/SciPy docstrings: combinaciÃ³n de NumPy de reStructured y Google Docstrings."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#zen-de-python",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#zen-de-python",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "El Zen de Python te darÃ¡ la guÃ­a para decidir sobre que hacer con tu cÃ³digo, no te dice como lo debes escribir, sino como debes pensar si estas programando en Python.\nPrincipios importantes:\n\nExplÃ­cito es mejor que implÃ­cito: Que no se asuma nada, asegÃºrate que las cosas sean.\nSimple es mejor que complejo: Evita cÃ³digo complejo, cÃ³digo espagueti o que hace mas cosas para poder hacer una simple tarea.\nPlano es mejor que anidado: Si tu cÃ³digo tiene mas de 3 niveles de identaciÃ³n, deberÃ­as mover parte de ese cÃ³digo a una funciÃ³n.\nLos errores nunca deberÃ­an pasar silenciosamente: No uses un Try/Except sin definir que tipo de error vas a cachar, viene de la mano con Explicito es mejor que implÃ­cito.\nSi la implementaciÃ³n es difÃ­cil de explicar, es mala idea.\n\nTambiÃ©n, podemos ver el mensaje original del zen de python, ejecutando la siguiente linea de comando.\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#mÃ¡s-consejos",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#mÃ¡s-consejos",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "Los consejos que se presentan son de mucha utilidad si usted quiere llevar sus conociminetos de programaciÃ³n al siguiente nivel, sin embargo, el contenido de cada uno amerita un curso por si solo. Se deja recomienda al lector seguir profundizando en estos temas.\n\n\nPython al ser multiparadigma, nos da una amplia gama de posibilidades de diseÃ±ar nuestros cÃ³digos. Dentro de estos se destacan:\n\nProgramaciÃ³n orientada a objetos (OOP)\nProgramaciÃ³n funcional\n\nCuÃ¡ndo ocupar uno o la otra, va a depender de cÃ³mo queremos abordar una determinada problemÃ¡tica, puesto que en la mayorÃ­a de los casos, se puede pasar de un paradigma a o otro (incluso mezclarlos de ser necesario).\n\n\n\nEn ingenierÃ­a de software, SOLID (Single responsibility, Open-closed, Liskov substitution, Interface segregation and Dependency inversion) es un acrÃ³nimo mnemÃ³nico introducido por Robert C. Martin a comienzos de la dÃ©cada del 2000 que representa cinco principios bÃ¡sicos de la programaciÃ³n orientada a objetos y el diseÃ±o. Cuando estos principios se aplican en conjunto es mÃ¡s probable que un desarrollador cree un sistema que sea fÃ¡cil de mantener y ampliar con el tiempo.\nEn el siguiente link se deja una guÃ­a para poder entender estos conceptos en python.\n\n\n\nLos patrones de diseÃ±o son la base para la bÃºsqueda de soluciones a problemas comunes en el desarrollo de software y otros Ã¡mbitos referentes al diseÃ±o de interacciÃ³n o interfaces.\n\nUn patrÃ³n de diseÃ±o es una soluciÃ³n a un problema de diseÃ±o.\n\nSe destacan tres tipos de patrones de diseÃ±os:\n\nComportamiento\nCreacionales\nEstructurales\n\nEn el siguiente link se deja una guÃ­a para poder entender estos conceptos en python."
  },
  {
    "objectID": "posts/2021/2021-08-31-buenas_practicas.html#referencias",
    "href": "posts/2021/2021-08-31-buenas_practicas.html#referencias",
    "title": "Buenas PrÃ¡cticas - Python",
    "section": "",
    "text": "The Clean Coder: A Code Of Conduct For Professional Programmers Robert C. Martin (2011)\nClean Code: A Handbook of Agile Software - Robert C. Martin (2009).\nWorking effectively with legacy code Michael C. Feathers (2004)\nRefactoring Martin Fowler (1999)\nThe Pragmatic Programmer Thomas Hunt (1999)"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Main objective is understand of the best way the challenge LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\nThe steps to follow are:\n\nOverview of the Dataset: Understanding the datasets available.\nPreprocessing: Preprocessing of the datasets available.\nEDA: Exploratory data analysis using visualization tools in Python.\n\n\nNote: My analysis is inspired by several of the notebooks that different profiles have uploaded to the challenge, so some graphics or images belong to these authors. The most important ones will be found in the references. On the other hand, my project is available in Jupyter Book, click in the following link.\n\n\n\nThe objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\nFile: File name (.csv).\nShape: Dimensionality of datasets.\nDescription: Basic description of the dataset.\nTop 5 rows: Show first 5 rows + explanation for some columns.\nSummary: Summary of datasets.\n\n::: {#d55eb0e5 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T21:59:13.410181Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T21:59:13.316042Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T21:59:41.807446Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T21:59:41.806324Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:48:09.998997Zâ€}â€™ papermill=â€˜{â€œdurationâ€:28.514116,â€œend_timeâ€:â€œ2021-08-24T21:59:41.807642â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T21:59:13.293526â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=1}\n# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n:::\n\n\n\nFile: districts_info.csv.\nShape: \\(233\\) rows \\(\\times\\) \\(7\\) columns.\nDescription: file contains information about each school district.\nTop 5 rows::\n\n\n\nSummary:\n\n\n\n\n\n\nFile: products_info.csv\nShape: \\(372\\) rows \\(\\times\\) \\(6\\) columns.\nDescription: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nFile: engagement_data/*.csv.\nShape: \\(22324190\\) rows \\(\\times\\) \\(5\\) columns.\nDescription: file contains information about each school district. The files can be joined by the key columns district_id and lp_id.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nPreprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.\n\none-hot encoding the product sectors\nsplitting up the primary essential function into main and sub category\n\n\nNote: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.\n\n\n::: {#0578f18e .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T21:59:42.315509Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T21:59:42.314913Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:00.669027Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:00.669504Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:48:44.902736Zâ€}â€™ papermill=â€˜{â€œdurationâ€:18.384236,â€œend_timeâ€:â€œ2021-08-24T22:00:00.669700â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T21:59:42.285464â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=2}\n# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n:::\n\n\n\nExploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\nVisualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy Grays scale next to the technique: dark text on a light background.\n\nNote: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.\n\n\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n::: {#74a2ee97 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:00.788039Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:00.787339Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:00.944794Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:00.944141Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.148181Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.179153,â€œend_timeâ€:â€œ2021-08-24T22:00:00.944932â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:00.765779â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=3}\n# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\n                                                \n\n:::\n::: {#7eac1448 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:01.006598Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:01.005538Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:01.460650Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:01.461172Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.338218Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.494452,â€œend_timeâ€:â€œ2021-08-24T22:00:01.461333â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:00.966881â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=4}\n# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nLocales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).\nFor the pct_black/hispanic variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\nFor pctfree/reduced and pp_total_raw indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n::: {#a15318df .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:01.557765Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:01.556848Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:03.160447Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:03.160919Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.865498Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.630377,â€œend_timeâ€:â€œ2021-08-24T22:00:03.161103â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:01.530726â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=5}\n# heatmap: districts -&gt; locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nSectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categorÃ­a mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the primary_function_main variable, all sectors are focused on theLC category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.\n::: {#84e6e3f2 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:03.301514Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:03.292826Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:03.796850Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:03.797314Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:17.574316Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.551066,â€œend_timeâ€:â€œ2021-08-24T22:00:03.797481â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:03.246415â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=6}\nplt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nContinuing the analysis of the primary_function_main variable, it was observed that most of these are in theLC category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is Sites, Resources & Reference (101).\n::: {#c0c96d39 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:03.924145Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:03.923467Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.076527Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.077022Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.054662Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.190305,â€œend_timeâ€:â€œ2021-08-24T22:00:04.077195â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:03.886890â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=7}\n# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#c52db360 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:04.155626Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:04.153843Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.430687Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.431156Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.238340Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.322608,â€œend_timeâ€:â€œ2021-08-24T22:00:04.431319â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:04.108711â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=8}\n# pieplot: products -&gt; subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n258 providers have 1 occurrences.\n18 providers have 2 occurrences.\n9 providers have 3 occurrences.\n2 providers have 4 occurrences.\n2 providers have 6 occurrences.\n1 provider have 30 occurrences.\n\nBased on this, only the top 15 providers will be displayed.\n::: {#8cf15baa .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:04.610677Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:04.586018Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.945261Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.945715Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.515334Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.414758,â€œend_timeâ€:â€œ2021-08-24T22:00:04.945874â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:04.531116â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=9}\ndct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nWith regard to products, there are about 372 different products.\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the product_name variable.\n::: {#53d5558a .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:05.106485Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:05.104433Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:06.068287Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:06.068956Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.935625Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.012728,â€œend_timeâ€:â€œ2021-08-24T22:00:06.069135â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:05.056407â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=10}\ncloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n\n\n\n\n\n\n\n:::\nTo understand more in detail the use of these products, we will analyze the use of these products with respect to the variable engagement_index. The first graph is related to the average engagement_index (per student) for the year 2020, where the first 15 products will be displayed.\nAn important fact is that 362 products have an average of less than 1!.\n::: {#4c2d24bf .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:06.276148Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:06.275229Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:07.642119Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:07.642568Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:19.888566Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.424686,â€œend_timeâ€:â€œ2021-08-24T22:00:07.642750â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:06.218064â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=11}\ngroup_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nLetâ€™s study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\nNote: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.\n\n::: {#9d39e70f .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:08.316803Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:08.316106Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:13.858922Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:13.859377Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:21.992666Zâ€}â€™ papermill=â€˜{â€œdurationâ€:6.060783,â€œend_timeâ€:â€œ2021-08-24T22:00:13.859531â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:07.798748â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=12}\ncol = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n\n\n\n\n\n\n\n:::\nNow, we can understand the engagement index for the most important tools about districts, where the districts of * Wisconsin ,  Missouri * and * Virginia * have the highest engagement index among the three most used tools.\n::: {#d1e1ba17 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:14.637147Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:14.636457Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:17.454954Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:17.455400Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:29.563655Zâ€}â€™ papermill=â€˜{â€œdurationâ€:3.421243,â€œend_timeâ€:â€œ2021-08-24T22:00:17.455572â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:14.034329â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=13}\ngroup_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\n\n\nDepending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\nWhen looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning\n\n\n\n\n\nDiverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland\nKaggle Competitions:\n\nEnthusiast to Data Professional - What changes?\nHow To Approach Analytics Challenges\nMost popular tools in 2020 Digital Learning"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#overview-of-the-dataset",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#overview-of-the-dataset",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "The objective of this section is to be able to read and give an interpretation to each one of the available datasets, analyzing column by column. For each dataset we will make a brief description:\n\nFile: File name (.csv).\nShape: Dimensionality of datasets.\nDescription: Basic description of the dataset.\nTop 5 rows: Show first 5 rows + explanation for some columns.\nSummary: Summary of datasets.\n\n::: {#d55eb0e5 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T21:59:13.410181Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T21:59:13.316042Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T21:59:41.807446Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T21:59:41.806324Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:48:09.998997Zâ€}â€™ papermill=â€˜{â€œdurationâ€:28.514116,â€œend_timeâ€:â€œ2021-08-24T21:59:41.807642â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T21:59:13.293526â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=1}\n# libraries\nimport glob\nimport re\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# read data\n\n## products dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/'\nproducts_df = pd.read_csv(path + \"products_info.csv\")\nproducts_df.columns = [x.lower().replace(' ','_') for x in products_df.columns]\n\n## districts dataset\ndistricts_df = pd.read_csv(path +\"districts_info.csv\")\n#districts_df.state = districts_df.state.replace('District Of Columbia','District of Columbia')\n\n## engagement dataset\npath = '../input/learnplatform-covid19-impact-on-digital-learning/engagement_data/' \nall_files = glob.glob(path + \"/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    district_id = filename.split(\"/\")[-1].split(\".\")[0]\n    df[\"district_id\"] = district_id\n    li.append(df)\nengagement_df = pd.concat(li)\nengagement_df = engagement_df.reset_index(drop=True)\n\n# summary\n\ndf_list = [\n    districts_df,\n    products_df,\n    engagement_df\n]\n\ndf_name = [\n    'districts_df',\n    'products_df',\n    'engagement_df'\n]\n\ncols = [\n    'dataframe',\n    'column', \n    'dtype', \n    'Non-Null Count', \n    'Null Count',\n    'unique'\n    \n]\n\nframes=[]\n\n\nfor i in range(len(df_list)):\n    df = df_list[i].copy()\n    a = df.dtypes.reset_index().rename(columns = {'index':'column',0:'dtype'})\n    b = df.count().reset_index().rename(columns = {'index':'column',0:'Non-Null Count'})\n    c = df.isnull().sum().reset_index().rename(columns = {'index':'column',0:'Null Count'})\n    temp = a.merge(b,on = 'column').merge(c,on = 'column')\n    \n    dct = {col: len(df[col].unique()) for col in df.columns}\n    df_unique = pd.DataFrame({\n    'column':dct.keys(),\n    'unique':dct.values(),\n    })\n    temp = temp.merge(df_unique,on = 'column')\n    temp['dataframe'] = df_name[i]\n    frames.append(temp)\n:::\n\n\n\nFile: districts_info.csv.\nShape: \\(233\\) rows \\(\\times\\) \\(7\\) columns.\nDescription: file contains information about each school district.\nTop 5 rows::\n\n\n\nSummary:\n\n\n\n\n\n\nFile: products_info.csv\nShape: \\(372\\) rows \\(\\times\\) \\(6\\) columns.\nDescription: for each school district, there is an additional file that contains the engagement for each tool for everyday in 2020.\nTop 5 rows:: \nSummary:\n\n\n\n\n\n\nFile: engagement_data/*.csv.\nShape: \\(22324190\\) rows \\(\\times\\) \\(5\\) columns.\nDescription: file contains information about each school district. The files can be joined by the key columns district_id and lp_id.\nTop 5 rows:: \nSummary:"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#preprocessing",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#preprocessing",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Preprocessing is an important step in any analytics competition. It helps you to handle your data more efficiently. However, please note that the way I preprocess the data may not be suited for your analysis purposes. Therefore, before you begin preprocessing your data, think about which data you would like to keep and/or modify and which data is not relevant for your analysis.\n\none-hot encoding the product sectors\nsplitting up the primary essential function into main and sub category\n\n\nNote: Preprocessing varies if you see other notebooks of this challenge. The processing will depend on the understanding of each of the datasets and the extra information that you may have.\n\n\n::: {#0578f18e .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T21:59:42.315509Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T21:59:42.314913Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:00.669027Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:00.669504Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:48:44.902736Zâ€}â€™ papermill=â€˜{â€œdurationâ€:18.384236,â€œend_timeâ€:â€œ2021-08-24T22:00:00.669700â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T21:59:42.285464â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=2}\n# products_df\n\nproducts_df['primary_function_main'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_df['primary_function_sub'] = products_df['primary_essential_function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_df['primary_function_sub'] = products_df['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\n#products_df.drop(\"Primary Essential Function\", axis=1, inplace=True)\n\ntemp_sectors = products_df['sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_df = products_df.join(temp_sectors)\n#products_df.drop(\"Sector(s)\", axis=1, inplace=True)\n\n#del temp_sectors\n\n# engagement_df\n\nengagement_df['time'] = pd.to_datetime(engagement_df['time'])\n\ntemp = engagement_df[['time']].drop_duplicates('time')\ntemp['week'] = temp['time'].apply(lambda x: x.isocalendar()[1])\nengagement_df = engagement_df.merge(temp,on ='time')\n\nengagement_df['lp_id'] = engagement_df['lp_id'].fillna(-1).astype(int)\nengagement_df['district_id'] = engagement_df['district_id'].fillna(-1).astype(int)\n\nengagement_df_mix = engagement_df.merge(\n    districts_df[['district_id','state']],\n    on = 'district_id'\n)\nengagement_df_mix = engagement_df_mix.merge(\n    products_df[['lp_id','product_name','sector_Corporate', 'sector_HigherEd','sector_PreK-12']],\n    on = 'lp_id'\n)\n:::"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#eda",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#eda",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Exploratory data analysis is the most important part of the challenge, since this will make the difference between the winner and the other participants. You should keep in mind that your visualizations must be able to simply and easily summarize the datasets. Also, it is hoped that the proposed visualizations can help to understand behaviors that are not easy to analyze with a simple table.\nVisualizations will be made in matplotlib, seaborn y plotly. Based on the article by Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland, we will occupy Grays scale next to the technique: dark text on a light background.\n\nNote: Visualizations made on this notebook are static. You can use different tools to be able to make dynamic visualizations (Altair, plotly, etc.). You can also perform tools like Streamlit to make Dashboards. On the other hand, if you fully understand python visualization tools and have knowledge of HTML/CSS, you can make beautiful notebook presentations like this one.\n\n\n\nFirst of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. . The states with the most available school districts are CT (30) and UT (29) while there are also states with only one school district (FL, TN, NY, AZ).\n::: {#74a2ee97 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:00.788039Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:00.787339Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:00.944794Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:00.944141Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.148181Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.179153,â€œend_timeâ€:â€œ2021-08-24T22:00:00.944932â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:00.765779â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=3}\n# map plot: districts\n\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_df['state_abbrev'] = districts_df['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_df['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\ntemp = pd.DataFrame({\n    'state_abbrev':us_state_abbrev.values(),\n})\n\ntemp = temp.merge(districts_info_by_state,on='state_abbrev',how='left').fillna(0)\ntemp['num_districts'] = temp['num_districts'].astype(int)\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    title_font_color=\"black\",\n    geo_scope='usa',\n)    \n\n\nfig.add_trace(\n    go.Choropleth(\n        locations=temp.state_abbrev,\n        zmax=1,\n        z = temp.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='black',\n        geo='geo',\n        colorscale=px.colors.sequential.Greys, \n        \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\n                                                \n\n:::\n::: {#7eac1448 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:01.006598Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:01.005538Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:01.460650Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:01.461172Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.338218Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.494452,â€œend_timeâ€:â€œ2021-08-24T22:00:01.461333â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:00.966881â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=4}\n# bar plot: districts\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.countplot(\n    y=\"state\",\n    data=districts_df,\n    order=districts_df.state.value_counts().index,\n    palette=\"Greys_d\",\n    linewidth=3\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=16)\n\n#Text\nplotting.text(x = -5, y = -4.2, s = \"State Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nplotting.text(x = -5, y = -3, s = \"Distribution of United States\",fontsize = 16, alpha = .85)\nplotting.text(x = 31.2, y = 0.08, s = 'Highest', weight = 'bold',fontsize = 14)\nplotting.text(x = 1.7, y = 22.3, s = 'Lowest', weight = 'bold',fontsize = 14)\n\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\n\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nLocales are separated into 4 categories: Suburb,Rural, City and Town, where most of the locales are concentrated in the Suburb category (104).\nFor the pct_black/hispanic variable, Rural and Town categories concentrate their entire population close to the interval $ [0,0.2 [$, while for the others sectors this percentage is varied.\nFor pctfree/reduced and pp_total_raw indicators, the distribution for each location is different, although they tend to focus on a particular interval.\n::: {#a15318df .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:01.557765Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:01.556848Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:03.160447Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:03.160919Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:15.865498Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.630377,â€œend_timeâ€:â€œ2021-08-24T22:00:03.161103â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:01.530726â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=5}\n# heatmap: districts -&gt; locale\n\ntemp = districts_df.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\ntemp1 = districts_df.groupby('locale')['pct_black/hispanic'].value_counts().to_frame()\ntemp1.columns = ['amount']\n\ntemp1 = temp1.reset_index(drop=False)\ntemp1 = temp1.pivot(index='locale', columns='pct_black/hispanic')['amount']\n\ntemp2 = districts_df.groupby('locale')['pct_free/reduced'].value_counts().to_frame()\ntemp2.columns = ['amount']\n\ntemp2 = temp2.reset_index(drop=False)\n\ntemp2 = temp2.pivot(index='locale', columns='pct_free/reduced')['amount']\n\nplt.style.use('default')\n\nfig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2, figsize=(24,18))\n\nsns.countplot(data=districts_df, x='locale', ax=ax1, palette='Greys_d')\nax1.text(x = -0.5, y = 120, s = \"Locale Distribution\",fontsize = 24, weight = 'bold', alpha = .90);\nax1.xaxis.set_tick_params(labelsize=16)\n\nfor container in ax1.containers:\n    ax1.bar_label(container,fontsize=16)\n\nsns.heatmap(temp1.fillna(0), annot=True,  cmap='Greys', ax=ax2,annot_kws={\"fontsize\":14})\nax2.set_title('Heatmap: locale and pct_black/hispanic',fontsize=16,loc='left')\nax2.xaxis.set_tick_params(labelsize=16)\nax2.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp.fillna(0), annot=True,  cmap='Greys', ax=ax3,annot_kws={\"fontsize\":14})\nax3.set_title('Heatmap: locale and pp_total_raw',fontsize=16,loc='left')\nax3.xaxis.set_tick_params(labelsize=16)\nax3.yaxis.set_tick_params(labelsize=16)\n\nsns.heatmap(temp2.fillna(0), annot=True,  cmap='Greys', ax=ax4,annot_kws={\"fontsize\":14})\nax4.set_title('Heatmap: locale and pct_free/reduced',fontsize=16,loc='left')\nax4.xaxis.set_tick_params(labelsize=16)\nax4.yaxis.set_tick_params(labelsize=16)\n\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nSectors are separated into 3 categories: sector_Corporate, sector_HigherEd and sector_PreK-12, donde la categorÃ­a mayoritaria corresponde a sector_PreK-12 (350). On the other hand, analyzing the primary_function_main variable, all sectors are focused on theLC category. It is worth mentioning that the distribution of the other categories remains almost the same between sectors.\n::: {#84e6e3f2 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:03.301514Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:03.292826Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:03.796850Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:03.797314Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:17.574316Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.551066,â€œend_timeâ€:â€œ2021-08-24T22:00:03.797481â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:03.246415â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=6}\nplt.style.use('default')\nnames = ['sector_Corporate', 'sector_HigherEd','sector_PreK-12']\ncounts = [products_df[x].sum() for x in names]\n\ntemp_bar = pd.DataFrame({\n    'sector':names,\n    'count':counts\n}).sort_values('count',ascending = False)\n\ntemp = products_df.groupby('primary_function_main')[names].sum()\n\n#fig, [ax1, ax2 ]= plt.subplots(nrows=1, ncols=2, figsize=(12,6))\nplt.figure(figsize=(18,18))\n\nplt.subplot(3,2,1)\nax = sns.barplot(x=\"sector\", y=\"count\", data=temp_bar,palette ='Greys_d')\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=12)\n\nplt.subplot(3,2,2)\n\nsns.heatmap(temp.T, annot=True,  cmap='Greys',annot_kws={\"fontsize\":10},fmt='g')\n\nplt.text(x = -6, y = -0.25, s = \"Sectors Distribution\",fontsize = 18, weight = 'bold', alpha = .90);\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nContinuing the analysis of the primary_function_main variable, it was observed that most of these are in theLC category (77%). Within this category, its subcategory is analyzed, where the predominant subcategory is Sites, Resources & Reference (101).\n::: {#c0c96d39 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:03.924145Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:03.923467Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.076527Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.077022Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.054662Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.190305,â€œend_timeâ€:â€œ2021-08-24T22:00:04.077195â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:03.886890â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=7}\n# pieplot: products\n\ncolor = [\n    'darkgray',\n    'silver',\n    'lightgray',\n    'gainsboro',\n]\n\nproducts_df[\"primary_function_main\"].value_counts().plot(\n    kind = 'pie', \n    autopct='%1d%%', \n    figsize=(6,6), \n    colors=color,\n    wedgeprops={\"edgecolor\":\"k\",'linewidth': 0.8,},\n    textprops={'color':\"black\"},\n    startangle=0)\nplt.text(x = -1.4, y = 1.1, s = \"Categories\",fontsize = 18, weight = 'bold', alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::\n::: {#c52db360 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:04.155626Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:04.153843Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.430687Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.431156Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.238340Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.322608,â€œend_timeâ€:â€œ2021-08-24T22:00:04.431319â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:04.108711â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=8}\n# pieplot: products -&gt; subcategories\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\ntemp = products_df[products_df.primary_function_main == 'LC']\nax = sns.countplot(\n    data=temp, \n    y='primary_function_sub',\n    order=temp.primary_function_sub.value_counts().index,\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=16)\n\n\n#plt.title('Sub-Categories in Primary Function LC')\nplt.text(x = -50, y = -0.8, \n         s = \"Sub-Categories in Primary Function LC\",fontsize = 24, weight = 'bold', alpha = .90);\n\nplt.text(x = 105, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 7, y = 6, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\n\nplt.show()\n\n\n\n\n\n\n\n:::\n\n\n\nAfter understanding the functionality of each of the tools, it is necessary to understand the distribution of the tools. The first thing is to study the distribution of the providers of the products we have, where:\n\n258 providers have 1 occurrences.\n18 providers have 2 occurrences.\n9 providers have 3 occurrences.\n2 providers have 4 occurrences.\n2 providers have 6 occurrences.\n1 provider have 30 occurrences.\n\nBased on this, only the top 15 providers will be displayed.\n::: {#8cf15baa .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:04.610677Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:04.586018Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:04.945261Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:04.945715Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.515334Zâ€}â€™ papermill=â€˜{â€œdurationâ€:0.414758,â€œend_timeâ€:â€œ2021-08-24T22:00:04.945874â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:04.531116â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=9}\ndct = {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n}\n\ntemp = products_df['provider/company_name'].value_counts().reset_index()\ntemp.columns = ['provider/company_name','count']\ntemp = temp.replace( {\n    'Savvas Learning Company | Formerly Pearson K12 Learning': 'Savvas Learning Company'\n})\n\nn = 15\ntemp = temp.sort_values('count',ascending = False).head(n)\n\nplt.style.use('default')\nplt.figure(figsize=(18,8))\n\nax = sns.barplot(\n    data=temp, \n    y='provider/company_name',\n    x='count',\n    palette ='Greys_d'\n)\n\nfor container in ax.containers:\n    ax.bar_label(container,fontsize=15)\n    \nplt.text(x = -7, y = -1, \n         s = f\"Top {n} provider/company name\",fontsize = 20, weight = 'bold', alpha = .90);\n   \nplt.text(x = 31, y =0.08, s = 'Highest', weight = 'bold',fontsize=16)\nplt.text(x = 3, y = 14.2, s = 'Lowest', weight = 'bold',fontsize=16)\nplt.yticks(fontsize=16)\n\n\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nWith regard to products, there are about 372 different products.\nWe can make a word cloud to be able to analyze in a different way, words by themselves that are repeated the most in the product_name variable.\n::: {#53d5558a .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:05.106485Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:05.104433Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:06.068287Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:06.068956Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:18.935625Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.012728,â€œend_timeâ€:â€œ2021-08-24T22:00:06.069135â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:05.056407â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=10}\ncloud = WordCloud(\n    width=1080,\n    height=270,\n    colormap='Greys',\n    background_color='white'\n    ).generate(\" \".join(products_df['product_name'].astype(str)))\n\nplt.figure(figsize=(22, 10))\nplt.imshow(cloud)\nplt.axis('off');\n\n\n\n\n\n\n\n:::\nTo understand more in detail the use of these products, we will analyze the use of these products with respect to the variable engagement_index. The first graph is related to the average engagement_index (per student) for the year 2020, where the first 15 products will be displayed.\nAn important fact is that 362 products have an average of less than 1!.\n::: {#4c2d24bf .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:06.276148Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:06.275229Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:07.642119Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:07.642568Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:19.888566Zâ€}â€™ papermill=â€˜{â€œdurationâ€:1.424686,â€œend_timeâ€:â€œ2021-08-24T22:00:07.642750â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:06.218064â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=11}\ngroup_01 = (engagement_df_mix.groupby('product_name')['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\ngroup_01['engagement_index'] = group_01['engagement_index'].apply(lambda x: round(x,2))\nless_1 = len(group_01.loc[lambda x:x['engagement_index']&lt;1])\n\n\nplt.style.use('default')\nplt.figure(figsize=(14,8))\n\nplotting = sns.barplot(\n    y=\"product_name\",\n    x = \"engagement_index\",\n    data=group_01.head(20),\n    palette=\"Greys_d\",\n\n)\n\nfor container in plotting.containers:\n    plotting.bar_label(container,fontsize=14)\n\nplt.text(x = -3.5, y = -3, \n         s = \"Mean daily page-load events in top 20 tools\",fontsize = 20, weight = 'bold', alpha = .90);\n\nplt.text(x = -3.5, y = -2, \n         s = \"per 1 student\",fontsize = 14,  alpha = .90);\n\nplt.text(x = 11, y =0.1, s = 'Highest', weight = 'bold',fontsize=14)\nplt.text(x = 1, y = 19.2, s = 'Lowest', weight = 'bold',fontsize=14)\nplt.yticks(fontsize=16)\nplt.xticks(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n:::\nLetâ€™s study the temporal behavior (at the level of weeks) of these tools during the year 2020, where the most three used tools will be shown with different colors, while the other tools will be visualized but with the same color (in order to understand their distribution).\n\nNote: The proposed analysis can be carried out at the day level and analyzing through time series each of the tools during the year 2020.\n\n::: {#9d39e70f .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:08.316803Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:08.316106Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:13.858922Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:13.859377Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:21.992666Zâ€}â€™ papermill=â€˜{â€œdurationâ€:6.060783,â€œend_timeâ€:â€œ2021-08-24T22:00:13.859531â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:07.798748â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=12}\ncol = 'week'\n\ngroup_04  = (engagement_df_mix.groupby(['product_name',col])['engagement_index'].mean()/1000).reset_index().sort_values('engagement_index',ascending = False)\n\ng_high = group_01.head(3)['product_name']\ngroup_04_top = group_04.loc[lambda x: x.product_name.isin(g_high)]\n\nstates = group_04['product_name'].unique()\ntimes= group_04[col].unique()\n\nindex = pd.MultiIndex.from_product([states,times], names = [\"product_name\", col])\n\ndf_complete = pd.DataFrame(index = index).reset_index().fillna(0)\n\ngroup_04 = df_complete.merge(group_04,on = ['product_name',col],how='left').fillna(0)\n\nn = 3\ng_high = group_04.groupby('product_name')['engagement_index'].sum().sort_values(ascending=False).head(n).index.to_list()\n\n\ncolors = [    \n    'lightgray', \n    'dimgray', \n    'black', \n    'firebrick', \n    'darkred']\npalette_01 = {x:'lavender' for x in group_04['product_name'].unique() if x not in g_high}\npalette_02 = {g_high[i]:colors[i] for i in range(n)}\n\nplt.style.use('default')\nplt.figure(figsize=(20,6))\n\n\nsns.lineplot(\n    data=group_04.loc[lambda x: ~x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    legend = False,\n    palette=palette_01,\n    linewidth = 1.\n\n    )\n\nsns.lineplot(\n    data=group_04.loc[lambda x: x.product_name.isin(g_high)], \n    x=col, \n    y=\"engagement_index\", \n    hue='product_name',\n    palette=palette_02,\n    linewidth = 1.\n\n    )\n\n\nplt.text(x = -2, y =23.7, s = 'Mean daily page-load events in top 3 tools', weight = 'bold',fontsize=14)\nplt.text(x = -2, y =22.3, s = 'by products and time, per 1 student',fontsize=12)\n\n\nplt.text(x = 12, y =20.7, s = '1,000 cases of COVID', weight = 'bold',fontsize=8)\nplt.text(x = 37, y =20.7, s = '1st September', weight = 'bold',fontsize=8)\n\n\nplt.axvline(x = 11, color = 'black', linestyle='--',linewidth = 0.5)\nplt.axvline(x = 36, color = 'black', linestyle='--',linewidth = 0.5)\nplt.show()\n\n\n\n\n\n\n\n:::\nNow, we can understand the engagement index for the most important tools about districts, where the districts of * Wisconsin ,  Missouri * and * Virginia * have the highest engagement index among the three most used tools.\n::: {#d1e1ba17 .cell _kg_hide-input=â€˜trueâ€™ execution=â€˜{â€œiopub.execute_inputâ€:â€œ2021-08-24T22:00:14.637147Zâ€,â€œiopub.status.busyâ€:â€œ2021-08-24T22:00:14.636457Zâ€,â€œiopub.status.idleâ€:â€œ2021-08-24T22:00:17.454954Zâ€,â€œshell.execute_replyâ€:â€œ2021-08-24T22:00:17.455400Zâ€,â€œshell.execute_reply.startedâ€:â€œ2021-08-24T21:49:29.563655Zâ€}â€™ papermill=â€˜{â€œdurationâ€:3.421243,â€œend_timeâ€:â€œ2021-08-24T22:00:17.455572â€,â€œexceptionâ€:false,â€œstart_timeâ€:â€œ2021-08-24T22:00:14.034329â€,â€œstatusâ€:â€œcompletedâ€}â€™ tags=â€˜[â€œhide-inputâ€]â€™ execution_count=13}\ngroup_02 = (engagement_df_mix.groupby(['state','product_name'])['engagement_index'].mean()/1000)\\\n            .reset_index().sort_values('engagement_index',ascending = False).fillna(0)\n\ngripo_02_top = group_02.loc[lambda x: x.product_name.isin(g_high)]\ngripo_02_top['engagement_index'] = gripo_02_top['engagement_index'].apply(lambda x: round(x,2))\n#gripo_02_top = gripo_02_top.loc[lambda x: x['engagement_index']&gt;0]\n\n\nplt.style.use('default')\n\ng = sns.FacetGrid(gripo_02_top,hue='product_name',col = 'product_name',height=4, col_wrap= 3  )\ng.map(sns.barplot, \"engagement_index\",\"state\", palette=\"Greys_d\",)\n\ng.fig.set_size_inches(15, 8)\ng.fig.subplots_adjust(top=0.81, right=0.86)\n\naxes = g.axes.flatten()\nfor ax in axes:\n    for container in ax.containers:\n        ax.bar_label(container,fontsize=8)\n\n\nplt.text(x = -50, y = -4, s = \"Mean daily page-load events in top 3 tools\",fontsize = 16, weight = 'bold', alpha = .90);\nplt.text(x = -50, y = -3, s = \"by state and products, per 1 student\",fontsize = 14,  alpha = .90);\nplt.show()\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#summary",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#summary",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Depending on what you want to achieve you might want to carefully preselect districts. Note that we approach in this notebook might not necessarily suit your individual purposes.\nWhen looking at digital learning, you might want to spend sometime in figuring out which districts actually applied digital learning"
  },
  {
    "objectID": "posts/2021/basic-analysis-impact-on-digital-learning.html#references",
    "href": "posts/2021/basic-analysis-impact-on-digital-learning.html#references",
    "title": "Impact on Digital Learning",
    "section": "",
    "text": "Diverging Color Maps for Scientific Visualization (Expanded) - Kenneth Moreland\nKaggle Competitions:\n\nEnthusiast to Data Professional - What changes?\nHow To Approach Analytics Challenges\nMost popular tools in 2020 Digital Learning"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html",
    "href": "posts/2021/2021-07-31-jupyter.html",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar cÃ³digo en Python (por defecto, aunque permite otros lenguajes) de manera dinÃ¡mica, a la vez que integrar en un mismo documento tanto bloques de cÃ³digo como texto, grÃ¡ficas o imÃ¡genes. Es un SaaS utilizado ampliamente en anÃ¡lisis numÃ©rico, estadÃ­stica y machine learning, entre otros campos de la informÃ¡tica y las matemÃ¡ticas.\nPor otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz mÃ¡s interesante para los usuarios. Eventualmente Jupyter Lab reemplazarÃ¡ a Jupyter Notebok.\nNos centraremos en comprender aspectos bÃ¡sicos de cÃ³mo trabajar un archivo en jupyter notebook (extensiÃ³n .ipynb).\n\n\n\n\n\nPara instalar RISE, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge notebook\nDe lo contrario, puede instalar con pip:\npip install notebook\n\nNota: SI desea instalar JupyterLab, simplemente reemplaza notebook por jupyterlab.\n\n\n\n\n\n\n\nUna vez que haya instalado Jupyter Notebook en su computadora, estarÃ¡ listo para ejecutar el servidor de la computadora portÃ¡til. Puede iniciar el servidor del portÃ¡til desde la lÃ­nea de comandos (usando Terminal en Mac/Linux, SÃ­mbolo del sistema en Windows) ejecutando:\njupyter notebook\n\nEsto imprimirÃ¡ cierta informaciÃ³n sobre el servidor en su terminal, incluida la URL de la aplicaciÃ³n web (de forma predeterminada, http://localhost:8888):\n$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nA continuaciÃ³n, abrirÃ¡ su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, verÃ¡ el Panel, que mostrarÃ¡ una lista de notebooks, archivos y subdirectorios en el directorio donde se iniciÃ³ el servidor.\n\nLa parte superior de la lista de notebooks se muestran rutas de navegaciÃ³n en las que se puede hacer clic del directorio actual.\nPara crear un nuevo notebook, haga clic en el botÃ³n New en la parte superior de la lista y seleccione el kernel del menÃº desplegable (como se ve a continuaciÃ³n). Los kernels que se enumeran dependen de lo que estÃ© instalado en el servidor.\n\nNota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opciÃ³n para usted.\n\n\nUna vez seleccionado el kernel, se abrira nuestro primer notebook!.\n\n\n\n\nJupyter notebook nos ofrece el siguiente toolbox:\n\n\nFile: En Ã©l, puede crear un nuevo cuaderno o abrir uno preexistente. AquÃ­ es tambiÃ©n a donde irÃ­a para cambiar el nombre de un Cuaderno. Creo que el elemento de menÃº mÃ¡s interesante es la opciÃ³n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.\nEdit: AquÃ­ puede cortar, copiar y pegar celdas. AquÃ­ tambiÃ©n es donde irÃ­as si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aquÃ­ tambiÃ©n.\nView: es Ãºtil para alternar la visibilidad del encabezado y la barra de herramientas. TambiÃ©n puede activar o desactivar los nÃºmeros de lÃ­nea dentro de las celdas. AquÃ­ tambiÃ©n es donde irÃ­as si quieres meterte con la barra de herramientas de la celda.\nInsert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.\nCell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. TambiÃ©n puede ir aquÃ­ para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es mÃ¡s intuitiva para eso.\nKernel: es para trabajar con el kernel que se ejecuta en segundo plano. AquÃ­ puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que estÃ¡ utilizando su computadora portÃ¡til.\nWidgets: es para guardar y borrar el estado del widget. Los widgets son bÃ¡sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido dinÃ¡mico utilizando Python (u otro Kernel).\nHelp: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia.\n\n\n\n\n\n\nJupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, tÃ­tulos de distintos tamaÃ±os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.\nLos lenguajes de markup son lenguajes ideados para procesar texto, algunos de los mÃ¡s conocidos son HTML y \\(\\LaTeX\\). Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.\nLa cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que mÃ¡s se utilizan.\n\nTexto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecerÃ¡ como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecerÃ¡ como palabra.\nListas: Las listas en Markdown se realizan indicando un asterisco o un nÃºmero seguido de un punto si se desean listas numeradas. Markdown organiza automÃ¡ticamente los items asignÃ¡ndoles el nÃºmero correcto.\nInclusiÃ³n de imÃ¡genes: La sintaxis para incluir imÃ¡genes en Markdown es ![nombre alternativo](direcciÃ³n de la imagen) en donde el nombre alternativo aparecerÃ¡ en caso de que no se pueda cargar la imÃ¡gen y la direcciÃ³n puede referirse a una imagen local o un enlace en Internet.\nInclusiÃ³n de cÃ³digo HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el cÃ³digo HTML.\nEnlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a pÃ¡ginas en internet u otros archivos locales. Su sintaxis es [texto](direcciÃ³n del enlace).\nFÃ³rmulas matemÃ¡ticas: Gracias al uso de MathJax, se puede incluir cÃ³digo en \\(\\LaTeX\\) para mostrar todo tipo de fÃ³rmulas y expresiones matemÃ¡ticas. Las fÃ³rmulas dentro de una lÃ­nea de texto se escriben entre sÃ­mbolos de dÃ³lar $...$, mientras que las expresiones separadas del texto utilizan sÃ­mbolos de dÃ³lar dobles $$...$$. Los siguientes son ejemplos de fÃ³rmulas matemÃ¡ticas escritas en \\(\\LaTeX\\):\n\n\\[p(x) = 3x^2 + 5y^2 + x^2y^2\\]\n\\[e^{\\pi i} - 1 = 0\\]\n\\[\\lim_{x \\rightarrow \\infty} 3x+1\\]\n\\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\]\n\\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\]\n\n\n\n\nJupyter Notebook permite que escribamos cÃ³digo dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.\nVeamos unos ejemplos sencillos de cÃ³digo:\n\nimport math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n\nLa raiz cuadra de 16 es 4.0\n\n\nTambiÃ©n es posible visualizar tablas de datos con la librerÃ­a pandas:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\n\nUnas de las cosas mÃ¡s significativas de Jupyter notebook es poder trabajar con distintos tipos de grÃ¡ficos (imagen estÃ¡tica o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.\n\n#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nLa completaciÃ³n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que estÃ© tratando.\nSimplemente escriba object_name.&lt;TAB&gt; para ver los atributos del objeto. AdemÃ¡s de los objetos y palabras clave de Python, la finalizaciÃ³n de pestaÃ±as tambiÃ©n funciona en nombres de archivos y directorios.\nimport collections\ncollections. # aprete la tecla &lt;ğ‘‡ğ´ğµ&gt;\n\n\n\nEn caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una funciÃ³n llamada help.\nEn resumen, Â¡suele ser mÃ¡s importante saber como buscar informaciÃ³n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaciÃ³n ? luego del nombre del comando (y luego ejecutar la celda con la combinaciÃ³n de teclas SHIFT + ENTER).\nimport numpy as np\nnp.sum?\n\n\n\nJupyter posee varias funciones mÃ¡gicas predefinidas que sirven para simplificar tareas comunes.\nHay dos tipos de magias:\n\nMagias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma lÃ­nea.\nMagias por celda (cell magics): son comandos que empiezan con los caracteres %%, y que reciben argumentos en la misma lÃ­nea y en toda la celda.\n\nEn general solo se puede usar una sola mÃ¡gias por celda en cada celda y debe ser escrita en la primer linea de la celda.\nUn buen ejemplo de mÃ¡gia es %lsmagic que lista todas las magias disponibles:\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\nEn varias situaciones resulta necesario medir el tiempo de ejecuciÃ³n de una porciÃ³n de cÃ³digo. Para ello podemos usar la magia %timeit. Esta magia estÃ¡ disponible tanto para lÃ­nea como para celda:\n\n%%timeit \n1+1 # timeit repite (adaptativamente) la mediciÃ³n a fin de reducir el error.\n\n8.68 ns Â± 0.387 ns per loop (mean Â± std. dev. of 7 runs, 100000000 loops each)\n\n\nJupyter notebook permite tambiÃ©n mezclar varios lenguajes de programaciÃ³n en una misma notebook. Por ejemplo, podrÃ­amos escribir en bash lo siguiente:\n\n%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n\n3\n2\n1\nHola desde /usr/bin/bash\n\n\nTambiÃ©n, puede acceder a la lÃ­nea de comandos, anteponiendo el sÃ­mbolo de !. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: !pip install pandas).\nVeamos un ejemplo:\n\n!pwd\n\n/home/fralfaro/PycharmProjects/ds_blog/_notebooks\n\n\n\n\n\n\n\nNotebook Basics\nRunning the Notebook"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#introducciÃ³n",
    "href": "posts/2021/2021-07-31-jupyter.html#introducciÃ³n",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar cÃ³digo en Python (por defecto, aunque permite otros lenguajes) de manera dinÃ¡mica, a la vez que integrar en un mismo documento tanto bloques de cÃ³digo como texto, grÃ¡ficas o imÃ¡genes. Es un SaaS utilizado ampliamente en anÃ¡lisis numÃ©rico, estadÃ­stica y machine learning, entre otros campos de la informÃ¡tica y las matemÃ¡ticas.\nPor otro lado, JupyterLab es similar a Jupyter Notebook en cuanto a sus funcionalidade, pero tiene un interfaz mÃ¡s interesante para los usuarios. Eventualmente Jupyter Lab reemplazarÃ¡ a Jupyter Notebok.\nNos centraremos en comprender aspectos bÃ¡sicos de cÃ³mo trabajar un archivo en jupyter notebook (extensiÃ³n .ipynb)."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#primeros-pasos",
    "href": "posts/2021/2021-07-31-jupyter.html#primeros-pasos",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Para instalar RISE, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge notebook\nDe lo contrario, puede instalar con pip:\npip install notebook\n\nNota: SI desea instalar JupyterLab, simplemente reemplaza notebook por jupyterlab."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#primeros-pasos-1",
    "href": "posts/2021/2021-07-31-jupyter.html#primeros-pasos-1",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Una vez que haya instalado Jupyter Notebook en su computadora, estarÃ¡ listo para ejecutar el servidor de la computadora portÃ¡til. Puede iniciar el servidor del portÃ¡til desde la lÃ­nea de comandos (usando Terminal en Mac/Linux, SÃ­mbolo del sistema en Windows) ejecutando:\njupyter notebook\n\nEsto imprimirÃ¡ cierta informaciÃ³n sobre el servidor en su terminal, incluida la URL de la aplicaciÃ³n web (de forma predeterminada, http://localhost:8888):\n$ jupyter notebook\n[I 08:58:24.417 NotebookApp] Serving notebooks from local directory: /Users/catherine\n[I 08:58:24.417 NotebookApp] 0 active kernels\n[I 08:58:24.417 NotebookApp] The Jupyter Notebook is running at: http://localhost:8888/\n[I 08:58:24.417 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\nA continuaciÃ³n, abrirÃ¡ su navegador web predeterminado a esta URL. Cuando el notebook se abra en su navegador, verÃ¡ el Panel, que mostrarÃ¡ una lista de notebooks, archivos y subdirectorios en el directorio donde se iniciÃ³ el servidor.\n\nLa parte superior de la lista de notebooks se muestran rutas de navegaciÃ³n en las que se puede hacer clic del directorio actual.\nPara crear un nuevo notebook, haga clic en el botÃ³n New en la parte superior de la lista y seleccione el kernel del menÃº desplegable (como se ve a continuaciÃ³n). Los kernels que se enumeran dependen de lo que estÃ© instalado en el servidor.\n\nNota: Es posible que algunos de los kernels de la siguiente captura de pantalla no existan como una opciÃ³n para usted.\n\n\nUna vez seleccionado el kernel, se abrira nuestro primer notebook!.\n\n\n\n\nJupyter notebook nos ofrece el siguiente toolbox:\n\n\nFile: En Ã©l, puede crear un nuevo cuaderno o abrir uno preexistente. AquÃ­ es tambiÃ©n a donde irÃ­a para cambiar el nombre de un Cuaderno. Creo que el elemento de menÃº mÃ¡s interesante es la opciÃ³n Guardar y Checkpoint. Esto le permite crear puntos de control a los que puede retroceder si lo necesita.\nEdit: AquÃ­ puede cortar, copiar y pegar celdas. AquÃ­ tambiÃ©n es donde irÃ­as si quisieras eliminar, dividir o fusionar una celda. Puede reordenar celdas aquÃ­ tambiÃ©n.\nView: es Ãºtil para alternar la visibilidad del encabezado y la barra de herramientas. TambiÃ©n puede activar o desactivar los nÃºmeros de lÃ­nea dentro de las celdas. AquÃ­ tambiÃ©n es donde irÃ­as si quieres meterte con la barra de herramientas de la celda.\nInsert: es solo para insertar celdas encima o debajo de la celda seleccionada actualmente.\nCell: le permite ejecutar una celda, un grupo de celdas o todas las celdas. TambiÃ©n puede ir aquÃ­ para cambiar el tipo de celda, aunque personalmente considero que la barra de herramientas es mÃ¡s intuitiva para eso.\nKernel: es para trabajar con el kernel que se ejecuta en segundo plano. AquÃ­ puede reiniciar el kernel, volver a conectarlo, apagarlo o incluso cambiar el kernel que estÃ¡ utilizando su computadora portÃ¡til.\nWidgets: es para guardar y borrar el estado del widget. Los widgets son bÃ¡sicamente widgets de JavaScript que puede agregar a sus celdas para crear contenido dinÃ¡mico utilizando Python (u otro Kernel).\nHelp: es donde debe aprender sobre los atajos de teclado del Notebook, un recorrido por la interfaz de usuario y mucho material de referencia."
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#markdown",
    "href": "posts/2021/2021-07-31-jupyter.html#markdown",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook permite que escribamos texto formateado, es decir, texto con cursiva, negritas, tÃ­tulos de distintos tamaÃ±os, etc., de forma simple. Para ello Jupyter nos permite usar Markdown, que es un lenguaje de marcado (markup) muy popular.\nLos lenguajes de markup son lenguajes ideados para procesar texto, algunos de los mÃ¡s conocidos son HTML y \\(\\LaTeX\\). Markdown tiene como objetivo ser un lenguaje de sintaxis minimalista, simple de aprender y usar; de esa forma uno puede dar formato al texto pero sin perder demasiado tiempo en los detalles.\nLa cantidad de tutoriales en la red sobre Markdown es inmenso, por lo que nos centraremos en indicar las opciones que mÃ¡s se utilizan.\n\nTexto en negrita/cursiva: El texto en negrita se indica entre dos pares de asteriscos. De este modo **palabra** aparecerÃ¡ como palabra. Por otro lado, el texto en cursiva se indica entre dos asteriscos simples; es decir *palabra* aparecerÃ¡ como palabra.\nListas: Las listas en Markdown se realizan indicando un asterisco o un nÃºmero seguido de un punto si se desean listas numeradas. Markdown organiza automÃ¡ticamente los items asignÃ¡ndoles el nÃºmero correcto.\nInclusiÃ³n de imÃ¡genes: La sintaxis para incluir imÃ¡genes en Markdown es ![nombre alternativo](direcciÃ³n de la imagen) en donde el nombre alternativo aparecerÃ¡ en caso de que no se pueda cargar la imÃ¡gen y la direcciÃ³n puede referirse a una imagen local o un enlace en Internet.\nInclusiÃ³n de cÃ³digo HTML: El lenguaje Markdown es un subconjunto del lenguaje HTML y en donde se necesite un mayor control del formato, se puede incluir directamente el cÃ³digo HTML.\nEnlaces: Las celdas de texto pueden contener enlaces, tanto a otras partes del documento, como a pÃ¡ginas en internet u otros archivos locales. Su sintaxis es [texto](direcciÃ³n del enlace).\nFÃ³rmulas matemÃ¡ticas: Gracias al uso de MathJax, se puede incluir cÃ³digo en \\(\\LaTeX\\) para mostrar todo tipo de fÃ³rmulas y expresiones matemÃ¡ticas. Las fÃ³rmulas dentro de una lÃ­nea de texto se escriben entre sÃ­mbolos de dÃ³lar $...$, mientras que las expresiones separadas del texto utilizan sÃ­mbolos de dÃ³lar dobles $$...$$. Los siguientes son ejemplos de fÃ³rmulas matemÃ¡ticas escritas en \\(\\LaTeX\\):\n\n\\[p(x) = 3x^2 + 5y^2 + x^2y^2\\]\n\\[e^{\\pi i} - 1 = 0\\]\n\\[\\lim_{x \\rightarrow \\infty} 3x+1\\]\n\\[\\sum_{n=1}^\\infty\\frac{1}{n^2}\\]\n\\[\\int_0^\\infty\\frac{\\sin x}{x}\\,\\mathrm{d}x=\\frac{\\pi}{2}\\]"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#cÃ³digo",
    "href": "posts/2021/2021-07-31-jupyter.html#cÃ³digo",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Jupyter Notebook permite que escribamos cÃ³digo dependiendo del kernel a trabajar. Por defecto, se trabaja con el kernel de Python.\nVeamos unos ejemplos sencillos de cÃ³digo:\n\nimport math\nn = 16\nprint(f\"La raiz cuadra de {n} es {math.sqrt(n)}\")\n\nLa raiz cuadra de 16 es 4.0\n\n\nTambiÃ©n es posible visualizar tablas de datos con la librerÃ­a pandas:\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n# datasets\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\n\ndf.head()\n\n\n\n\n\n\n\n\n\nTitle\nUS_Gross\nWorldwide_Gross\nUS_DVD_Sales\nProduction_Budget\nRelease_Date\nMPAA_Rating\nRunning_Time_min\nDistributor\nSource\nMajor_Genre\nCreative_Type\nDirector\nRotten_Tomatoes_Rating\nIMDB_Rating\nIMDB_Votes\n\n\n\n\n0\nThe Land Girls\n146083.0\n146083.0\nNaN\n8000000.0\nJun 12 1998\nR\nNaN\nGramercy\nNone\nNone\nNone\nNone\nNaN\n6.1\n1071.0\n\n\n1\nFirst Love, Last Rites\n10876.0\n10876.0\nNaN\n300000.0\nAug 07 1998\nR\nNaN\nStrand\nNone\nDrama\nNone\nNone\nNaN\n6.9\n207.0\n\n\n2\nI Married a Strange Person\n203134.0\n203134.0\nNaN\n250000.0\nAug 28 1998\nNone\nNaN\nLionsgate\nNone\nComedy\nNone\nNone\nNaN\n6.8\n865.0\n\n\n3\nLet's Talk About Sex\n373615.0\n373615.0\nNaN\n300000.0\nSep 11 1998\nNone\nNaN\nFine Line\nNone\nComedy\nNone\nNone\n13.0\nNaN\nNaN\n\n\n4\nSlam\n1009819.0\n1087521.0\nNaN\n1000000.0\nOct 09 1998\nR\nNaN\nTrimark\nOriginal Screenplay\nDrama\nContemporary Fiction\nNone\n62.0\n3.4\n165.0\n\n\n\n\n\n\n\n\nUnas de las cosas mÃ¡s significativas de Jupyter notebook es poder trabajar con distintos tipos de grÃ¡ficos (imagen estÃ¡tica o interactiva). Estos son de bastante utilidad para poder comprender nuestros procedimientos.\n\n#collapse-hide\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\nLa completaciÃ³n mediante tabs, especialmente para los atributos, es una forma conveniente de explorar la estructura de cualquier objeto con el que estÃ© tratando.\nSimplemente escriba object_name.&lt;TAB&gt; para ver los atributos del objeto. AdemÃ¡s de los objetos y palabras clave de Python, la finalizaciÃ³n de pestaÃ±as tambiÃ©n funciona en nombres de archivos y directorios.\nimport collections\ncollections. # aprete la tecla &lt;ğ‘‡ğ´ğµ&gt;\n\n\n\nEn caso de necesitar ayuda sobre cualquier comando de Python, Jupyter nos ofrece una funciÃ³n llamada help.\nEn resumen, Â¡suele ser mÃ¡s importante saber como buscar informaciÃ³n que memorizarla! Por todo esto, Jupyter nos ofrece ayuda sobre cualquier comando agregando un signo de interrogaciÃ³n ? luego del nombre del comando (y luego ejecutar la celda con la combinaciÃ³n de teclas SHIFT + ENTER).\nimport numpy as np\nnp.sum?\n\n\n\nJupyter posee varias funciones mÃ¡gicas predefinidas que sirven para simplificar tareas comunes.\nHay dos tipos de magias:\n\nMagias por linea (line magics): son comandos que empiezan con el caracter % y que toman como argumentos valores escritos en la misma lÃ­nea.\nMagias por celda (cell magics): son comandos que empiezan con los caracteres %%, y que reciben argumentos en la misma lÃ­nea y en toda la celda.\n\nEn general solo se puede usar una sola mÃ¡gias por celda en cada celda y debe ser escrita en la primer linea de la celda.\nUn buen ejemplo de mÃ¡gia es %lsmagic que lista todas las magias disponibles:\n\n%lsmagic\n\nAvailable line magics:\n%alias  %alias_magic  %autoawait  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %conda  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %pip  %popd  %pprint  %precision  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics.\n\n\nEn varias situaciones resulta necesario medir el tiempo de ejecuciÃ³n de una porciÃ³n de cÃ³digo. Para ello podemos usar la magia %timeit. Esta magia estÃ¡ disponible tanto para lÃ­nea como para celda:\n\n%%timeit \n1+1 # timeit repite (adaptativamente) la mediciÃ³n a fin de reducir el error.\n\n8.68 ns Â± 0.387 ns per loop (mean Â± std. dev. of 7 runs, 100000000 loops each)\n\n\nJupyter notebook permite tambiÃ©n mezclar varios lenguajes de programaciÃ³n en una misma notebook. Por ejemplo, podrÃ­amos escribir en bash lo siguiente:\n\n%%bash\nfor i in {3..1}; do\n    echo $i\ndone\necho \"Hola desde $BASH\"\n\n3\n2\n1\nHola desde /usr/bin/bash\n\n\nTambiÃ©n, puede acceder a la lÃ­nea de comandos, anteponiendo el sÃ­mbolo de !. Esto es de bastante utilidad cuando se quiere mostrar las dependencias que se necesitan instalar. (ejemplo: !pip install pandas).\nVeamos un ejemplo:\n\n!pwd\n\n/home/fralfaro/PycharmProjects/ds_blog/_notebooks"
  },
  {
    "objectID": "posts/2021/2021-07-31-jupyter.html#referencias",
    "href": "posts/2021/2021-07-31-jupyter.html#referencias",
    "title": "Jupyter Noteboook",
    "section": "",
    "text": "Notebook Basics\nRunning the Notebook"
  },
  {
    "objectID": "posts/2023/art_docs.html",
    "href": "posts/2023/art_docs.html",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "Esperamos que, si estÃ¡s leyendo este tutorial, ya comprendas la importancia de documentar tu cÃ³digo. Pero, por si acaso, permÃ­teme citar algo que Guido mencionÃ³ en la reciente PyCon 2016:\n\n\n\n\n\n\nCita\n\n\n\nâ€œEl cÃ³digo se lee mÃ¡s a menudo de lo que se escribe.â€ â€” Guido van Rossum\n\n\nCuando escribes cÃ³digo, lo haces para dos audiencias principales: tus usuarios y tus desarrolladores (incluyÃ©ndote a ti mismo). Ambas audiencias son igualmente cruciales. Si eres como yo, es posible que hayas abierto antiguas bases de cÃ³digo y te hayas preguntado: â€œÂ¿En quÃ© estaba pensando?â€. Si tienes dificultades para entender tu propio cÃ³digo, imagina lo que tus usuarios u otros desarrolladores sienten cuando intentan utilizarlo o contribuir a tu cÃ³digo.\nPor otro lado, es probable que hayas pasado por situaciones en las que deseabas realizar algo en Python y encontraste lo que parecÃ­a ser una excelente biblioteca que podrÃ­a hacer el trabajo. Sin embargo, al comenzar a usar la biblioteca, buscaste ejemplos, descripciones o incluso documentaciÃ³n oficial sobre cÃ³mo realizar una tarea especÃ­fica y no pudiste encontrar una soluciÃ³n de inmediato.\nDespuÃ©s de buscar durante un tiempo, te das cuenta de que la documentaciÃ³n es insuficiente o, peor aÃºn, estÃ¡ completamente ausente. Esta es una experiencia frustrante que te impide utilizar la biblioteca, sin importar cuÃ¡n bueno o eficiente sea el cÃ³digo. Daniele Procida resumiÃ³ esta situaciÃ³n de manera acertada:\n!!! quote â€œNo importa cuÃ¡n bueno sea tu software, porque si la documentaciÃ³n no es lo suficientemente buena, la gente no lo usarÃ¡.â€ â€” Daniele Procida\nEn esta guÃ­a, aprenderÃ¡s desde cero cÃ³mo documentar adecuadamente tu cÃ³digo en Python, desde los scripts mÃ¡s pequeÃ±os hasta los proyectos mÃ¡s grandes de Python, para evitar que tus usuarios se sientan frustrados al usar o contribuir a tu proyecto.\n\n\n\nAntes de sumergirnos en el arte de documentar tu cÃ³digo en Python, es crucial establecer una distinciÃ³n fundamental: los comentarios y la documentaciÃ³n desempeÃ±an roles distintos y estÃ¡n dirigidos a audiencias diferentes.\nComentarios:\nEn tÃ©rminos generales, los comentarios estÃ¡n diseÃ±ados para proporcionar informaciÃ³n sobre tu cÃ³digo a los desarrolladores.\nLa audiencia principal a la que se dirigen son aquellos que mantienen y trabajan en el cÃ³digo Python. Cuando se combinan con un cÃ³digo bien escrito, los comentarios actÃºan como guÃ­as que ayudan a los lectores a comprender mejor el cÃ³digo, su propÃ³sito y su estructura. Esto se alinea perfectamente con la sabia observaciÃ³n de Jeff Atwood,\n!!! quote â€œEl cÃ³digo te dice cÃ³mo; los comentarios te dicen por quÃ©.â€ â€” Jeff Atwood\nDocumentaciÃ³n del CÃ³digo:\nPor otro lado, la documentaciÃ³n del cÃ³digo se enfoca en describir el uso y la funcionalidad del cÃ³digo a los usuarios. Aunque puede ser Ãºtil durante el proceso de desarrollo, su audiencia principal son los usuarios finales del software. La siguiente secciÃ³n de este artÃ­culo se adentrarÃ¡ en cuÃ¡ndo y cÃ³mo debes abordar la tarea de comentar tu cÃ³digo en Python.\n\n\n\n\nEn Python, los comentarios son esenciales para proporcionar informaciÃ³n adicional sobre tu cÃ³digo.\nSe crean utilizando el sÃ­mbolo de nÃºmero (#) y deben ser declaraciones breves, no mÃ¡s largas que unas pocas frases. AquÃ­ tienes un ejemplo simple:\ndef hello_world():    \n    # Un comentario simple antes de una simple declaraciÃ³n de impresiÃ³n\n    print(\"Hola Mundo\")\nDe acuerdo con las pautas de estilo de cÃ³digo de Python (PEP 8), los comentarios deben tener una longitud mÃ¡xima de 72 caracteres. Esto es vÃ¡lido incluso si tu proyecto cambia la longitud mÃ¡xima de lÃ­nea recomendada para que sea mayor que los 80 caracteres. Si un comentario va a superar el lÃ­mite de caracteres recomendado, es apropiado usar mÃºltiples lÃ­neas para el comentario:\ndef hello_long_world():     \n    # Una declaraciÃ³n muy larga que sigue y sigue y sigue y sigue y sigue \n    # sin terminar hasta que alcance el lÃ­mite de 80 caracteres\n    print(\"Â¡Hola Mundoooooooooooooooooooooooooooooooooooooooooooooooooooooo!\")\nComentar tu cÃ³digo sirve para varios propÃ³sitos, incluyendo:\n\nPlanificaciÃ³n y RevisiÃ³n: Durante el desarrollo de nuevas partes de tu cÃ³digo, los comentarios pueden servir como una forma de planificar o esquematizar esa secciÃ³n. Es importante recordar eliminar estos comentarios una vez que se haya implementado y revisado/testeado el cÃ³digo real:\n# Primer paso\n# Segundo paso\n# Tercer paso\nDescripciÃ³n del CÃ³digo: Los comentarios se utilizan para explicar la intenciÃ³n de secciones especÃ­ficas del cÃ³digo:\n# Intentar una conexiÃ³n basada en configuraciones anteriores. Si no tiene Ã©xito,\n# solicitar al usuario nuevas configuraciones.\nDescripciÃ³n AlgorÃ­tmica: Al usar algoritmos, especialmente los complicados, es Ãºtil explicar cÃ³mo funcionan o cÃ³mo se implementan en tu cÃ³digo. TambiÃ©n es apropiado describir por quÃ© seleccionaste un algoritmo especÃ­fico en lugar de otro:\n# Usar el ordenamiento rÃ¡pido para obtener ganancias de rendimiento.\nEtiquetado: Puedes utilizar etiquetas para seÃ±alar secciones especÃ­ficas de cÃ³digo donde se encuentran problemas conocidos o Ã¡reas de mejora. Algunos ejemplos son BUG, FIXME y TODO:\n# TODO: Agregar condiciÃ³n para cuando 'val' sea None\n\nLos comentarios en tu cÃ³digo deben ser breves y centrados. Evita comentarios largos cuando sea posible. AdemÃ¡s, sigue las siguientes cuatro reglas esenciales sugeridas por Jeff Atwood:\n\nMantÃ©n los Comentarios Cerca del CÃ³digo: Los comentarios deben estar lo mÃ¡s cerca posible del cÃ³digo que describen. Los comentarios distantes del cÃ³digo descriptivo son frustrantes y pueden pasarse por alto fÃ¡cilmente al realizar actualizaciones.\nEvita el Formato Complejo: No uses formatos complejos como tablas o figuras ASCII. Estos formatos pueden distraer y ser difÃ­ciles de mantener con el tiempo.\nEvita InformaciÃ³n Redundante: SupÃ³n que el lector del cÃ³digo tiene un entendimiento bÃ¡sico de los principios de programaciÃ³n y la sintaxis del lenguaje. No incluyas informaciÃ³n redundante.\nDiseÃ±a Tu CÃ³digo para que se Comente por SÃ­ Mismo: La forma mÃ¡s fÃ¡cil de entender el cÃ³digo es leyÃ©ndolo. Cuando diseÃ±es tu cÃ³digo utilizando conceptos claros y fÃ¡ciles de entender, ayudarÃ¡s al lector a comprender tu intenciÃ³n de manera rÃ¡pida y sencilla.\n\nRecuerda que los comentarios estÃ¡n diseÃ±ados para los lectores, incluyÃ©ndote a ti mismo, para ayudarlos a comprender el propÃ³sito y diseÃ±o del software.\n\n\n\nEl Type Hinting es una caracterÃ­stica que te permite indicar explÃ­citamente los tipos de datos que esperas en las funciones y mÃ©todos. Aunque Python es un lenguaje de programaciÃ³n de tipado dinÃ¡mico, el Type Hinting no cambia esa naturaleza, pero proporciona informaciÃ³n adicional a los desarrolladores y a las herramientas de anÃ¡lisis estÃ¡tico sobre cÃ³mo deberÃ­a funcionar el cÃ³digo.\nEl Type Hinting no afecta el comportamiento en tiempo de ejecuciÃ³n, por lo que no impide que el cÃ³digo funcione si los tipos no coinciden.\nEn cambio, es una herramienta para ayudar a los desarrolladores a comprender y depurar el cÃ³digo de manera mÃ¡s eficiente y prevenir posibles errores.\nConsidera la siguiente funciÃ³n hello_name:\ndef hello_name(name: str) -&gt; str:\n    return f\"Hello {name}\"\nEn este ejemplo, hemos utilizado Type Hinting para especificar que el parÃ¡metro name debe ser una cadena (str) y que la funciÃ³n hello_name debe devolver una cadena (str). Esta informaciÃ³n es Ãºtil para otros desarrolladores que utilicen esta funciÃ³n porque ahora saben quÃ© tipo de dato esperar como entrada y quÃ© tipo de dato obtendrÃ¡n como resultado.\n\n\n\n\n\nUna parte fundamental de la documentaciÃ³n en Python son las docstrings, que son cadenas de texto utilizadas para describir funciones, clases, mÃ³dulos y mÃ¡s.\n\n\nLas docstrings son cadenas de documentaciÃ³n que se encuentran dentro del cÃ³digo fuente Python. Estas cadenas proporcionan informaciÃ³n sobre el propÃ³sito y el funcionamiento de funciones, clases y otros elementos del cÃ³digo.\nLas docstrings son especialmente valiosas para ayudar a los usuarios y desarrolladores a comprender cÃ³mo utilizar y trabajar con tu cÃ³digo.\nÂ¿CÃ³mo Funcionan las Docstrings?\nCuando definimos una funciÃ³n, clase o mÃ³dulo en Python, podemos incluir una docstring justo debajo de la definiciÃ³n. Por ejemplo:\ndef saludar(nombre):\n    \"\"\"Esta funciÃ³n imprime un saludo personalizado.\"\"\"\n    print(f\"Hola, {nombre}!\")\nLas docstrings se pueden acceder a travÃ©s del atributo __doc__ del objeto. Por ejemplo:\nprint(saludar.__doc__)\nLa salida serÃ­a: â€œEsta funciÃ³n imprime un saludo personalizado.â€ Las docstrings tambiÃ©n se utilizan en entornos de desarrollo interactivo y se muestran al utilizar la funciÃ³n help().\nManipulaciÃ³n de Docstrings\nEs importante destacar que puedes manipular directamente las docstrings. Sin embargo, existen restricciones para los objetos incorporados en Python. Por ejemplo, no puedes cambiar la docstring de un objeto str incorporado:\nstr.__doc__ = \"Â¡Esto no funcionarÃ¡ para objetos incorporados!\"\nPero para funciones y objetos personalizados, puedes establecer o modificar sus docstrings de la siguiente manera:\ndef decir_hola(nombre):\n    \"\"\"Una funciÃ³n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, Â¿soy yo a quien estÃ¡s buscando?\")\n\ndecir_hola.__doc__ = \"Una funciÃ³n que saluda estilo Richie Rich.\"\nUbicaciÃ³n EstratÃ©gica de las Docstrings\nUna forma mÃ¡s sencilla de definir docstrings es colocar una cadena literal justo debajo de la definiciÃ³n de la funciÃ³n o clase. Python automÃ¡ticamente interpreta esta cadena como la docstring. Por ejemplo:\ndef decir_hola(nombre):\n    \"\"\"Una funciÃ³n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, Â¿soy yo a quien estÃ¡s buscando?\")\n\n\n\nLas docstrings son elementos esenciales para documentar tu cÃ³digo Python de manera clara y coherente. Siguen convenciones y pautas que se describen en PEP 257.\nEl propÃ³sito de las docstrings es proporcionar a los usuarios de tu cÃ³digo un resumen conciso y Ãºtil del objeto, como una funciÃ³n, clase, mÃ³dulo o script. Deben ser lo suficientemente concisas como para ser fÃ¡ciles de mantener, pero lo suficientemente detalladas como para que los nuevos usuarios comprendan su propÃ³sito y cÃ³mo utilizar el objeto documentado.\n\n\nTodas las docstrings deben utilizar el formato de triple comilla doble (\"\"\") y deben colocarse justo debajo de la definiciÃ³n del objeto, ya sea en una sola lÃ­nea o en varias lÃ­neas:\nUna lÃ­nea:\n\"\"\"Esta es una lÃ­nea de resumen rÃ¡pida utilizada como descripciÃ³n del objeto.\"\"\"\nVarias lÃ­neas:\n\"\"\"\nEsta es la lÃ­nea de resumen\nEsta es la elaboraciÃ³n adicional de la docstring. Dentro de esta secciÃ³n, puedes proporcionar mÃ¡s detalles segÃºn sea apropiado para la situaciÃ³n. Observa que el resumen y la elaboraciÃ³n estÃ¡n separados por una nueva lÃ­nea en blanco.\n\"\"\"\nEs importante destacar que todas las docstrings de varias lÃ­neas deben seguir un patrÃ³n especÃ­fico:\n\nUna lÃ­nea de resumen de una sola lÃ­nea.\nUna lÃ­nea en blanco despuÃ©s del resumen.\nCualquier elaboraciÃ³n adicional de la docstring.\nOtra lÃ­nea en blanco.\n\nAdemÃ¡s, todas las docstrings deben tener una longitud mÃ¡xima de caracteres que sigue las mismas pautas que los comentarios, que es de 72 caracteres.\n\n\n\nLas docstrings de clase se crean para la clase en sÃ­, asÃ­ como para cualquier mÃ©todo de clase. Las docstrings se colocan inmediatamente despuÃ©s de la clase o el mÃ©todo de clase, con un nivel de sangrÃ­a:\nclass ClaseSimple:\n    \"\"\"AquÃ­ van las docstrings de clase.\"\"\"\n    def decir_hola(self, nombre: str):\n        \"\"\"AquÃ­ van las docstrings de mÃ©todo de clase.\"\"\"\n        print(f'Hola {nombre}')\nLas docstrings de clase deben contener la siguiente informaciÃ³n:\n\nUn breve resumen de su propÃ³sito y comportamiento.\nCualquier mÃ©todo pÃºblico, junto con una breve descripciÃ³n.\nCualquier propiedad de clase (atributos).\nCualquier cosa relacionada con la interfaz para los subclases, si la clase estÃ¡ destinada a ser subclaseada.\n\nLos parÃ¡metros del constructor de clase deben documentarse dentro de la docstring del mÃ©todo __init__ de la clase. Los mÃ©todos individuales deben documentarse utilizando sus propias docstrings individuales. Las docstrings de mÃ©todo de clase deben contener lo siguiente:\n\nUna breve descripciÃ³n de lo que hace el mÃ©todo y para quÃ© se utiliza.\nCualquier argumento (tanto requerido como opcional) que se pase, incluidos los argumentos de palabras clave.\nEtiqueta para cualquier argumento que se considere opcional o tenga un valor predeterminado.\nCualquier efecto secundario que ocurra al ejecutar el mÃ©todo.\nCualquier excepciÃ³n que se genere.\nCualquier restricciÃ³n sobre cuÃ¡ndo se puede llamar al mÃ©todo.\n\nEchemos un vistazo a un ejemplo simple de una clase de datos que representa un Animal. Esta clase contendrÃ¡ algunas propiedades de clase, propiedades de instancia, un __init__ y un Ãºnico mÃ©todo de instancia:\nclass Animal:\n    \"\"\"Una clase utilizada para representar un Animal\n    \n    Attributes:\n        dice_str (str): una cadena formateada para imprimir lo que dice el animal\n        nombre (str): el nombre del animal\n        sonido (str): el sonido que hace el animal\n        num_patas (int): el nÃºmero de patas del animal (predeterminado 4)\n    \"\"\"\n    \n    dice_str = \"Un {nombre} dice {sonido}\"\n    \n    def __init__(self, nombre, sonido, num_patas=4):\n        \"\"\"Inicializa una nueva instancia de Animal\n        \n        Parameters:\n            nombre (str): El nombre del animal\n            sonido (str): El sonido que hace el animal\n            num_patas (int, opcional): El nÃºmero de patas del animal (predeterminado es 4)\n        \"\"\"\n        self.nombre = nombre\n        self.sonido = sonido\n        self.num_patas = num_patas\n        \n    def dice(self, sonido=None):\n        \"\"\"Imprime el nombre del animal y el sonido que hace.\n        \n        Si no se pasa el argumento `sonido`, se utiliza el sonido predeterminado del Animal.\n        \n        Parameters:\n            sonido (str, opcional): El sonido que hace el animal (predeterminado es None)\n        \n        Raises:\n            NotImplementedError: Si no se establece ningÃºn sonido para el animal o se pasa como parÃ¡metro.\n        \"\"\"\n        if self.sonido is None and sonido is None:\n            raise NotImplementedError(\"Â¡No se admiten animales silenciosos!\")\n        sonido_salida = self.sonido if sonido is None else sonido\n        print(self.dice_str.format(nombre=self.nombre, sonido=sonido_salida))\n\n\n\n\n\nExisten formatos especÃ­ficos de docstrings que pueden ser utilizados para ayudar a los analizadores de docstrings y a los usuarios a tener un formato familiar y reconocido.\nAlgunos de los formatos mÃ¡s comunes son los siguientes:\n\n\n\nTipo de Formato\nDescripciÃ³n\nCompatible con Sphinx\nEspecificaciÃ³n Formal\n\n\n\n\nGoogle docstrings\nForma de documentaciÃ³n recomendada por Google\nSÃ­\nNo\n\n\nreStructuredText\nEstÃ¡ndar oficial de documentaciÃ³n de Python; no es amigable para principiantes pero rico en caracterÃ­sticas\nSÃ­\nSÃ­\n\n\nNumPy/SciPy docstrings\nCombinaciÃ³n de reStructuredText y Docstrings de Google utilizada por NumPy\nSÃ­\nSÃ­\n\n\nEpytext\nUna adaptaciÃ³n de Epydoc para Python; ideal para desarrolladores de Java\nNo oficialmente\nSÃ­\n\n\n\nLa elecciÃ³n del formato de docstring depende de ti, pero debes mantener el mismo formato en todo tu documento o proyecto. A continuaciÃ³n, se presentan ejemplos de cada tipo para darte una idea de cÃ³mo se ve cada formato de documentaciÃ³n.\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\nArgs:\n    file_loc (str): La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo.\n    print_cols (bool): Una bandera utilizada para imprimir las columnas en la consola\n        (el valor predeterminado es Falso)\n\nReturns:\n    list: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\n:param file_loc: La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\n:type file_loc: str\n:param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n:type print_cols: bool\n\n:returns: una lista de cadenas que representan las columnas de encabezado\n:rtype: list\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\nParameters\n----------\nfile_loc : str\n    La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\nprint_cols : bool, opcional\n    Una bandera utilizada para imprimir las columnas en la consola (el valor predeterminado es Falso)\n\nReturns\n-------\nlist\n    una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\n@type file_loc: str\n@param file_loc: La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\n@type print_cols: bool\n@param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n@rtype: list\n@returns: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\nEstos ejemplos te proporcionan una idea de cÃ³mo se estructuran y formatean las docstrings en diferentes estilos de documentaciÃ³n.\nPuedes elegir el que mejor se adapte a tus preferencias y necesidades de documentaciÃ³n, pero asegÃºrate de mantener la coherencia en todo tu proyecto.\n\n\n\n\n\nLos proyectos de Python vienen en todo tipo de formas, tamaÃ±os y propÃ³sitos. La forma en que documentas tu proyecto debe adaptarse a tu situaciÃ³n especÃ­fica. Ten en cuenta quiÃ©nes serÃ¡n los usuarios de tu proyecto y adÃ¡ptate a sus necesidades. Dependiendo del tipo de proyecto, se recomiendan ciertos aspectos de la documentaciÃ³n. La estructura general del proyecto y su documentaciÃ³n debe ser la siguiente:\nproject_root/\nâ”‚\nâ”œâ”€â”€ project/  # Project source code\nâ”œâ”€â”€ docs/\nâ”œâ”€â”€ README\nâ”œâ”€â”€ HOW_TO_CONTRIBUTE\nâ”œâ”€â”€ CODE_OF_CONDUCT\nâ”œâ”€â”€ examples.py\nEsta estructura de directorios es un diseÃ±o comÃºn para organizar un proyecto de software en Python. A continuaciÃ³n, se explica en detalle cada elemento de esta estructura:\n\nproject_root (Directorio RaÃ­z del Proyecto): Este es el directorio principal que contiene todos los archivos y carpetas relacionados con tu proyecto. Es el punto de partida para tu proyecto.\nproject/ (Carpeta â€œprojectâ€): Esta carpeta suele contener el cÃ³digo fuente principal de tu proyecto. AquÃ­ se almacenan todos los archivos de Python que forman parte de tu proyecto. Puedes organizar estos archivos en subdirectorios segÃºn la estructura de tu proyecto. Por ejemplo, puedes tener subdirectorios para mÃ³dulos especÃ­ficos o componentes del proyecto.\ndocs/ (Carpeta â€œdocsâ€): La carpeta â€œdocsâ€ se utiliza para almacenar la documentaciÃ³n de tu proyecto. AquÃ­ puedes incluir documentos explicativos, manuales de usuario, instrucciones de instalaciÃ³n y cualquier otra documentaciÃ³n relevante. Mantener una documentaciÃ³n clara y organizada es esencial para que los usuarios comprendan y utilicen tu proyecto de manera efectiva.\nREADME: El archivo â€œREADMEâ€ es un documento importante que proporciona una breve descripciÃ³n de tu proyecto y su propÃ³sito. Suele incluir informaciÃ³n sobre cÃ³mo instalar y utilizar el proyecto, asÃ­ como otros detalles importantes. Los usuarios suelen consultar este archivo primero cuando exploran un proyecto.\nHOW_TO_CONTRIBUTE: Este archivo contiene instrucciones para las personas que deseen contribuir al desarrollo de tu proyecto. Incluye detalles sobre cÃ³mo pueden colaborar, enviar correcciones, agregar nuevas funciones y seguir las pautas de contribuciÃ³n.\nCODE_OF_CONDUCT: El archivo â€œCODE_OF_CONDUCTâ€ establece las reglas y pautas de comportamiento que deben seguir los colaboradores y usuarios del proyecto. Define cÃ³mo deben interactuar entre sÃ­ de manera respetuosa y profesional. TambiÃ©n puede indicar las consecuencias en caso de violaciÃ³n del cÃ³digo de conducta.\nexamples.py: Este archivo es un script de Python que contiene ejemplos simples de cÃ³mo utilizar las funcionalidades de tu proyecto. Estos ejemplos pueden ayudar a los usuarios a comprender cÃ³mo utilizar tu cÃ³digo en situaciones reales y proporcionar ejemplos de uso prÃ¡ctico.\n\n\n\n\nLa documentaciÃ³n es una parte fundamental de cualquier proyecto de desarrollo de software. Proporciona informaciÃ³n crucial sobre cÃ³mo utilizar, mantener y contribuir al cÃ³digo. En el ecosistema de Python, existen varias bibliotecas y herramientas que facilitan la tarea de documentar el cÃ³digo de manera efectiva. En este artÃ­culo, exploraremos algunas de las principales bibliotecas de Python utilizadas para documentar cÃ³digo.\n\n\n\nSphinx es una de las herramientas de documentaciÃ³n mÃ¡s populares en el mundo de Python. Fue originalmente desarrollada para documentar la propia documentaciÃ³n de Python y se ha convertido en una elecciÃ³n comÃºn para proyectos de cÃ³digo abierto y proyectos internos. Algunas de sus caracterÃ­sticas clave incluyen:\n\nGeneraciÃ³n de documentaciÃ³n en varios formatos, incluyendo HTML, PDF, ePub y mÃ¡s.\nUtiliza reStructuredText como su formato de marcado predeterminado, que es altamente estructurado y permite documentar de manera eficiente los aspectos tÃ©cnicos.\nAmplia gama de extensiones y complementos que permiten personalizar y mejorar la documentaciÃ³n.\nAdmite la generaciÃ³n automÃ¡tica de documentaciÃ³n a partir de docstrings en el cÃ³digo Python.\nEs especialmente adecuado para documentar bibliotecas, API y proyectos tÃ©cnicos.\n\nSphinx es altamente configurable y puede generar documentaciÃ³n de alta calidad y profesional. Sin embargo, puede requerir un tiempo de configuraciÃ³n inicial y tiene una curva de aprendizaje empinada para los principiantes.\n\n\n\n\nMkDocs es una herramienta de generaciÃ³n de documentaciÃ³n que se centra en la simplicidad y la facilidad de uso. EstÃ¡ diseÃ±ada para crear documentaciÃ³n de proyectos de una manera simple y rÃ¡pida, principalmente enfocada en la generaciÃ³n de sitios web de documentaciÃ³n. Algunas de sus caracterÃ­sticas clave incluyen:\n\nUtiliza Markdown como formato de marcado predeterminado, que es fÃ¡cil de aprender y escribir.\nOfrece una interfaz de lÃ­nea de comandos simple para iniciar y generar sitios de documentaciÃ³n.\nProporciona temas y extensiones para personalizar el aspecto y la funcionalidad de la documentaciÃ³n generada.\nIdeal para proyectos de cÃ³digo abierto y documentaciÃ³n de proyectos pequeÃ±os a medianos.\n\nMkDocs es especialmente adecuado para proyectos con necesidades de documentaciÃ³n simples. Es fÃ¡cil de aprender y usar, lo que lo convierte en una excelente opciÃ³n para principiantes. Sin embargo, puede ser limitado en funcionalidad en comparaciÃ³n con Sphinx para proyectos tÃ©cnicos y complejos.\n\n\n\nMkDocs-Material es un tema personalizado para MkDocs, una popular herramienta de generaciÃ³n de sitios web estÃ¡ticos diseÃ±ada para crear documentaciÃ³n de proyectos de manera sencilla y efectiva. Este tema, conocido como â€œMaterial for MkDocsâ€, se inspira en el elegante diseÃ±o de Material Design de Google y estÃ¡ diseÃ±ado para ofrecer una experiencia de documentaciÃ³n moderna y atractiva.\nUna de las principales caracterÃ­sticas de MkDocs-Material es su enfoque en la legibilidad y la facilidad de navegaciÃ³n. Esto se logra mediante un diseÃ±o limpio y organizado que hace que la documentaciÃ³n sea mÃ¡s accesible para los usuarios. AdemÃ¡s, el tema proporciona herramientas Ãºtiles para mejorar la experiencia de los lectores, como una funciÃ³n de bÃºsqueda integrada que permite a los usuarios encontrar rÃ¡pidamente la informaciÃ³n que necesitan.\nOtra ventaja de MkDocs-Material es su navegaciÃ³n intuitiva, que facilita a los usuarios la exploraciÃ³n de la documentaciÃ³n y la navegaciÃ³n entre secciones y pÃ¡ginas. Esto es fundamental para garantizar que los usuarios puedan acceder fÃ¡cilmente a la informaciÃ³n que estÃ¡n buscando sin esfuerzo.\n\n\n\n\n\nLa documentaciÃ³n de proyectos sigue una progresiÃ³n simple:\n\nSin DocumentaciÃ³n\nAlguna DocumentaciÃ³n\nDocumentaciÃ³n Completa\nBuena DocumentaciÃ³n\nExcelente DocumentaciÃ³n\n\nSi te sientes perdido acerca de por dÃ³nde continuar con tu documentaciÃ³n, observa en quÃ© punto se encuentra tu proyecto en relaciÃ³n con la progresiÃ³n mencionada anteriormente. Â¿Tienes alguna documentaciÃ³n? Si no la tienes, comienza por ahÃ­. Si ya tienes algo de documentaciÃ³n pero te faltan algunos de los archivos clave del proyecto, comienza agregÃ¡ndolos.\nAl final, no te desanimes ni te sientas abrumado por la cantidad de trabajo necesario para documentar el cÃ³digo.\nUna vez que comiences a documentar tu cÃ³digo, te resultarÃ¡ mÃ¡s fÃ¡cil seguir adelante."
  },
  {
    "objectID": "posts/2023/art_docs.html#comentarios-vs-documentaciÃ³n",
    "href": "posts/2023/art_docs.html#comentarios-vs-documentaciÃ³n",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "Antes de sumergirnos en el arte de documentar tu cÃ³digo en Python, es crucial establecer una distinciÃ³n fundamental: los comentarios y la documentaciÃ³n desempeÃ±an roles distintos y estÃ¡n dirigidos a audiencias diferentes.\nComentarios:\nEn tÃ©rminos generales, los comentarios estÃ¡n diseÃ±ados para proporcionar informaciÃ³n sobre tu cÃ³digo a los desarrolladores.\nLa audiencia principal a la que se dirigen son aquellos que mantienen y trabajan en el cÃ³digo Python. Cuando se combinan con un cÃ³digo bien escrito, los comentarios actÃºan como guÃ­as que ayudan a los lectores a comprender mejor el cÃ³digo, su propÃ³sito y su estructura. Esto se alinea perfectamente con la sabia observaciÃ³n de Jeff Atwood,\n!!! quote â€œEl cÃ³digo te dice cÃ³mo; los comentarios te dicen por quÃ©.â€ â€” Jeff Atwood\nDocumentaciÃ³n del CÃ³digo:\nPor otro lado, la documentaciÃ³n del cÃ³digo se enfoca en describir el uso y la funcionalidad del cÃ³digo a los usuarios. Aunque puede ser Ãºtil durante el proceso de desarrollo, su audiencia principal son los usuarios finales del software. La siguiente secciÃ³n de este artÃ­culo se adentrarÃ¡ en cuÃ¡ndo y cÃ³mo debes abordar la tarea de comentar tu cÃ³digo en Python."
  },
  {
    "objectID": "posts/2023/art_docs.html#comentarios",
    "href": "posts/2023/art_docs.html#comentarios",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "En Python, los comentarios son esenciales para proporcionar informaciÃ³n adicional sobre tu cÃ³digo.\nSe crean utilizando el sÃ­mbolo de nÃºmero (#) y deben ser declaraciones breves, no mÃ¡s largas que unas pocas frases. AquÃ­ tienes un ejemplo simple:\ndef hello_world():    \n    # Un comentario simple antes de una simple declaraciÃ³n de impresiÃ³n\n    print(\"Hola Mundo\")\nDe acuerdo con las pautas de estilo de cÃ³digo de Python (PEP 8), los comentarios deben tener una longitud mÃ¡xima de 72 caracteres. Esto es vÃ¡lido incluso si tu proyecto cambia la longitud mÃ¡xima de lÃ­nea recomendada para que sea mayor que los 80 caracteres. Si un comentario va a superar el lÃ­mite de caracteres recomendado, es apropiado usar mÃºltiples lÃ­neas para el comentario:\ndef hello_long_world():     \n    # Una declaraciÃ³n muy larga que sigue y sigue y sigue y sigue y sigue \n    # sin terminar hasta que alcance el lÃ­mite de 80 caracteres\n    print(\"Â¡Hola Mundoooooooooooooooooooooooooooooooooooooooooooooooooooooo!\")\nComentar tu cÃ³digo sirve para varios propÃ³sitos, incluyendo:\n\nPlanificaciÃ³n y RevisiÃ³n: Durante el desarrollo de nuevas partes de tu cÃ³digo, los comentarios pueden servir como una forma de planificar o esquematizar esa secciÃ³n. Es importante recordar eliminar estos comentarios una vez que se haya implementado y revisado/testeado el cÃ³digo real:\n# Primer paso\n# Segundo paso\n# Tercer paso\nDescripciÃ³n del CÃ³digo: Los comentarios se utilizan para explicar la intenciÃ³n de secciones especÃ­ficas del cÃ³digo:\n# Intentar una conexiÃ³n basada en configuraciones anteriores. Si no tiene Ã©xito,\n# solicitar al usuario nuevas configuraciones.\nDescripciÃ³n AlgorÃ­tmica: Al usar algoritmos, especialmente los complicados, es Ãºtil explicar cÃ³mo funcionan o cÃ³mo se implementan en tu cÃ³digo. TambiÃ©n es apropiado describir por quÃ© seleccionaste un algoritmo especÃ­fico en lugar de otro:\n# Usar el ordenamiento rÃ¡pido para obtener ganancias de rendimiento.\nEtiquetado: Puedes utilizar etiquetas para seÃ±alar secciones especÃ­ficas de cÃ³digo donde se encuentran problemas conocidos o Ã¡reas de mejora. Algunos ejemplos son BUG, FIXME y TODO:\n# TODO: Agregar condiciÃ³n para cuando 'val' sea None\n\nLos comentarios en tu cÃ³digo deben ser breves y centrados. Evita comentarios largos cuando sea posible. AdemÃ¡s, sigue las siguientes cuatro reglas esenciales sugeridas por Jeff Atwood:\n\nMantÃ©n los Comentarios Cerca del CÃ³digo: Los comentarios deben estar lo mÃ¡s cerca posible del cÃ³digo que describen. Los comentarios distantes del cÃ³digo descriptivo son frustrantes y pueden pasarse por alto fÃ¡cilmente al realizar actualizaciones.\nEvita el Formato Complejo: No uses formatos complejos como tablas o figuras ASCII. Estos formatos pueden distraer y ser difÃ­ciles de mantener con el tiempo.\nEvita InformaciÃ³n Redundante: SupÃ³n que el lector del cÃ³digo tiene un entendimiento bÃ¡sico de los principios de programaciÃ³n y la sintaxis del lenguaje. No incluyas informaciÃ³n redundante.\nDiseÃ±a Tu CÃ³digo para que se Comente por SÃ­ Mismo: La forma mÃ¡s fÃ¡cil de entender el cÃ³digo es leyÃ©ndolo. Cuando diseÃ±es tu cÃ³digo utilizando conceptos claros y fÃ¡ciles de entender, ayudarÃ¡s al lector a comprender tu intenciÃ³n de manera rÃ¡pida y sencilla.\n\nRecuerda que los comentarios estÃ¡n diseÃ±ados para los lectores, incluyÃ©ndote a ti mismo, para ayudarlos a comprender el propÃ³sito y diseÃ±o del software.\n\n\n\nEl Type Hinting es una caracterÃ­stica que te permite indicar explÃ­citamente los tipos de datos que esperas en las funciones y mÃ©todos. Aunque Python es un lenguaje de programaciÃ³n de tipado dinÃ¡mico, el Type Hinting no cambia esa naturaleza, pero proporciona informaciÃ³n adicional a los desarrolladores y a las herramientas de anÃ¡lisis estÃ¡tico sobre cÃ³mo deberÃ­a funcionar el cÃ³digo.\nEl Type Hinting no afecta el comportamiento en tiempo de ejecuciÃ³n, por lo que no impide que el cÃ³digo funcione si los tipos no coinciden.\nEn cambio, es una herramienta para ayudar a los desarrolladores a comprender y depurar el cÃ³digo de manera mÃ¡s eficiente y prevenir posibles errores.\nConsidera la siguiente funciÃ³n hello_name:\ndef hello_name(name: str) -&gt; str:\n    return f\"Hello {name}\"\nEn este ejemplo, hemos utilizado Type Hinting para especificar que el parÃ¡metro name debe ser una cadena (str) y que la funciÃ³n hello_name debe devolver una cadena (str). Esta informaciÃ³n es Ãºtil para otros desarrolladores que utilicen esta funciÃ³n porque ahora saben quÃ© tipo de dato esperar como entrada y quÃ© tipo de dato obtendrÃ¡n como resultado."
  },
  {
    "objectID": "posts/2023/art_docs.html#docstrings",
    "href": "posts/2023/art_docs.html#docstrings",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "Una parte fundamental de la documentaciÃ³n en Python son las docstrings, que son cadenas de texto utilizadas para describir funciones, clases, mÃ³dulos y mÃ¡s.\n\n\nLas docstrings son cadenas de documentaciÃ³n que se encuentran dentro del cÃ³digo fuente Python. Estas cadenas proporcionan informaciÃ³n sobre el propÃ³sito y el funcionamiento de funciones, clases y otros elementos del cÃ³digo.\nLas docstrings son especialmente valiosas para ayudar a los usuarios y desarrolladores a comprender cÃ³mo utilizar y trabajar con tu cÃ³digo.\nÂ¿CÃ³mo Funcionan las Docstrings?\nCuando definimos una funciÃ³n, clase o mÃ³dulo en Python, podemos incluir una docstring justo debajo de la definiciÃ³n. Por ejemplo:\ndef saludar(nombre):\n    \"\"\"Esta funciÃ³n imprime un saludo personalizado.\"\"\"\n    print(f\"Hola, {nombre}!\")\nLas docstrings se pueden acceder a travÃ©s del atributo __doc__ del objeto. Por ejemplo:\nprint(saludar.__doc__)\nLa salida serÃ­a: â€œEsta funciÃ³n imprime un saludo personalizado.â€ Las docstrings tambiÃ©n se utilizan en entornos de desarrollo interactivo y se muestran al utilizar la funciÃ³n help().\nManipulaciÃ³n de Docstrings\nEs importante destacar que puedes manipular directamente las docstrings. Sin embargo, existen restricciones para los objetos incorporados en Python. Por ejemplo, no puedes cambiar la docstring de un objeto str incorporado:\nstr.__doc__ = \"Â¡Esto no funcionarÃ¡ para objetos incorporados!\"\nPero para funciones y objetos personalizados, puedes establecer o modificar sus docstrings de la siguiente manera:\ndef decir_hola(nombre):\n    \"\"\"Una funciÃ³n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, Â¿soy yo a quien estÃ¡s buscando?\")\n\ndecir_hola.__doc__ = \"Una funciÃ³n que saluda estilo Richie Rich.\"\nUbicaciÃ³n EstratÃ©gica de las Docstrings\nUna forma mÃ¡s sencilla de definir docstrings es colocar una cadena literal justo debajo de la definiciÃ³n de la funciÃ³n o clase. Python automÃ¡ticamente interpreta esta cadena como la docstring. Por ejemplo:\ndef decir_hola(nombre):\n    \"\"\"Una funciÃ³n simple que saluda a la manera de Richie.\"\"\"\n    print(f\"Hola {nombre}, Â¿soy yo a quien estÃ¡s buscando?\")\n\n\n\nLas docstrings son elementos esenciales para documentar tu cÃ³digo Python de manera clara y coherente. Siguen convenciones y pautas que se describen en PEP 257.\nEl propÃ³sito de las docstrings es proporcionar a los usuarios de tu cÃ³digo un resumen conciso y Ãºtil del objeto, como una funciÃ³n, clase, mÃ³dulo o script. Deben ser lo suficientemente concisas como para ser fÃ¡ciles de mantener, pero lo suficientemente detalladas como para que los nuevos usuarios comprendan su propÃ³sito y cÃ³mo utilizar el objeto documentado.\n\n\nTodas las docstrings deben utilizar el formato de triple comilla doble (\"\"\") y deben colocarse justo debajo de la definiciÃ³n del objeto, ya sea en una sola lÃ­nea o en varias lÃ­neas:\nUna lÃ­nea:\n\"\"\"Esta es una lÃ­nea de resumen rÃ¡pida utilizada como descripciÃ³n del objeto.\"\"\"\nVarias lÃ­neas:\n\"\"\"\nEsta es la lÃ­nea de resumen\nEsta es la elaboraciÃ³n adicional de la docstring. Dentro de esta secciÃ³n, puedes proporcionar mÃ¡s detalles segÃºn sea apropiado para la situaciÃ³n. Observa que el resumen y la elaboraciÃ³n estÃ¡n separados por una nueva lÃ­nea en blanco.\n\"\"\"\nEs importante destacar que todas las docstrings de varias lÃ­neas deben seguir un patrÃ³n especÃ­fico:\n\nUna lÃ­nea de resumen de una sola lÃ­nea.\nUna lÃ­nea en blanco despuÃ©s del resumen.\nCualquier elaboraciÃ³n adicional de la docstring.\nOtra lÃ­nea en blanco.\n\nAdemÃ¡s, todas las docstrings deben tener una longitud mÃ¡xima de caracteres que sigue las mismas pautas que los comentarios, que es de 72 caracteres.\n\n\n\nLas docstrings de clase se crean para la clase en sÃ­, asÃ­ como para cualquier mÃ©todo de clase. Las docstrings se colocan inmediatamente despuÃ©s de la clase o el mÃ©todo de clase, con un nivel de sangrÃ­a:\nclass ClaseSimple:\n    \"\"\"AquÃ­ van las docstrings de clase.\"\"\"\n    def decir_hola(self, nombre: str):\n        \"\"\"AquÃ­ van las docstrings de mÃ©todo de clase.\"\"\"\n        print(f'Hola {nombre}')\nLas docstrings de clase deben contener la siguiente informaciÃ³n:\n\nUn breve resumen de su propÃ³sito y comportamiento.\nCualquier mÃ©todo pÃºblico, junto con una breve descripciÃ³n.\nCualquier propiedad de clase (atributos).\nCualquier cosa relacionada con la interfaz para los subclases, si la clase estÃ¡ destinada a ser subclaseada.\n\nLos parÃ¡metros del constructor de clase deben documentarse dentro de la docstring del mÃ©todo __init__ de la clase. Los mÃ©todos individuales deben documentarse utilizando sus propias docstrings individuales. Las docstrings de mÃ©todo de clase deben contener lo siguiente:\n\nUna breve descripciÃ³n de lo que hace el mÃ©todo y para quÃ© se utiliza.\nCualquier argumento (tanto requerido como opcional) que se pase, incluidos los argumentos de palabras clave.\nEtiqueta para cualquier argumento que se considere opcional o tenga un valor predeterminado.\nCualquier efecto secundario que ocurra al ejecutar el mÃ©todo.\nCualquier excepciÃ³n que se genere.\nCualquier restricciÃ³n sobre cuÃ¡ndo se puede llamar al mÃ©todo.\n\nEchemos un vistazo a un ejemplo simple de una clase de datos que representa un Animal. Esta clase contendrÃ¡ algunas propiedades de clase, propiedades de instancia, un __init__ y un Ãºnico mÃ©todo de instancia:\nclass Animal:\n    \"\"\"Una clase utilizada para representar un Animal\n    \n    Attributes:\n        dice_str (str): una cadena formateada para imprimir lo que dice el animal\n        nombre (str): el nombre del animal\n        sonido (str): el sonido que hace el animal\n        num_patas (int): el nÃºmero de patas del animal (predeterminado 4)\n    \"\"\"\n    \n    dice_str = \"Un {nombre} dice {sonido}\"\n    \n    def __init__(self, nombre, sonido, num_patas=4):\n        \"\"\"Inicializa una nueva instancia de Animal\n        \n        Parameters:\n            nombre (str): El nombre del animal\n            sonido (str): El sonido que hace el animal\n            num_patas (int, opcional): El nÃºmero de patas del animal (predeterminado es 4)\n        \"\"\"\n        self.nombre = nombre\n        self.sonido = sonido\n        self.num_patas = num_patas\n        \n    def dice(self, sonido=None):\n        \"\"\"Imprime el nombre del animal y el sonido que hace.\n        \n        Si no se pasa el argumento `sonido`, se utiliza el sonido predeterminado del Animal.\n        \n        Parameters:\n            sonido (str, opcional): El sonido que hace el animal (predeterminado es None)\n        \n        Raises:\n            NotImplementedError: Si no se establece ningÃºn sonido para el animal o se pasa como parÃ¡metro.\n        \"\"\"\n        if self.sonido is None and sonido is None:\n            raise NotImplementedError(\"Â¡No se admiten animales silenciosos!\")\n        sonido_salida = self.sonido if sonido is None else sonido\n        print(self.dice_str.format(nombre=self.nombre, sonido=sonido_salida))\n\n\n\n\n\nExisten formatos especÃ­ficos de docstrings que pueden ser utilizados para ayudar a los analizadores de docstrings y a los usuarios a tener un formato familiar y reconocido.\nAlgunos de los formatos mÃ¡s comunes son los siguientes:\n\n\n\nTipo de Formato\nDescripciÃ³n\nCompatible con Sphinx\nEspecificaciÃ³n Formal\n\n\n\n\nGoogle docstrings\nForma de documentaciÃ³n recomendada por Google\nSÃ­\nNo\n\n\nreStructuredText\nEstÃ¡ndar oficial de documentaciÃ³n de Python; no es amigable para principiantes pero rico en caracterÃ­sticas\nSÃ­\nSÃ­\n\n\nNumPy/SciPy docstrings\nCombinaciÃ³n de reStructuredText y Docstrings de Google utilizada por NumPy\nSÃ­\nSÃ­\n\n\nEpytext\nUna adaptaciÃ³n de Epydoc para Python; ideal para desarrolladores de Java\nNo oficialmente\nSÃ­\n\n\n\nLa elecciÃ³n del formato de docstring depende de ti, pero debes mantener el mismo formato en todo tu documento o proyecto. A continuaciÃ³n, se presentan ejemplos de cada tipo para darte una idea de cÃ³mo se ve cada formato de documentaciÃ³n.\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\nArgs:\n    file_loc (str): La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo.\n    print_cols (bool): Una bandera utilizada para imprimir las columnas en la consola\n        (el valor predeterminado es Falso)\n\nReturns:\n    list: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\n:param file_loc: La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\n:type file_loc: str\n:param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n:type print_cols: bool\n\n:returns: una lista de cadenas que representan las columnas de encabezado\n:rtype: list\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\nParameters\n----------\nfile_loc : str\n    La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\nprint_cols : bool, opcional\n    Una bandera utilizada para imprimir las columnas en la consola (el valor predeterminado es Falso)\n\nReturns\n-------\nlist\n    una lista de cadenas que representan las columnas de encabezado\n\"\"\"\n\n\n\n\"\"\"Obtiene e imprime las columnas de encabezado de la hoja de cÃ¡lculo\n\n@type file_loc: str\n@param file_loc: La ubicaciÃ³n del archivo de la hoja de cÃ¡lculo\n@type print_cols: bool\n@param print_cols: Una bandera utilizada para imprimir las columnas en la consola\n    (el valor predeterminado es Falso)\n@rtype: list\n@returns: una lista de cadenas que representan las columnas de encabezado\n\"\"\"\nEstos ejemplos te proporcionan una idea de cÃ³mo se estructuran y formatean las docstrings en diferentes estilos de documentaciÃ³n.\nPuedes elegir el que mejor se adapte a tus preferencias y necesidades de documentaciÃ³n, pero asegÃºrate de mantener la coherencia en todo tu proyecto."
  },
  {
    "objectID": "posts/2023/art_docs.html#documentar-tus-proyectos-de-python",
    "href": "posts/2023/art_docs.html#documentar-tus-proyectos-de-python",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "Los proyectos de Python vienen en todo tipo de formas, tamaÃ±os y propÃ³sitos. La forma en que documentas tu proyecto debe adaptarse a tu situaciÃ³n especÃ­fica. Ten en cuenta quiÃ©nes serÃ¡n los usuarios de tu proyecto y adÃ¡ptate a sus necesidades. Dependiendo del tipo de proyecto, se recomiendan ciertos aspectos de la documentaciÃ³n. La estructura general del proyecto y su documentaciÃ³n debe ser la siguiente:\nproject_root/\nâ”‚\nâ”œâ”€â”€ project/  # Project source code\nâ”œâ”€â”€ docs/\nâ”œâ”€â”€ README\nâ”œâ”€â”€ HOW_TO_CONTRIBUTE\nâ”œâ”€â”€ CODE_OF_CONDUCT\nâ”œâ”€â”€ examples.py\nEsta estructura de directorios es un diseÃ±o comÃºn para organizar un proyecto de software en Python. A continuaciÃ³n, se explica en detalle cada elemento de esta estructura:\n\nproject_root (Directorio RaÃ­z del Proyecto): Este es el directorio principal que contiene todos los archivos y carpetas relacionados con tu proyecto. Es el punto de partida para tu proyecto.\nproject/ (Carpeta â€œprojectâ€): Esta carpeta suele contener el cÃ³digo fuente principal de tu proyecto. AquÃ­ se almacenan todos los archivos de Python que forman parte de tu proyecto. Puedes organizar estos archivos en subdirectorios segÃºn la estructura de tu proyecto. Por ejemplo, puedes tener subdirectorios para mÃ³dulos especÃ­ficos o componentes del proyecto.\ndocs/ (Carpeta â€œdocsâ€): La carpeta â€œdocsâ€ se utiliza para almacenar la documentaciÃ³n de tu proyecto. AquÃ­ puedes incluir documentos explicativos, manuales de usuario, instrucciones de instalaciÃ³n y cualquier otra documentaciÃ³n relevante. Mantener una documentaciÃ³n clara y organizada es esencial para que los usuarios comprendan y utilicen tu proyecto de manera efectiva.\nREADME: El archivo â€œREADMEâ€ es un documento importante que proporciona una breve descripciÃ³n de tu proyecto y su propÃ³sito. Suele incluir informaciÃ³n sobre cÃ³mo instalar y utilizar el proyecto, asÃ­ como otros detalles importantes. Los usuarios suelen consultar este archivo primero cuando exploran un proyecto.\nHOW_TO_CONTRIBUTE: Este archivo contiene instrucciones para las personas que deseen contribuir al desarrollo de tu proyecto. Incluye detalles sobre cÃ³mo pueden colaborar, enviar correcciones, agregar nuevas funciones y seguir las pautas de contribuciÃ³n.\nCODE_OF_CONDUCT: El archivo â€œCODE_OF_CONDUCTâ€ establece las reglas y pautas de comportamiento que deben seguir los colaboradores y usuarios del proyecto. Define cÃ³mo deben interactuar entre sÃ­ de manera respetuosa y profesional. TambiÃ©n puede indicar las consecuencias en caso de violaciÃ³n del cÃ³digo de conducta.\nexamples.py: Este archivo es un script de Python que contiene ejemplos simples de cÃ³mo utilizar las funcionalidades de tu proyecto. Estos ejemplos pueden ayudar a los usuarios a comprender cÃ³mo utilizar tu cÃ³digo en situaciones reales y proporcionar ejemplos de uso prÃ¡ctico."
  },
  {
    "objectID": "posts/2023/art_docs.html#principales-librerÃ­as",
    "href": "posts/2023/art_docs.html#principales-librerÃ­as",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "La documentaciÃ³n es una parte fundamental de cualquier proyecto de desarrollo de software. Proporciona informaciÃ³n crucial sobre cÃ³mo utilizar, mantener y contribuir al cÃ³digo. En el ecosistema de Python, existen varias bibliotecas y herramientas que facilitan la tarea de documentar el cÃ³digo de manera efectiva. En este artÃ­culo, exploraremos algunas de las principales bibliotecas de Python utilizadas para documentar cÃ³digo.\n\n\n\nSphinx es una de las herramientas de documentaciÃ³n mÃ¡s populares en el mundo de Python. Fue originalmente desarrollada para documentar la propia documentaciÃ³n de Python y se ha convertido en una elecciÃ³n comÃºn para proyectos de cÃ³digo abierto y proyectos internos. Algunas de sus caracterÃ­sticas clave incluyen:\n\nGeneraciÃ³n de documentaciÃ³n en varios formatos, incluyendo HTML, PDF, ePub y mÃ¡s.\nUtiliza reStructuredText como su formato de marcado predeterminado, que es altamente estructurado y permite documentar de manera eficiente los aspectos tÃ©cnicos.\nAmplia gama de extensiones y complementos que permiten personalizar y mejorar la documentaciÃ³n.\nAdmite la generaciÃ³n automÃ¡tica de documentaciÃ³n a partir de docstrings en el cÃ³digo Python.\nEs especialmente adecuado para documentar bibliotecas, API y proyectos tÃ©cnicos.\n\nSphinx es altamente configurable y puede generar documentaciÃ³n de alta calidad y profesional. Sin embargo, puede requerir un tiempo de configuraciÃ³n inicial y tiene una curva de aprendizaje empinada para los principiantes.\n\n\n\n\nMkDocs es una herramienta de generaciÃ³n de documentaciÃ³n que se centra en la simplicidad y la facilidad de uso. EstÃ¡ diseÃ±ada para crear documentaciÃ³n de proyectos de una manera simple y rÃ¡pida, principalmente enfocada en la generaciÃ³n de sitios web de documentaciÃ³n. Algunas de sus caracterÃ­sticas clave incluyen:\n\nUtiliza Markdown como formato de marcado predeterminado, que es fÃ¡cil de aprender y escribir.\nOfrece una interfaz de lÃ­nea de comandos simple para iniciar y generar sitios de documentaciÃ³n.\nProporciona temas y extensiones para personalizar el aspecto y la funcionalidad de la documentaciÃ³n generada.\nIdeal para proyectos de cÃ³digo abierto y documentaciÃ³n de proyectos pequeÃ±os a medianos.\n\nMkDocs es especialmente adecuado para proyectos con necesidades de documentaciÃ³n simples. Es fÃ¡cil de aprender y usar, lo que lo convierte en una excelente opciÃ³n para principiantes. Sin embargo, puede ser limitado en funcionalidad en comparaciÃ³n con Sphinx para proyectos tÃ©cnicos y complejos.\n\n\n\nMkDocs-Material es un tema personalizado para MkDocs, una popular herramienta de generaciÃ³n de sitios web estÃ¡ticos diseÃ±ada para crear documentaciÃ³n de proyectos de manera sencilla y efectiva. Este tema, conocido como â€œMaterial for MkDocsâ€, se inspira en el elegante diseÃ±o de Material Design de Google y estÃ¡ diseÃ±ado para ofrecer una experiencia de documentaciÃ³n moderna y atractiva.\nUna de las principales caracterÃ­sticas de MkDocs-Material es su enfoque en la legibilidad y la facilidad de navegaciÃ³n. Esto se logra mediante un diseÃ±o limpio y organizado que hace que la documentaciÃ³n sea mÃ¡s accesible para los usuarios. AdemÃ¡s, el tema proporciona herramientas Ãºtiles para mejorar la experiencia de los lectores, como una funciÃ³n de bÃºsqueda integrada que permite a los usuarios encontrar rÃ¡pidamente la informaciÃ³n que necesitan.\nOtra ventaja de MkDocs-Material es su navegaciÃ³n intuitiva, que facilita a los usuarios la exploraciÃ³n de la documentaciÃ³n y la navegaciÃ³n entre secciones y pÃ¡ginas. Esto es fundamental para garantizar que los usuarios puedan acceder fÃ¡cilmente a la informaciÃ³n que estÃ¡n buscando sin esfuerzo."
  },
  {
    "objectID": "posts/2023/art_docs.html#por-dÃ³nde-empiezo",
    "href": "posts/2023/art_docs.html#por-dÃ³nde-empiezo",
    "title": "DocumentaciÃ³n",
    "section": "",
    "text": "La documentaciÃ³n de proyectos sigue una progresiÃ³n simple:\n\nSin DocumentaciÃ³n\nAlguna DocumentaciÃ³n\nDocumentaciÃ³n Completa\nBuena DocumentaciÃ³n\nExcelente DocumentaciÃ³n\n\nSi te sientes perdido acerca de por dÃ³nde continuar con tu documentaciÃ³n, observa en quÃ© punto se encuentra tu proyecto en relaciÃ³n con la progresiÃ³n mencionada anteriormente. Â¿Tienes alguna documentaciÃ³n? Si no la tienes, comienza por ahÃ­. Si ya tienes algo de documentaciÃ³n pero te faltan algunos de los archivos clave del proyecto, comienza agregÃ¡ndolos.\nAl final, no te desanimes ni te sientas abrumado por la cantidad de trabajo necesario para documentar el cÃ³digo.\nUna vez que comiences a documentar tu cÃ³digo, te resultarÃ¡ mÃ¡s fÃ¡cil seguir adelante."
  },
  {
    "objectID": "posts/2023/data_sharing.html",
    "href": "posts/2023/data_sharing.html",
    "title": "Compartir datos",
    "section": "",
    "text": "Esta guÃ­a estÃ¡ diseÃ±ada para todos aquellos que necesitan colaborar con un estadÃ­stico o cientÃ­fico de datos en el proceso de anÃ¡lisis de datos. Nuestro pÃºblico objetivo abarca:\n\nColaboradores que requieren anÃ¡lisis estadÃ­sticos o de datos para sus proyectos.\nEstudiantes o posdoctorados en diversas disciplinas en busca de asesoramiento y consultorÃ­a.\nEstudiantes de estadÃ­stica junior que desempeÃ±an un papel crucial en la recopilaciÃ³n, limpieza y preparaciÃ³n de conjuntos de datos.\n\nEl propÃ³sito de esta guÃ­a es ofrecer pautas y recomendaciones para compartir datos de manera eficiente, evitando errores comunes y retrasos en la transiciÃ³n desde la recopilaciÃ³n de datos hasta su anÃ¡lisis.\nSostenemos firmemente la idea de que los estadÃ­sticos deben ser capaces de trabajar con los datos en cualquier estado en el que se encuentren.\nEs esencial examinar los datos en su estado crudo, comprender los pasos involucrados en su procesamiento y poder identificar fuentes ocultas de variabilidad en el anÃ¡lisis de datos. Sin embargo, para muchos tipos de datos, los pasos de procesamiento estÃ¡n bien documentados y estandarizados. Por lo tanto, la labor de transformar los datos desde su forma original a una forma directamente analizable puede realizarse antes de involucrar a un estadÃ­stico.\nEsta prÃ¡ctica puede acelerar significativamente el tiempo de respuesta, ya que el estadÃ­stico no tendrÃ¡ que ocuparse de todos los pasos de preprocesamiento inicialmente.\n\n\nSi deseas asegurar un anÃ¡lisis eficiente y oportuno, es fundamental proporcionar al estadÃ­stico la siguiente informaciÃ³n:\n\nDatos en su Estado Original (raw data): Esto incluye los datos sin procesar, tal como se recopilaron, sin ningÃºn tipo de transformaciÃ³n o manipulaciÃ³n. Al brindar los datos en su forma bruta, permites al estadÃ­stico comprender la fuente y la calidad de la informaciÃ³n.\nConjunto de Datos Ordenado (Tidy Data Set): Un conjunto de datos ordenado sigue los principios del â€œTidy Dataâ€, lo que significa que se organiza de manera que cada variable se encuentre en una columna y cada observaciÃ³n en una fila. Esta estructura facilita el anÃ¡lisis y la interpretaciÃ³n de los datos de manera efectiva.\nDetalles del Conjunto de Datos: Se debe describir minuciosamente cada variable presente en el conjunto de datos ordenado, incluyendo informaciÃ³n sobre los valores que pueden tomar. Proporcionar un libro de cÃ³digos es esencial para que el estadÃ­stico comprenda la naturaleza de las variables y cÃ³mo interpretarlas correctamente.\nLista de Instrucciones (script): Es crucial suministrar una descripciÃ³n detallada de los pasos exactos que seguiste para transformar los datos desde su estado original hasta el conjunto de datos ordenado. Esto garantiza la reproducibilidad y una comprensiÃ³n completa de tu proceso por parte del estadÃ­stico.\n\n\n\nUno de los elementos fundamentales en cualquier anÃ¡lisis de datos es la inclusiÃ³n de los datos en su forma mÃ¡s â€œcrudaâ€ y original. Esto garantiza que la procedencia de los datos se mantenga intacta a lo largo de todo el proceso de trabajo. AquÃ­ te proporcionamos ejemplos de lo que se entiende por datos en su estado mÃ¡s â€œcrudoâ€:\n\nUn enigmÃ¡tico archivo binario generado por tu mÃ¡quina de mediciÃ³n.\nUn archivo de Excel sin procesar con 10 hojas de trabajo que has recibido de la empresa con la que colaboras.\nComplejos datos en formato JSON que has extraÃ­do al hacer web scraping de la API de Twitter.\nNÃºmeros ingresados manualmente mientras observabas a travÃ©s de un microscopio.\n\nPuedes considerar que los datos estÃ¡n en su formato crudo si:\n\nNo se ha aplicado ningÃºn software a los datos.\nLos valores de los datos no han sido alterados.\nNo se ha eliminado ninguna parte de los datos del conjunto.\nLos datos no han sido resumidos o transformados de ninguna manera.\n\nSi realizaste alguna modificaciÃ³n en los datos en bruto, estos ya no se consideran en su forma cruda. Reportar datos modificados como datos en bruto es una prÃ¡ctica comÃºn que puede ralentizar significativamente el proceso de anÃ¡lisis, ya que el analista a menudo debe realizar un examen minucioso de tus datos para comprender por quÃ© los datos en su forma cruda parecen inusuales. (Imagina tambiÃ©n cÃ³mo impactarÃ­a en futuros anÃ¡lisis si llegan nuevos datos). Mantener la integridad de los datos en su forma cruda es esencial para un anÃ¡lisis sÃ³lido y confiable.\nClaro, te proporcionarÃ© un ejemplo prÃ¡ctico y aplicado en Python que ilustra los principios de datos ordenados (tidy data) de Hadley Wickham. En este ejemplo, trabajaremos con un conjunto de datos de resultados de exÃ¡menes mÃ©dicos para varios pacientes.\nEjemplo PrÃ¡ctico\nSupongamos que tenemos un conjunto de datos de resultados de exÃ¡menes mÃ©dicos para tres pacientes en una clÃ­nica. Los exÃ¡menes incluyen mediciones de presiÃ³n arterial (sistÃ³lica y diastÃ³lica), nivel de glucosa y nivel de colesterol. El conjunto de datos se ve de la siguiente manera:\n\n\n\n\n\n\n\n\n\n\nPaciente\nPresion_Sistolica\nPresion_Diastolica\nGlucosa\nColesterol\n\n\n\n\n1\n120\n80\n95\n180\n\n\n2\n130\n85\n105\n190\n\n\n3\n115\n75\n90\n170\n\n\n\nEstos datos estÃ¡n sin procesar, tal como se recopilarÃ­an de los pacientes.\n\n\n\nLos principios generales de los datos ordenados (tidy data) son presentados por Hadley Wickham en este artÃ­culo y este video. Aunque tanto el artÃ­culo como el video describen los datos ordenados utilizando R, los principios son aplicables de manera mÃ¡s general:\n\nCada variable que mides debe estar en una columna.\nCada observaciÃ³n diferente de esa variable debe estar en una fila diferente.\nDebe haber una tabla para cada â€œtipoâ€ de variable.\nSi tienes varias tablas, deben incluir una columna en la tabla que permita unirlas o combinarlas.\n\nLa idea detrÃ¡s de los datos ordenados es que los datos se organicen de manera que sea fÃ¡cil identificar y seleccionar variables, asÃ­ como realizar anÃ¡lisis de manera eficiente. Siguiendo estos principios, se logra una estructura de datos que es intuitiva y facilita el trabajo de los analistas y cientÃ­ficos de datos, ya que no necesitan descifrar la estructura de los datos en cada proyecto. TambiÃ©n se promueve el uso de nombres descriptivos para las variables, lo que mejora la comprensiÃ³n de los datos por parte de otros colaboradores.\nEjemplo PrÃ¡ctico\nContinuando con el ejemplo, tenemos cuatro variables: Paciente, Presion_Sistolica, Presion_Diastolica, Glucosa y Colesterol. AsÃ­ que, creamos un conjunto de datos ordenado donde cada una de estas variables tiene su propia columna:\n\n\n\nPaciente\nVariable\nValor\n\n\n\n\n1\nPresion_Sistolica\n120\n\n\n1\nPresion_Diastolica\n80\n\n\n1\nGlucosa\n95\n\n\n1\nColesterol\n180\n\n\n2\nPresion_Sistolica\n130\n\n\n2\nPresion_Diastolica\n85\n\n\n2\nGlucosa\n105\n\n\n2\nColesterol\n190\n\n\n3\nPresion_Sistolica\n115\n\n\n3\nPresion_Diastolica\n75\n\n\n3\nGlucosa\n90\n\n\n3\nColesterol\n170\n\n\n\nEsto es un conjunto de datos ordenado porque cumple con las reglas mencionadas anteriormente.\n\n\n\nPara casi cualquier conjunto de datos, las mediciones que calculas necesitarÃ¡n ser descritas con mÃ¡s detalle de lo que puedes o debes incluir en la hoja de cÃ¡lculo. El libro de cÃ³digos contiene esta informaciÃ³n. Como mÃ­nimo, deberÃ­a contener:\n\nInformaciÃ³n sobre las variables (Â¡incluyendo unidades!) en el conjunto de datos que no estÃ¡n contenidas en los datos ordenados (tidy data).\nInformaciÃ³n sobre las elecciones de resumen que hiciste.\nInformaciÃ³n sobre el diseÃ±o experimental que utilizaste.\n\nEjemplo PrÃ¡ctico\nInformaciÃ³n sobre las Variables:\nEn nuestro ejemplo de resultados de exÃ¡menes mÃ©dicos, tenemos los detalles adicionales sobre las variables:\n\nPresion_Sistolica: PresiÃ³n arterial sistÃ³lica medida en mmHg (milÃ­metros de mercurio).\nPresion_Diastolica: PresiÃ³n arterial diastÃ³lica medida en mmHg.\nGlucosa: Nivel de glucosa en sangre medido en mg/dL (miligramos por decilitro).\nColesterol: Nivel de colesterol en sangre medido en mg/dL.\n\nEstos detalles son importantes para que otros comprendan las unidades de medida y la naturaleza de las variables.\nInformaciÃ³n sobre las Elecciones de Resumen:\nSi realizaste alguna operaciÃ³n de resumen en los datos, como el cÃ¡lculo de promedios o medianas, debes explicar estas elecciones en el libro de cÃ³digos. Por ejemplo, si calculaste el promedio de la presiÃ³n arterial sistÃ³lica para resumir los datos, debes mencionarlo y proporcionar la fÃ³rmula utilizada.\n\nPromedio de la presiÃ³n sistÃ³lica: Se calculÃ³ como la media de todas las mediciones de presiÃ³n sistÃ³lica en el conjunto de datos.\n\nPromedio de la presiÃ³n sistÃ³lica = (Suma de todas las mediciones de presiÃ³n sistÃ³lica) / (NÃºmero de pacientes)\n\nMediana de la presiÃ³n diastÃ³lica: Se calculÃ³ como la mediana de todas las mediciones de presiÃ³n diastÃ³lica en el conjunto de datos.\n\nMediana de la presiÃ³n diastÃ³lica = Valor central cuando las mediciones se ordenan de menor a mayor\n\n\nInformaciÃ³n sobre el DiseÃ±o Experimental:\nSe debe incluir detalles sobre cÃ³mo se recopilaron los datos y el diseÃ±o experimental subyacente. Por ejemplo, si los datos se recopilaron de pacientes en un estudio clÃ­nico, debes describir cÃ³mo se seleccionaron los pacientes, si hubo aleatorizaciÃ³n en los tratamientos, y cualquier otro detalle relevante sobre el diseÃ±o del estudio.\n\nSelecciÃ³n de Pacientes: Los pacientes fueron seleccionados de un centro de atenciÃ³n mÃ©dica en el que se atienden personas con afecciones cardiovasculares. La selecciÃ³n de pacientes se realizÃ³ de manera no aleatoria, y se incluyeron aquellos que cumplieron con los siguientes criterios de inclusiÃ³n: diagnÃ³stico de enfermedad cardiovascular confirmado, disponibilidad de datos de presiÃ³n arterial, niveles de glucosa y colesterol, y consentimiento para participar en el estudio.\nAleatorizaciÃ³n en Tratamientos: En este estudio, no se aplicÃ³ aleatorizaciÃ³n en los tratamientos. Los pacientes recibieron el tratamiento mÃ©dico habitual segÃºn las pautas clÃ­nicas establecidas por sus mÃ©dicos tratantes. Por lo tanto, no se realizaron intervenciones experimentales ni asignaciones aleatorias de tratamientos.\nRecopilaciÃ³n de Datos: Los datos se recopilaron mediante la revisiÃ³n de los registros mÃ©dicos electrÃ³nicos de los pacientes. Se registraron las mediciones de presiÃ³n arterial sistÃ³lica y diastÃ³lica, los niveles de glucosa en sangre y los niveles de colesterol en momentos especÃ­ficos de las consultas mÃ©dicas de los pacientes.\n\n\n\n\nEs posible que hayas escuchado esto antes, pero la reproducibilidad es un gran tema en la ciencia computacional. Esto significa que cuando envÃ­es tu artÃ­culo, los revisores y el resto del mundo deberÃ­an poder replicar exactamente los anÃ¡lisis desde los datos crudos hasta los resultados finales. Si estÃ¡s tratando de ser eficiente, es probable que realices algunas etapas de resumen/anÃ¡lisis de datos antes de que los datos se consideren ordenados.\nLo ideal que debes hacer al realizar el resumen es crear un script de computadora (en R, Python, u otro lenguaje) que tome los datos crudos como entrada y produzca los datos ordenados que estÃ¡s compartiendo como salida. Puedes intentar ejecutar tu script algunas veces y ver si produce la misma salida.\nEn muchos casos, la persona que recopilÃ³ los datos tiene incentivos para que sean ordenados para un estadÃ­stico y acelerar el proceso de colaboraciÃ³n. Es posible que no sepa cÃ³mo programar en un lenguaje de script. En ese caso, lo que debes proporcionar al estadÃ­stico es algo llamado pseudocÃ³digo. DeberÃ­a verse algo como:\n\nPaso 1 - tomar el archivo crudo, ejecutar la versiÃ³n 3.1.2 del software de resumen con parÃ¡metros a=1, b=2, c=3\nPaso 2 - ejecutar el software por separado para cada muestra\nPaso 3 - tomar la tercera columna del archivo de salida para cada muestra y esa es la fila correspondiente en el conjunto de datos de salida\n\nTambiÃ©n debes incluir informaciÃ³n sobre quÃ© sistema (Mac/Windows/Linux) utilizaste para el software y si lo intentaste mÃ¡s de una vez para confirmar que daba los mismos resultados. Idealmente, deberÃ­as hacer que otro estudiante o compaÃ±ero de laboratorio confirme que puede obtener el mismo archivo de salida que tÃº.\nEjemplo PrÃ¡ctico\nEste cÃ³digo se ejecutÃ³ en Python 3.8.5 y utiliza las bibliotecas pandas y IPython.display. Las dependencias y la versiÃ³n de Python estÃ¡n indicadas en los comentarios para mayor claridad.\n# Importar las bibliotecas necesarias\nimport pandas as pd\nfrom IPython.display import display\n\n# VersiÃ³n de Python utilizada: 3.8.5\n\n# Crear un DataFrame con los datos originales\ndata = {\n    'Paciente': [1, 2, 3],\n    'Presion_Sistolica': [120, 130, 115],\n    'Presion_Diastolica': [80, 85, 75],\n    'Glucosa': [95, 105, 90],\n    'Colesterol': [180, 190, 170]\n}\n\ndf_original = pd.DataFrame(data)\n\n# Crear un nuevo DataFrame en el formato deseado (unstack)\ndf_unstacked = df_original.set_index('Paciente').stack().reset_index()\ndf_unstacked.columns = ['Paciente', 'Variable', 'Valor']\n\n# Calcular diferentes estadÃ­sticas\nestadisticas = df_unstacked.groupby('Variable')['Valor'].describe()\n\n# Mostrar el DataFrame en el formato antiguo   \nprint(\"Mostrar el DataFrame en el formato antiguo:\")\ndisplay(df_original)\n\n# Mostrar el DataFrame en el formato nuevo   \nprint(\"\\nMostrar el DataFrame en el formato nuevo:\")\ndisplay(df_unstacked)\n\n# Mostrar las estadÃ­sticas bÃ¡sicas\nprint(\"\\nMostrar las estadÃ­sticas bÃ¡sicas:\")\ndisplay(estadisticas)\n\n\n\nCuando entregas un conjunto de datos debidamente ordenados, disminuye drÃ¡sticamente la carga de trabajo del estadÃ­stico. Entonces, con suerte, te responderÃ¡n mucho mÃ¡s rÃ¡pido. Pero la mayorÃ­a de los estadÃ­sticos cuidadosos verificarÃ¡n tu procedimiento, harÃ¡n preguntas sobre las etapas que realizaste y tratarÃ¡n de confirmar que pueden obtener los mismos datos ordenados que tÃº, al menos con verificaciones puntuales.\nDeberÃ­as esperar del estadÃ­stico:\n\nUn script de anÃ¡lisis que realice cada uno de los anÃ¡lisis (no solo instrucciones).\nEl cÃ³digo de computadora exacto que utilizaron para ejecutar el anÃ¡lisis.\nTodos los archivos de salida/figuras que generaron.\n\nEsta es la informaciÃ³n que utilizarÃ¡s para establecer la reproducibilidad y la precisiÃ³n de tus resultados. Cada uno de los pasos en el anÃ¡lisis debe estar claramente explicado y debes hacer preguntas cuando no entiendas lo que hizo el analista. Es responsabilidad tanto del estadÃ­stico como del cientÃ­fico comprender el anÃ¡lisis estadÃ­stico.\nEs posible que no puedas realizar los anÃ¡lisis exactos sin el cÃ³digo del estadÃ­stico, pero deberÃ­as poder explicar por quÃ© el estadÃ­stico realizÃ³ cada paso a un compaÃ±ero de laboratorio o a tu investigador principal."
  },
  {
    "objectID": "posts/2023/data_sharing.html#elementos-esenciales-para-una-colaboraciÃ³n-exitosa",
    "href": "posts/2023/data_sharing.html#elementos-esenciales-para-una-colaboraciÃ³n-exitosa",
    "title": "Compartir datos",
    "section": "",
    "text": "Si deseas asegurar un anÃ¡lisis eficiente y oportuno, es fundamental proporcionar al estadÃ­stico la siguiente informaciÃ³n:\n\nDatos en su Estado Original (raw data): Esto incluye los datos sin procesar, tal como se recopilaron, sin ningÃºn tipo de transformaciÃ³n o manipulaciÃ³n. Al brindar los datos en su forma bruta, permites al estadÃ­stico comprender la fuente y la calidad de la informaciÃ³n.\nConjunto de Datos Ordenado (Tidy Data Set): Un conjunto de datos ordenado sigue los principios del â€œTidy Dataâ€, lo que significa que se organiza de manera que cada variable se encuentre en una columna y cada observaciÃ³n en una fila. Esta estructura facilita el anÃ¡lisis y la interpretaciÃ³n de los datos de manera efectiva.\nDetalles del Conjunto de Datos: Se debe describir minuciosamente cada variable presente en el conjunto de datos ordenado, incluyendo informaciÃ³n sobre los valores que pueden tomar. Proporcionar un libro de cÃ³digos es esencial para que el estadÃ­stico comprenda la naturaleza de las variables y cÃ³mo interpretarlas correctamente.\nLista de Instrucciones (script): Es crucial suministrar una descripciÃ³n detallada de los pasos exactos que seguiste para transformar los datos desde su estado original hasta el conjunto de datos ordenado. Esto garantiza la reproducibilidad y una comprensiÃ³n completa de tu proceso por parte del estadÃ­stico.\n\n\n\nUno de los elementos fundamentales en cualquier anÃ¡lisis de datos es la inclusiÃ³n de los datos en su forma mÃ¡s â€œcrudaâ€ y original. Esto garantiza que la procedencia de los datos se mantenga intacta a lo largo de todo el proceso de trabajo. AquÃ­ te proporcionamos ejemplos de lo que se entiende por datos en su estado mÃ¡s â€œcrudoâ€:\n\nUn enigmÃ¡tico archivo binario generado por tu mÃ¡quina de mediciÃ³n.\nUn archivo de Excel sin procesar con 10 hojas de trabajo que has recibido de la empresa con la que colaboras.\nComplejos datos en formato JSON que has extraÃ­do al hacer web scraping de la API de Twitter.\nNÃºmeros ingresados manualmente mientras observabas a travÃ©s de un microscopio.\n\nPuedes considerar que los datos estÃ¡n en su formato crudo si:\n\nNo se ha aplicado ningÃºn software a los datos.\nLos valores de los datos no han sido alterados.\nNo se ha eliminado ninguna parte de los datos del conjunto.\nLos datos no han sido resumidos o transformados de ninguna manera.\n\nSi realizaste alguna modificaciÃ³n en los datos en bruto, estos ya no se consideran en su forma cruda. Reportar datos modificados como datos en bruto es una prÃ¡ctica comÃºn que puede ralentizar significativamente el proceso de anÃ¡lisis, ya que el analista a menudo debe realizar un examen minucioso de tus datos para comprender por quÃ© los datos en su forma cruda parecen inusuales. (Imagina tambiÃ©n cÃ³mo impactarÃ­a en futuros anÃ¡lisis si llegan nuevos datos). Mantener la integridad de los datos en su forma cruda es esencial para un anÃ¡lisis sÃ³lido y confiable.\nClaro, te proporcionarÃ© un ejemplo prÃ¡ctico y aplicado en Python que ilustra los principios de datos ordenados (tidy data) de Hadley Wickham. En este ejemplo, trabajaremos con un conjunto de datos de resultados de exÃ¡menes mÃ©dicos para varios pacientes.\nEjemplo PrÃ¡ctico\nSupongamos que tenemos un conjunto de datos de resultados de exÃ¡menes mÃ©dicos para tres pacientes en una clÃ­nica. Los exÃ¡menes incluyen mediciones de presiÃ³n arterial (sistÃ³lica y diastÃ³lica), nivel de glucosa y nivel de colesterol. El conjunto de datos se ve de la siguiente manera:\n\n\n\n\n\n\n\n\n\n\nPaciente\nPresion_Sistolica\nPresion_Diastolica\nGlucosa\nColesterol\n\n\n\n\n1\n120\n80\n95\n180\n\n\n2\n130\n85\n105\n190\n\n\n3\n115\n75\n90\n170\n\n\n\nEstos datos estÃ¡n sin procesar, tal como se recopilarÃ­an de los pacientes.\n\n\n\nLos principios generales de los datos ordenados (tidy data) son presentados por Hadley Wickham en este artÃ­culo y este video. Aunque tanto el artÃ­culo como el video describen los datos ordenados utilizando R, los principios son aplicables de manera mÃ¡s general:\n\nCada variable que mides debe estar en una columna.\nCada observaciÃ³n diferente de esa variable debe estar en una fila diferente.\nDebe haber una tabla para cada â€œtipoâ€ de variable.\nSi tienes varias tablas, deben incluir una columna en la tabla que permita unirlas o combinarlas.\n\nLa idea detrÃ¡s de los datos ordenados es que los datos se organicen de manera que sea fÃ¡cil identificar y seleccionar variables, asÃ­ como realizar anÃ¡lisis de manera eficiente. Siguiendo estos principios, se logra una estructura de datos que es intuitiva y facilita el trabajo de los analistas y cientÃ­ficos de datos, ya que no necesitan descifrar la estructura de los datos en cada proyecto. TambiÃ©n se promueve el uso de nombres descriptivos para las variables, lo que mejora la comprensiÃ³n de los datos por parte de otros colaboradores.\nEjemplo PrÃ¡ctico\nContinuando con el ejemplo, tenemos cuatro variables: Paciente, Presion_Sistolica, Presion_Diastolica, Glucosa y Colesterol. AsÃ­ que, creamos un conjunto de datos ordenado donde cada una de estas variables tiene su propia columna:\n\n\n\nPaciente\nVariable\nValor\n\n\n\n\n1\nPresion_Sistolica\n120\n\n\n1\nPresion_Diastolica\n80\n\n\n1\nGlucosa\n95\n\n\n1\nColesterol\n180\n\n\n2\nPresion_Sistolica\n130\n\n\n2\nPresion_Diastolica\n85\n\n\n2\nGlucosa\n105\n\n\n2\nColesterol\n190\n\n\n3\nPresion_Sistolica\n115\n\n\n3\nPresion_Diastolica\n75\n\n\n3\nGlucosa\n90\n\n\n3\nColesterol\n170\n\n\n\nEsto es un conjunto de datos ordenado porque cumple con las reglas mencionadas anteriormente.\n\n\n\nPara casi cualquier conjunto de datos, las mediciones que calculas necesitarÃ¡n ser descritas con mÃ¡s detalle de lo que puedes o debes incluir en la hoja de cÃ¡lculo. El libro de cÃ³digos contiene esta informaciÃ³n. Como mÃ­nimo, deberÃ­a contener:\n\nInformaciÃ³n sobre las variables (Â¡incluyendo unidades!) en el conjunto de datos que no estÃ¡n contenidas en los datos ordenados (tidy data).\nInformaciÃ³n sobre las elecciones de resumen que hiciste.\nInformaciÃ³n sobre el diseÃ±o experimental que utilizaste.\n\nEjemplo PrÃ¡ctico\nInformaciÃ³n sobre las Variables:\nEn nuestro ejemplo de resultados de exÃ¡menes mÃ©dicos, tenemos los detalles adicionales sobre las variables:\n\nPresion_Sistolica: PresiÃ³n arterial sistÃ³lica medida en mmHg (milÃ­metros de mercurio).\nPresion_Diastolica: PresiÃ³n arterial diastÃ³lica medida en mmHg.\nGlucosa: Nivel de glucosa en sangre medido en mg/dL (miligramos por decilitro).\nColesterol: Nivel de colesterol en sangre medido en mg/dL.\n\nEstos detalles son importantes para que otros comprendan las unidades de medida y la naturaleza de las variables.\nInformaciÃ³n sobre las Elecciones de Resumen:\nSi realizaste alguna operaciÃ³n de resumen en los datos, como el cÃ¡lculo de promedios o medianas, debes explicar estas elecciones en el libro de cÃ³digos. Por ejemplo, si calculaste el promedio de la presiÃ³n arterial sistÃ³lica para resumir los datos, debes mencionarlo y proporcionar la fÃ³rmula utilizada.\n\nPromedio de la presiÃ³n sistÃ³lica: Se calculÃ³ como la media de todas las mediciones de presiÃ³n sistÃ³lica en el conjunto de datos.\n\nPromedio de la presiÃ³n sistÃ³lica = (Suma de todas las mediciones de presiÃ³n sistÃ³lica) / (NÃºmero de pacientes)\n\nMediana de la presiÃ³n diastÃ³lica: Se calculÃ³ como la mediana de todas las mediciones de presiÃ³n diastÃ³lica en el conjunto de datos.\n\nMediana de la presiÃ³n diastÃ³lica = Valor central cuando las mediciones se ordenan de menor a mayor\n\n\nInformaciÃ³n sobre el DiseÃ±o Experimental:\nSe debe incluir detalles sobre cÃ³mo se recopilaron los datos y el diseÃ±o experimental subyacente. Por ejemplo, si los datos se recopilaron de pacientes en un estudio clÃ­nico, debes describir cÃ³mo se seleccionaron los pacientes, si hubo aleatorizaciÃ³n en los tratamientos, y cualquier otro detalle relevante sobre el diseÃ±o del estudio.\n\nSelecciÃ³n de Pacientes: Los pacientes fueron seleccionados de un centro de atenciÃ³n mÃ©dica en el que se atienden personas con afecciones cardiovasculares. La selecciÃ³n de pacientes se realizÃ³ de manera no aleatoria, y se incluyeron aquellos que cumplieron con los siguientes criterios de inclusiÃ³n: diagnÃ³stico de enfermedad cardiovascular confirmado, disponibilidad de datos de presiÃ³n arterial, niveles de glucosa y colesterol, y consentimiento para participar en el estudio.\nAleatorizaciÃ³n en Tratamientos: En este estudio, no se aplicÃ³ aleatorizaciÃ³n en los tratamientos. Los pacientes recibieron el tratamiento mÃ©dico habitual segÃºn las pautas clÃ­nicas establecidas por sus mÃ©dicos tratantes. Por lo tanto, no se realizaron intervenciones experimentales ni asignaciones aleatorias de tratamientos.\nRecopilaciÃ³n de Datos: Los datos se recopilaron mediante la revisiÃ³n de los registros mÃ©dicos electrÃ³nicos de los pacientes. Se registraron las mediciones de presiÃ³n arterial sistÃ³lica y diastÃ³lica, los niveles de glucosa en sangre y los niveles de colesterol en momentos especÃ­ficos de las consultas mÃ©dicas de los pacientes.\n\n\n\n\nEs posible que hayas escuchado esto antes, pero la reproducibilidad es un gran tema en la ciencia computacional. Esto significa que cuando envÃ­es tu artÃ­culo, los revisores y el resto del mundo deberÃ­an poder replicar exactamente los anÃ¡lisis desde los datos crudos hasta los resultados finales. Si estÃ¡s tratando de ser eficiente, es probable que realices algunas etapas de resumen/anÃ¡lisis de datos antes de que los datos se consideren ordenados.\nLo ideal que debes hacer al realizar el resumen es crear un script de computadora (en R, Python, u otro lenguaje) que tome los datos crudos como entrada y produzca los datos ordenados que estÃ¡s compartiendo como salida. Puedes intentar ejecutar tu script algunas veces y ver si produce la misma salida.\nEn muchos casos, la persona que recopilÃ³ los datos tiene incentivos para que sean ordenados para un estadÃ­stico y acelerar el proceso de colaboraciÃ³n. Es posible que no sepa cÃ³mo programar en un lenguaje de script. En ese caso, lo que debes proporcionar al estadÃ­stico es algo llamado pseudocÃ³digo. DeberÃ­a verse algo como:\n\nPaso 1 - tomar el archivo crudo, ejecutar la versiÃ³n 3.1.2 del software de resumen con parÃ¡metros a=1, b=2, c=3\nPaso 2 - ejecutar el software por separado para cada muestra\nPaso 3 - tomar la tercera columna del archivo de salida para cada muestra y esa es la fila correspondiente en el conjunto de datos de salida\n\nTambiÃ©n debes incluir informaciÃ³n sobre quÃ© sistema (Mac/Windows/Linux) utilizaste para el software y si lo intentaste mÃ¡s de una vez para confirmar que daba los mismos resultados. Idealmente, deberÃ­as hacer que otro estudiante o compaÃ±ero de laboratorio confirme que puede obtener el mismo archivo de salida que tÃº.\nEjemplo PrÃ¡ctico\nEste cÃ³digo se ejecutÃ³ en Python 3.8.5 y utiliza las bibliotecas pandas y IPython.display. Las dependencias y la versiÃ³n de Python estÃ¡n indicadas en los comentarios para mayor claridad.\n# Importar las bibliotecas necesarias\nimport pandas as pd\nfrom IPython.display import display\n\n# VersiÃ³n de Python utilizada: 3.8.5\n\n# Crear un DataFrame con los datos originales\ndata = {\n    'Paciente': [1, 2, 3],\n    'Presion_Sistolica': [120, 130, 115],\n    'Presion_Diastolica': [80, 85, 75],\n    'Glucosa': [95, 105, 90],\n    'Colesterol': [180, 190, 170]\n}\n\ndf_original = pd.DataFrame(data)\n\n# Crear un nuevo DataFrame en el formato deseado (unstack)\ndf_unstacked = df_original.set_index('Paciente').stack().reset_index()\ndf_unstacked.columns = ['Paciente', 'Variable', 'Valor']\n\n# Calcular diferentes estadÃ­sticas\nestadisticas = df_unstacked.groupby('Variable')['Valor'].describe()\n\n# Mostrar el DataFrame en el formato antiguo   \nprint(\"Mostrar el DataFrame en el formato antiguo:\")\ndisplay(df_original)\n\n# Mostrar el DataFrame en el formato nuevo   \nprint(\"\\nMostrar el DataFrame en el formato nuevo:\")\ndisplay(df_unstacked)\n\n# Mostrar las estadÃ­sticas bÃ¡sicas\nprint(\"\\nMostrar las estadÃ­sticas bÃ¡sicas:\")\ndisplay(estadisticas)\n\n\n\nCuando entregas un conjunto de datos debidamente ordenados, disminuye drÃ¡sticamente la carga de trabajo del estadÃ­stico. Entonces, con suerte, te responderÃ¡n mucho mÃ¡s rÃ¡pido. Pero la mayorÃ­a de los estadÃ­sticos cuidadosos verificarÃ¡n tu procedimiento, harÃ¡n preguntas sobre las etapas que realizaste y tratarÃ¡n de confirmar que pueden obtener los mismos datos ordenados que tÃº, al menos con verificaciones puntuales.\nDeberÃ­as esperar del estadÃ­stico:\n\nUn script de anÃ¡lisis que realice cada uno de los anÃ¡lisis (no solo instrucciones).\nEl cÃ³digo de computadora exacto que utilizaron para ejecutar el anÃ¡lisis.\nTodos los archivos de salida/figuras que generaron.\n\nEsta es la informaciÃ³n que utilizarÃ¡s para establecer la reproducibilidad y la precisiÃ³n de tus resultados. Cada uno de los pasos en el anÃ¡lisis debe estar claramente explicado y debes hacer preguntas cuando no entiendas lo que hizo el analista. Es responsabilidad tanto del estadÃ­stico como del cientÃ­fico comprender el anÃ¡lisis estadÃ­stico.\nEs posible que no puedas realizar los anÃ¡lisis exactos sin el cÃ³digo del estadÃ­stico, pero deberÃ­as poder explicar por quÃ© el estadÃ­stico realizÃ³ cada paso a un compaÃ±ero de laboratorio o a tu investigador principal."
  },
  {
    "objectID": "about-es.html",
    "href": "about-es.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English EspaÃ±ol\n\n\n\nHolağŸ‘‹ Mi nombre es Francisco!\n\n\n\n\nProfesiÃ³n: ğŸ“Š Ingeniero MatemÃ¡tico\nTrabajo Actual:\n\nğŸ’» Jefe de AnalÃ­tica Avanzada (Grupo Security)\nğŸ“– Profesor Asociado (UTFSM)\n\n\n\nIntereses\nğŸ® Videojuegos | ğŸ€ Baloncesto | ğŸ’¡ Aprendizaje\nâœ… Desarrollo de Software  âœ… Modelado EstadÃ­stico, Series Temporales  âœ… Aprendizaje AutomÃ¡tico/Profundo  âœ… ComputaciÃ³n en la Nube, Big Data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Francisco Alfaro M.",
    "section": "",
    "text": "English EspaÃ±ol\n\n\n\nHi ğŸ‘‹ My name is Francisco!\n\n\n\n\nProfession: ğŸ“Š Mathematical Engineer\nCurrent Work:\n\nğŸ’» Head of Advanced Analytics (Grupo Security)\nğŸ“– Associate Lecturer (UTFSM)\n\n\n\nInterests\nğŸ® Gaming | ğŸ€ Basketball | ğŸ’¡ Learning\nâœ… Software Development  âœ… Statistical Modelling, Time Series  âœ… Machine/Deep Learning  âœ… Cloud computing, Big Data"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Compartir datos\n\n\nCÃ³mo Compartir Datos de Manera Efectiva (desde el punto de vista estadÃ­stico) y no morir en el intento.\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n26 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentaciÃ³n\n\n\nEnteneder los pasos para crear una buena documentaciÃ³n en Python (mÃ¡s algunas recomendaciones).\n\n\n\npython\n\n\ndocs\n\n\n\n\n\n\n\n\n\n7 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nGitlab PDF\n\n\nCÃ³mo aprovechar GitLab CI/CD para generar archivos PDF utilizando los artefactos de un Pipeline.\n\n\n\nci-cd\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n1 oct 2023\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Impact\n\n\nCausalImpact creado por Google estima el impacto de una intervenciÃ³n en una serie temporal.\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n15 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nCollaborative Filtering\n\n\nUser-based collaborative filtering para realizar un mejor sistema de recomendaciÃ³n de pelÃ­culas.\n\n\n\npython\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n12 oct 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nTest Driven Development\n\n\nCÃ³mo abordar el desarrollo de software para Data Science usando Test Driven Development.\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nPolars\n\n\nPolars es una librerÃ­a de DataFrames increÃ­blemente rÃ¡pida y eficiente implementada en Rust.\n\n\n\npython\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n25 may 2022\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nImpact on Digital Learning\n\n\nCompetition Solution: LearnPlatform COVID-19 Impact on Digital Learning proposed by Kaggle.\n\n\n\nkaggle\n\n\ndata-analysis\n\n\n\n\n\n\n\n\n\n31 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nFastpages\n\n\nFastpages es una plataforma que te permite crear y alojar un blog con Jupyter Notebooks.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n20 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nBuenas PrÃ¡cticas - Python\n\n\nConsejos que te ayudarÃ¡n a mejorar tus skills en el desarrollo de software (con Python).\n\n\n\npython\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\n15 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Book\n\n\nJupyter Book es una herramienta para crear documentos mediante Jupyter Notebooks y/o Markdown.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n11 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nRISE\n\n\nRISE es una extensiÃ³n a los Jupyter Notebooks que permite transformar tus notebooks en presentaciones interactivas.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n5 ago 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Noteboook\n\n\nJupyter Notebook, es un entorno de trabajo interactivo que permite desarrollar cÃ³digo en Python.\n\n\n\npython\n\n\njupyter-notebooks\n\n\n\n\n\n\n\n\n\n31 jul 2021\n\n\nFrancisco Alfaro\n\n\n\n\n\n\nNo hay resultados"
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html",
    "href": "posts/2023/gitlab_pdf.html",
    "title": "Gitlab PDF",
    "section": "",
    "text": "GitLab CI/CD es una potente herramienta que permite automatizar y gestionar el ciclo de vida de las aplicaciones de software.\nCI (IntegraciÃ³n Continua) y CD (Entrega Continua) son prÃ¡cticas esenciales en el desarrollo de software moderno que buscan mejorar la calidad del cÃ³digo, aumentar la eficiencia y reducir los errores. GitLab CI/CD se integra de manera nativa en el flujo de trabajo de GitLab, lo que lo convierte en una opciÃ³n atractiva para equipos de desarrollo.\n\nCI (IntegraciÃ³n Continua): Es el proceso de integrar cambios de cÃ³digo frecuentes en un repositorio compartido. Esto implica la ejecuciÃ³n automÃ¡tica de pruebas y anÃ¡lisis de calidad cada vez que se envÃ­a cÃ³digo. El objetivo es identificar y corregir problemas de manera temprana en el ciclo de desarrollo.\nCD (Entrega Continua): Una vez que las pruebas de CI se han superado con Ã©xito, el cÃ³digo se considera apto para su implementaciÃ³n en entornos de producciÃ³n o de pruebas. El objetivo es entregar de manera eficiente y confiable el software a los usuarios finales.\n\n\n\n\nAntes de profundizar en la generaciÃ³n de archivos PDF, es importante comprender el concepto de â€œartefactosâ€ en GitLab CI/CD. Los artefactos son archivos o conjuntos de archivos generados como resultado de una ejecuciÃ³n exitosa de un pipeline.\nEstos artefactos se almacenan en GitLab y se pueden utilizar posteriormente en otros trabajos o pipelines.\nEn el contexto de la generaciÃ³n de archivos PDF, los artefactos son esenciales porque permiten que los archivos PDF generados en un trabajo se conserven y utilicen en otros trabajos o etapas del pipeline.\n\n\n\n\nLa generaciÃ³n de archivos PDF como parte de su proceso de CI/CD puede ser Ãºtil en varios escenarios, como la creaciÃ³n de informes automatizados, la generaciÃ³n de documentaciÃ³n tÃ©cnica o la producciÃ³n de facturas en lÃ­nea.\nA continuaciÃ³n, detallaremos cÃ³mo lograrlo utilizando GitLab CI/CD:\n\nNota: Tomaremos como referencia el siguiente repositorio.\n\n\n\nAntes de comenzar, asegÃºrese de que su proyecto de GitLab estÃ© configurado correctamente y tenga acceso a GitLab CI/CD. TambiÃ©n debe tener un archivo de cÃ³digo fuente que desee convertir en un archivo PDF.\nAsegÃºrese de que cualquier dependencia necesaria estÃ© especificada en su archivo de configuraciÃ³n de CI/CD.\n\n\n\n\nCree un script que sea capaz de generar el archivo PDF a partir de sus datos de entrada.\nEste script deberÃ­a tomar los datos relevantes y formatearlos en un archivo PDF.\n\n\n\n\nEn su repositorio de GitLab, cree un archivo llamado .gitlab-ci.yml si aÃºn no lo ha hecho.\nEste archivo contiene la configuraciÃ³n de su pipeline.\nAquÃ­ hay un ejemplo de cÃ³mo podrÃ­a verse:\nstages:\n  - pdf\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\n  artifacts:\n    paths:\n      - ./*.pdf\nEn este ejemplo:\nstages:\n  - pdf\n\nstages: Esta secciÃ³n define las etapas (stages). En este caso, solo se define una etapa llamada â€œpdfâ€. Las etapas son divisiones lÃ³gicas en el pipeline que agrupan trabajos relacionados. En este caso, el pipeline tiene una sola etapa llamada â€œpdfâ€.\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\ngenerate_pdf: Esta secciÃ³n define un trabajo (job) llamado â€œgenerate_pdfâ€. Un trabajo es una unidad de ejecuciÃ³n en el pipeline. AquÃ­ estÃ¡ el desglose de esta secciÃ³n:\n\nstage: pdf: Esta lÃ­nea especifica que este trabajo pertenece a la etapa â€œpdfâ€ definida previamente. En otras palabras, este trabajo se ejecutarÃ¡ en la etapa â€œpdfâ€ del pipeline.\nimage: aergus/latex: Esta lÃ­nea especifica la imagen Docker que se utilizarÃ¡ para ejecutar este trabajo. En este caso, se utiliza la imagen â€œaergus/latexâ€, que contiene un entorno LaTeX para compilar documentos PDF. Esta imagen es esencial para compilar archivos LaTeX en archivos PDF.\nscript: AquÃ­ se definen los comandos que se ejecutarÃ¡n en el trabajo. En este caso, se utiliza el comando â€œlatexmk -pdf **/*.texâ€. Este comando utiliza â€œlatexmkâ€ para compilar todos los archivos â€œ.texâ€ en el proyecto en archivos PDF. El uso de **/*.tex significa que buscarÃ¡ archivos â€œ.texâ€ en todos los subdirectorios del proyecto.\n\n\nartifacts:\n  paths:\n    - ./*.pdf\n\nartifacts: Esta secciÃ³n especifica quÃ© archivos deben considerarse artefactos y, por lo tanto, se conservarÃ¡n despuÃ©s de una ejecuciÃ³n exitosa del trabajo. AquÃ­ estÃ¡ el desglose de esta secciÃ³n:\n\npaths: ./*.pdf: Esta lÃ­nea especifica que todos los archivos con extensiÃ³n â€œ.pdfâ€ en el directorio actual deben considerarse artefactos. Esto significa que los archivos PDF generados como resultado de la ejecuciÃ³n de este trabajo se conservarÃ¡n y estarÃ¡n disponibles para su descarga despuÃ©s de el pipeline se haya ejecutado con Ã©xito.\n\n\n\n\n\nCada vez que realice un envÃ­o de cÃ³digo (push) o active manualmente el pipeline, GitLab ejecutarÃ¡ el trabajo de generaciÃ³n de PDF. El script generarÃ¡ el archivo PDF y lo almacenarÃ¡ como un artefacto.\n\n\n\n\nUna vez que el pipeline se haya ejecutado con Ã©xito, puede acceder a los archivos PDF generados en GitLab.\nVaya a la pÃ¡gina de su proyecto en GitLab, seleccione â€œCI/CDâ€ y luego â€œArtefactosâ€.\nAquÃ­ encontrarÃ¡ el archivo PDF generado que puede descargar.\n\n\n\n\nGitLab CI/CD es una herramienta poderosa que puede ayudar en la automatizaciÃ³n de una amplia variedad de tareas, incluida la generaciÃ³n de archivos PDF.\nAl comprender cÃ³mo utilizar artefactos en GitLab CI/CD y seguir los pasos mencionados anteriormente, puede integrar fÃ¡cilmente la generaciÃ³n de PDF en su flujo de trabajo de desarrollo, lo que ahorra tiempo y esfuerzo, y garantiza la consistencia y la calidad en la creaciÃ³n de documentos PDF automatizados."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#artefactos-en-gitlab-cicd",
    "href": "posts/2023/gitlab_pdf.html#artefactos-en-gitlab-cicd",
    "title": "Gitlab PDF",
    "section": "",
    "text": "Antes de profundizar en la generaciÃ³n de archivos PDF, es importante comprender el concepto de â€œartefactosâ€ en GitLab CI/CD. Los artefactos son archivos o conjuntos de archivos generados como resultado de una ejecuciÃ³n exitosa de un pipeline.\nEstos artefactos se almacenan en GitLab y se pueden utilizar posteriormente en otros trabajos o pipelines.\nEn el contexto de la generaciÃ³n de archivos PDF, los artefactos son esenciales porque permiten que los archivos PDF generados en un trabajo se conserven y utilicen en otros trabajos o etapas del pipeline."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#generaciÃ³n-de-archivos-pdf-con-gitlab-cicd",
    "href": "posts/2023/gitlab_pdf.html#generaciÃ³n-de-archivos-pdf-con-gitlab-cicd",
    "title": "Gitlab PDF",
    "section": "",
    "text": "La generaciÃ³n de archivos PDF como parte de su proceso de CI/CD puede ser Ãºtil en varios escenarios, como la creaciÃ³n de informes automatizados, la generaciÃ³n de documentaciÃ³n tÃ©cnica o la producciÃ³n de facturas en lÃ­nea.\nA continuaciÃ³n, detallaremos cÃ³mo lograrlo utilizando GitLab CI/CD:\n\nNota: Tomaremos como referencia el siguiente repositorio.\n\n\n\nAntes de comenzar, asegÃºrese de que su proyecto de GitLab estÃ© configurado correctamente y tenga acceso a GitLab CI/CD. TambiÃ©n debe tener un archivo de cÃ³digo fuente que desee convertir en un archivo PDF.\nAsegÃºrese de que cualquier dependencia necesaria estÃ© especificada en su archivo de configuraciÃ³n de CI/CD.\n\n\n\n\nCree un script que sea capaz de generar el archivo PDF a partir de sus datos de entrada.\nEste script deberÃ­a tomar los datos relevantes y formatearlos en un archivo PDF.\n\n\n\n\nEn su repositorio de GitLab, cree un archivo llamado .gitlab-ci.yml si aÃºn no lo ha hecho.\nEste archivo contiene la configuraciÃ³n de su pipeline.\nAquÃ­ hay un ejemplo de cÃ³mo podrÃ­a verse:\nstages:\n  - pdf\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\n  artifacts:\n    paths:\n      - ./*.pdf\nEn este ejemplo:\nstages:\n  - pdf\n\nstages: Esta secciÃ³n define las etapas (stages). En este caso, solo se define una etapa llamada â€œpdfâ€. Las etapas son divisiones lÃ³gicas en el pipeline que agrupan trabajos relacionados. En este caso, el pipeline tiene una sola etapa llamada â€œpdfâ€.\n\ngenerate_pdf:\n  stage: pdf\n  image: aergus/latex\n  script:\n    - latexmk -pdf **/*.tex\n\ngenerate_pdf: Esta secciÃ³n define un trabajo (job) llamado â€œgenerate_pdfâ€. Un trabajo es una unidad de ejecuciÃ³n en el pipeline. AquÃ­ estÃ¡ el desglose de esta secciÃ³n:\n\nstage: pdf: Esta lÃ­nea especifica que este trabajo pertenece a la etapa â€œpdfâ€ definida previamente. En otras palabras, este trabajo se ejecutarÃ¡ en la etapa â€œpdfâ€ del pipeline.\nimage: aergus/latex: Esta lÃ­nea especifica la imagen Docker que se utilizarÃ¡ para ejecutar este trabajo. En este caso, se utiliza la imagen â€œaergus/latexâ€, que contiene un entorno LaTeX para compilar documentos PDF. Esta imagen es esencial para compilar archivos LaTeX en archivos PDF.\nscript: AquÃ­ se definen los comandos que se ejecutarÃ¡n en el trabajo. En este caso, se utiliza el comando â€œlatexmk -pdf **/*.texâ€. Este comando utiliza â€œlatexmkâ€ para compilar todos los archivos â€œ.texâ€ en el proyecto en archivos PDF. El uso de **/*.tex significa que buscarÃ¡ archivos â€œ.texâ€ en todos los subdirectorios del proyecto.\n\n\nartifacts:\n  paths:\n    - ./*.pdf\n\nartifacts: Esta secciÃ³n especifica quÃ© archivos deben considerarse artefactos y, por lo tanto, se conservarÃ¡n despuÃ©s de una ejecuciÃ³n exitosa del trabajo. AquÃ­ estÃ¡ el desglose de esta secciÃ³n:\n\npaths: ./*.pdf: Esta lÃ­nea especifica que todos los archivos con extensiÃ³n â€œ.pdfâ€ en el directorio actual deben considerarse artefactos. Esto significa que los archivos PDF generados como resultado de la ejecuciÃ³n de este trabajo se conservarÃ¡n y estarÃ¡n disponibles para su descarga despuÃ©s de el pipeline se haya ejecutado con Ã©xito.\n\n\n\n\n\nCada vez que realice un envÃ­o de cÃ³digo (push) o active manualmente el pipeline, GitLab ejecutarÃ¡ el trabajo de generaciÃ³n de PDF. El script generarÃ¡ el archivo PDF y lo almacenarÃ¡ como un artefacto.\n\n\n\n\nUna vez que el pipeline se haya ejecutado con Ã©xito, puede acceder a los archivos PDF generados en GitLab.\nVaya a la pÃ¡gina de su proyecto en GitLab, seleccione â€œCI/CDâ€ y luego â€œArtefactosâ€.\nAquÃ­ encontrarÃ¡ el archivo PDF generado que puede descargar."
  },
  {
    "objectID": "posts/2023/gitlab_pdf.html#conclusiones",
    "href": "posts/2023/gitlab_pdf.html#conclusiones",
    "title": "Gitlab PDF",
    "section": "",
    "text": "GitLab CI/CD es una herramienta poderosa que puede ayudar en la automatizaciÃ³n de una amplia variedad de tareas, incluida la generaciÃ³n de archivos PDF.\nAl comprender cÃ³mo utilizar artefactos en GitLab CI/CD y seguir los pasos mencionados anteriormente, puede integrar fÃ¡cilmente la generaciÃ³n de PDF en su flujo de trabajo de desarrollo, lo que ahorra tiempo y esfuerzo, y garantiza la consistencia y la calidad en la creaciÃ³n de documentos PDF automatizados."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html",
    "href": "posts/2021/2021-08-11-jb.html",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter Book es un proyecto de cÃ³digo abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.\nAlgunas caracterÃ­sticas importantes del uso de Jupyter Book:\n\ncontenido con calidad de publicaciÃ³n que incluya figuras, sÃ­mbolos matemÃ¡ticos, citas y referencias cruzadas!\nescribir contenido como Jupyter Notebooks, markdown o reStructuredText\nAgregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en lÃ­nea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).\ngenerar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.\nuna interfaz de lÃ­nea de comandos para crear libros rÃ¡pidamente, por ejemplo, jupyter-book build mybook\n\nEn esta sesiÃ³n, se muestra un ejemplo de cÃ³mo crear un Jupyter Book desde cero y algunas de las caracterÃ­sticas clave que ofrece Jupyter Book.\n\nNota: Puede encontrar los cÃ³digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaciÃ³n con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar Jupyter Book, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge jupyter-book\nDe lo contrario, puede instalar con pip:\npip install jupyter-book\n\n\n\nJupyter Book viene con una herramienta que le permite crear y construir libros rÃ¡pidamente. Para crear el esqueleto del libro, escriba lo siguiente en la lÃ­nea de comando:\njupyter-book create jupiter\n\nNota: AquÃ­ llamamos al libro jupiter, pero puedes elegir llamar a tu libro como quieras.\n\nAhora tendrÃ¡s un nuevo directorio llamado jupiter (o como quieras llamar a tu libro), con el siguiente contenido:\njupiter\n  â”œâ”€â”€ _config.yml\n  â”œâ”€â”€ _toc.yml\n  â”œâ”€â”€ content.md\n  â”œâ”€â”€ intro.md\n  â”œâ”€â”€ markdown.md\n  â”œâ”€â”€ notebooks.ipynb\n  â”œâ”€â”€ references.bib\n  â””â”€â”€ requirements.txt\nen donde: * _config.yml: archivo que contiene las configuraciones del proyecto. * _toc.yml: archivo que ordena los capÃ­tulos del libro. * content.md: archivo genÃ©rico .md. * intro.md: archivo genÃ©rico .md. * markdown.md: archivo genÃ©rico .md. * notebooks.ipynb: archivo genÃ©rico .ipynb. * references.bib: archivo para aÃ±adir las referencias. * requirements.txt: archivo que contiene las dependencias python) del proyecto.\n\n\n\nJupyter Book admite varios tipos de archivos:\n\nMarkdown (.md)\nnotebooks (.ipynb)\netc.\n\nComo Markdown y Jupyter Notebooks probablemente serÃ¡n los tipos de archivo mÃ¡s comunes que usarÃ¡, se mostrarÃ¡ un ejemplo de ello.\nLo primero serÃ¡ eliminar los archivos de inicio en el directorio:\n\ncontent.md\nintro.md\nmarkdown.md\nnotebooks.ipynb\n\nAsÃ­ que ejecutamos por lÃ­nea de comando:\nrm content.md intro.md markdown.md notebooks.ipynb\nPor otro lado, nuestro proyecto estarÃ© conformado por tres archivos:\n\nindex.md\nIntroduction.md\ngreat_red_spot.ipynb\n\nLuego, debemos indicar cÃ³mo serÃ¡n mostrados estos documentos en el archivo _toc.yml. La estructura serÃ¡ la siguiente:\nformat: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\nEn este caso, root: index corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo index.md escribiremos:\n# Home\n\njupyter book example\n\n## Contenidos\n\n\n```roknnpsbfrmktjfhd\n```\n\n\n\n\n\n\nSe comienza por agregar un archivo de markdown. Con algÃºn editor a elecciÃ³n (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado Introduction.md.\nSe usa este archivo como demostraciÃ³n de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.\n\n\nSe agrega un texto de Markdown simple a nuestro archivo. Si no estÃ¡ familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo Introduction.md.\n# Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n\n\n\nPuedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:\n```gkwoiqvj my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n\nSi bien la imagen puede estar contenida y referenciada desde el directorio raÃ­z, tambiÃ©n se puede incluir imÃ¡genes a travÃ©s de URL. Incluyamos una imagen del planeta JÃºpiter en nuestro archivo Introduction.md usando lo siguiente:\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n\nLa razÃ³n por la que le damos un â€œnombreâ€ a nuestra imagen es para que podamos hacer referencia a ella fÃ¡cilmente con la sintaxis:\n{numref}`jupiter-figure`\nSe agregarÃ¡ una oraciÃ³n que incluya esta referencia. El archivo completo ahora deberÃ­a verse asÃ­:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\nEn este punto, probablemente se deberÃ­a crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo _toc.yml. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay allÃ­. Luego, simplemente agregue lo siguiente:\n- file: introduction\nAhora podemos construir nuestro libro desde la lÃ­nea de comandos asegurÃ¡ndonos de que estamos en el directorio raÃ­z de nuestro libro y luego usando:\njupyter-book build .\nUna vez finalizada la compilaciÃ³n, tendrÃ¡ un nuevo subdirectorio llamado_build/html/ en la raÃ­z de su libro, navegue hasta esa ubicaciÃ³n y abra _build/html/index.html. DeberÃ­a verse algo como esto:\n\n\n\n\nJupyter Book usa MathJax para componer matemÃ¡ticas, lo que le permite agregar matemÃ¡ticas de estilo LaTeX a su libro. Puede agregar matemÃ¡ticas en lÃ­nea, bloques matemÃ¡ticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matemÃ¡ticas.\nLas matemÃ¡ticas en lÃ­nea se pueden definir usando $ de la siguiente manera:\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n\nJupiter has a mass of: \\(m_{j} \\approx 1.9 \\times 10^{27} kg\\)\nLos bloques matemÃ¡ticos se pueden definir usando la notaciÃ³n $$:\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n\n\\[m_{j} \\approx 1.9 \\times 10^{27} kg\\]\n\nNota: Si lo prefiere, los bloques matemÃ¡ticos tambiÃ©n se pueden definir con \\begin{equation} en lugar de $$.\n\nLas ecuaciones numeradas se pueden definir asÃ­ (este es el estilo que te recomiendo que uses con mÃ¡s frecuencia):\n```lbgrcp\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nAgreguemos mÃ¡s contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo Introduction.md:\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nA continuaciÃ³n, puede reconstruir su libro (jupyter-book build .) y abrir _build/html/index.html para asegurarse de que todo se estÃ© procesando como se esperaba.\n\n\n\n\nHay varias formas diferentes de controlar el diseÃ±o de las pÃ¡ginas de su Jupyter Book. El cambio de diseÃ±o que utilizo con mÃ¡s frecuencia es agregar contenido a un margen en la pÃ¡gina. Puede agregar un margen usando la siguiente directiva:\n```txbrpgfy An optional title\nSome margin content.\n```\n\nAgreguemos algo de contenido marginal al libro:\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n\n\nHay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aquÃ­ en la documentaciÃ³n de Jupyter Book. Las advertencias se crean con la sintaxis:\n```uzejlz\nI am a useful note!\n```\n\n\nNo dude en agregar la siguiente advertencia a Introduction.md:\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\n\n\n\nEl Ãºltimo contenido corresponde a referencias y una bibliografÃ­a. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex Reference.bib que se encuentra en el directorio raÃ­z de su libro.\nPara incluir una cita en su libro, agregue una entrada bibtex a references.bib, por ejemplo:\n@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n\nNota: Consulte la documentaciÃ³n de BibTex para obtener informaciÃ³n sobre el estilo de referencia de BibTex. Google Scholar facilita la exportaciÃ³n de un formato de cita bibtex.\n\nA continuaciÃ³n, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:\n{cite}`mayor1995jupiter`\nO para mÃºltiples citas:\n{cite}`mayor1995jupiter,guillot1999interiors`\nLuego puede crear una bibliografÃ­a a partir de reference.bib usando:\n```vrvfdaunxkbvhs references.bib\n```\n\nPor ejemplo, intente agregar esto a su archivo Introduction.md:\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nSu archivo final Introduction.md deberÃ­a verse asÃ­:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nY deberÃ­a renderizarse asÃ­: \n\n\n\nTodos los flujos de trabajo de formato y estilo que vimos en markdown tambiÃ©n se aplican a un Jupyter Notebook; simplemente agrÃ©guelos a una celda de markdown y listo.\nComencemos con lo siguiente:\n\nCree un nuevo notebook llamado great_red_spot.ipynb;\nAgregue este archivo a su _toc.yml;\nAgregue una celda de markdown con el siguiente contenido:\n\n# The Great Red Spot\n\nJupiterâ€™s iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n\nÂ¡Ahora intente construir su libro (jupyter-book build .) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva pÃ¡gina â€œThe Great Red Spotâ€, que deberÃ­a verse asÃ­:\n\nÂ¡Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos grÃ¡ficos.\nCree una nueva celda de cÃ³digo debajo de la celda de rebaja actual y agregue el siguiente cÃ³digo para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n\nNota: Estamos imprimiendo la salida en la pantalla con el uso de df.head() y esto se mostrarÃ¡ en nuestro Jupyter Book renderizado.\n\nSi reconstruye su libro (jupyter-book build .) en este punto, verÃ¡ algo como lo siguiente:\n\nAhora, podemos usar estos datos para crear algunos grÃ¡ficos.\nLas tramas en su Jupyter Book pueden ser estÃ¡ticas (por ejemplo, matplotlib, seaborn) o interactivas (por ejemplo, altair, plotly, bokeh). Para este tutorial, crearemos algunos grÃ¡ficos de ejemplo usando Plotly (a travÃ©s del backend de Pandas).\nPrimero creemos un diagrama de dispersiÃ³n simple de nuestros datos. Cree una nueva celda de cÃ³digo en su cuaderno y agregue el siguiente cÃ³digo:\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\nYa que estamos en eso, creemos tambiÃ©n una trama animada. Cree otra celda de cÃ³digo nueva y agregue el siguiente cÃ³digo:\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n\nNota: Plotly tiene diferentes renderizadores disponibles para generar grÃ¡ficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que pio.renderers.default = \"notebook\" funciona con la versiÃ³n actual de Jupyter Book.\n\nÂ¡Ahora, reconstruyamos nuestro libro y echemos un vistazo!\n\nEs posible que desee ocultar parte del cÃ³digo en su libro, Â¡no hay problema! Eso tambiÃ©n se hace fÃ¡cilmente con Jupyter Book.\nEl que nos interesa aquÃ­ es ocultar la entrada de cÃ³digo. Podemos hacerlo fÃ¡cilmente agregando la etiqueta hide-input a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuaciÃ³n:\n\nContinÃºe y agregue las etiquetas hide-input a ambas celdas de trazado en su archivo great_red_spot.ipynb. Cuando reconstruyas el libro, verÃ¡s que la entrada del cÃ³digo estÃ¡ oculta (pero se puede alternar con el Ã­cono +):\n\n\nNota: TambiÃ©n puede almacenar el contenido de la libreta como valores, grÃ¡ficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta glue.\n\n\n\n\n\n\nJupyter-Book - Documentation\nTutorial - Jupyter Book"
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#introducciÃ³n",
    "href": "posts/2021/2021-08-11-jb.html#introducciÃ³n",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter Book es un proyecto de cÃ³digo abierto para crear libros y documentos mediante Jupyter Notebooks y/o Markdown.\nAlgunas caracterÃ­sticas importantes del uso de Jupyter Book:\n\ncontenido con calidad de publicaciÃ³n que incluya figuras, sÃ­mbolos matemÃ¡ticos, citas y referencias cruzadas!\nescribir contenido como Jupyter Notebooks, markdown o reStructuredText\nAgregue interactividad a su libro, por ejemplo, alternar la visibilidad de las celdas, conectarse con un servicio en lÃ­nea como Binder e incluir resultados interactivos (por ejemplo, figuras y widgets).\ngenerar una variedad de resultados, incluidos sitios web (HTML, CSS, JS), markdown y PDF.\nuna interfaz de lÃ­nea de comandos para crear libros rÃ¡pidamente, por ejemplo, jupyter-book build mybook\n\nEn esta sesiÃ³n, se muestra un ejemplo de cÃ³mo crear un Jupyter Book desde cero y algunas de las caracterÃ­sticas clave que ofrece Jupyter Book.\n\nNota: Puede encontrar los cÃ³digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaciÃ³n con GitLab CI/CD."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#primeros-pasos",
    "href": "posts/2021/2021-08-11-jb.html#primeros-pasos",
    "title": "Jupyter Book",
    "section": "",
    "text": "Para instalar Jupyter Book, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge jupyter-book\nDe lo contrario, puede instalar con pip:\npip install jupyter-book\n\n\n\nJupyter Book viene con una herramienta que le permite crear y construir libros rÃ¡pidamente. Para crear el esqueleto del libro, escriba lo siguiente en la lÃ­nea de comando:\njupyter-book create jupiter\n\nNota: AquÃ­ llamamos al libro jupiter, pero puedes elegir llamar a tu libro como quieras.\n\nAhora tendrÃ¡s un nuevo directorio llamado jupiter (o como quieras llamar a tu libro), con el siguiente contenido:\njupiter\n  â”œâ”€â”€ _config.yml\n  â”œâ”€â”€ _toc.yml\n  â”œâ”€â”€ content.md\n  â”œâ”€â”€ intro.md\n  â”œâ”€â”€ markdown.md\n  â”œâ”€â”€ notebooks.ipynb\n  â”œâ”€â”€ references.bib\n  â””â”€â”€ requirements.txt\nen donde: * _config.yml: archivo que contiene las configuraciones del proyecto. * _toc.yml: archivo que ordena los capÃ­tulos del libro. * content.md: archivo genÃ©rico .md. * intro.md: archivo genÃ©rico .md. * markdown.md: archivo genÃ©rico .md. * notebooks.ipynb: archivo genÃ©rico .ipynb. * references.bib: archivo para aÃ±adir las referencias. * requirements.txt: archivo que contiene las dependencias python) del proyecto.\n\n\n\nJupyter Book admite varios tipos de archivos:\n\nMarkdown (.md)\nnotebooks (.ipynb)\netc.\n\nComo Markdown y Jupyter Notebooks probablemente serÃ¡n los tipos de archivo mÃ¡s comunes que usarÃ¡, se mostrarÃ¡ un ejemplo de ello.\nLo primero serÃ¡ eliminar los archivos de inicio en el directorio:\n\ncontent.md\nintro.md\nmarkdown.md\nnotebooks.ipynb\n\nAsÃ­ que ejecutamos por lÃ­nea de comando:\nrm content.md intro.md markdown.md notebooks.ipynb\nPor otro lado, nuestro proyecto estarÃ© conformado por tres archivos:\n\nindex.md\nIntroduction.md\ngreat_red_spot.ipynb\n\nLuego, debemos indicar cÃ³mo serÃ¡n mostrados estos documentos en el archivo _toc.yml. La estructura serÃ¡ la siguiente:\nformat: jb-book\nroot: index\nchapters:\n- file: Introduction\n- file: great_red_spot\nEn este caso, root: index corresponde al primer archivo que se visualiza en el jupyter-book. Dentro del archivo index.md escribiremos:\n# Home\n\njupyter book example\n\n## Contenidos\n\n\n```roknnpsbfrmktjfhd\n```\n\n\n\n\n\n\nSe comienza por agregar un archivo de markdown. Con algÃºn editor a elecciÃ³n (por ejemplo, jupyter notebook o jupyterlab) se crea un nuevo archivo markdown llamado Introduction.md.\nSe usa este archivo como demostraciÃ³n de algunos de los principales tipos de contenido que puede agregar en Jupyter-Book.\n\n\nSe agrega un texto de Markdown simple a nuestro archivo. Si no estÃ¡ familiarizado con la sintaxis de markdown, consulte Markdown Cheat Sheet. Puede copiar y pegar el siguiente contenido directamente en su archivo Introduction.md.\n# Jupiter Book\n\nThis book contains information about the planet **Jupiter** - the fifth planet from the sun and the largest planet in the solar system!\n\n\n\nPuedes incluir figuras en tu Jupyter Book usando la siguiente sintaxis:\n```gkwoiqvj my_image.png\n---\nheight: 150px\nname: my-image\n---\nHere is my image's caption!\n```\n\nSi bien la imagen puede estar contenida y referenciada desde el directorio raÃ­z, tambiÃ©n se puede incluir imÃ¡genes a travÃ©s de URL. Incluyamos una imagen del planeta JÃºpiter en nuestro archivo Introduction.md usando lo siguiente:\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter!\n```\n\nLa razÃ³n por la que le damos un â€œnombreâ€ a nuestra imagen es para que podamos hacer referencia a ella fÃ¡cilmente con la sintaxis:\n{numref}`jupiter-figure`\nSe agregarÃ¡ una oraciÃ³n que incluya esta referencia. El archivo completo ahora deberÃ­a verse asÃ­:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\nEn este punto, probablemente se deberÃ­a crear nuetro libro para asegurarnos de que tenga el aspecto esperado. Para hacer eso, primero necesitamos modificar nuestro archivo _toc.yml. Este archivo contiene la tabla de contenido de nuestro libro. Abra ese archivo ahora y elimine todo lo que hay allÃ­. Luego, simplemente agregue lo siguiente:\n- file: introduction\nAhora podemos construir nuestro libro desde la lÃ­nea de comandos asegurÃ¡ndonos de que estamos en el directorio raÃ­z de nuestro libro y luego usando:\njupyter-book build .\nUna vez finalizada la compilaciÃ³n, tendrÃ¡ un nuevo subdirectorio llamado_build/html/ en la raÃ­z de su libro, navegue hasta esa ubicaciÃ³n y abra _build/html/index.html. DeberÃ­a verse algo como esto:\n\n\n\n\nJupyter Book usa MathJax para componer matemÃ¡ticas, lo que le permite agregar matemÃ¡ticas de estilo LaTeX a su libro. Puede agregar matemÃ¡ticas en lÃ­nea, bloques matemÃ¡ticos y ecuaciones numeradas a su libro Jupyter. Sigamos adelante y creemos un nuevo encabezado en nuestro archivo Introduction.md que incluye algunas matemÃ¡ticas.\nLas matemÃ¡ticas en lÃ­nea se pueden definir usando $ de la siguiente manera:\nJupiter has a mass of:  $m_{j} \\approx 1.9 \\times 10^{27} kg$\n\nJupiter has a mass of: \\(m_{j} \\approx 1.9 \\times 10^{27} kg\\)\nLos bloques matemÃ¡ticos se pueden definir usando la notaciÃ³n $$:\n$$m_{j} \\approx 1.9 \\times 10^{27} kg$$\n\n\\[m_{j} \\approx 1.9 \\times 10^{27} kg\\]\n\nNota: Si lo prefiere, los bloques matemÃ¡ticos tambiÃ©n se pueden definir con \\begin{equation} en lugar de $$.\n\nLas ecuaciones numeradas se pueden definir asÃ­ (este es el estilo que te recomiendo que uses con mÃ¡s frecuencia):\n```lbgrcp\n:label: my_label\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nAgreguemos mÃ¡s contenido a nuestro libro. Copie y agregue el siguiente texto a su archivo Introduction.md:\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\nA continuaciÃ³n, puede reconstruir su libro (jupyter-book build .) y abrir _build/html/index.html para asegurarse de que todo se estÃ© procesando como se esperaba.\n\n\n\n\nHay varias formas diferentes de controlar el diseÃ±o de las pÃ¡ginas de su Jupyter Book. El cambio de diseÃ±o que utilizo con mÃ¡s frecuencia es agregar contenido a un margen en la pÃ¡gina. Puede agregar un margen usando la siguiente directiva:\n```txbrpgfy An optional title\nSome margin content.\n```\n\nAgreguemos algo de contenido marginal al libro:\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n\n\nHay todo tipo de advertencias diferentes que puede usar en Jupyter Book que se enumeran aquÃ­ en la documentaciÃ³n de Jupyter Book. Las advertencias se crean con la sintaxis:\n```uzejlz\nI am a useful note!\n```\n\n\nNo dude en agregar la siguiente advertencia a Introduction.md:\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\n\n\n\nEl Ãºltimo contenido corresponde a referencias y una bibliografÃ­a. Puede agregar citas de cualquier trabajo almacenado en el archivo Bibtex Reference.bib que se encuentra en el directorio raÃ­z de su libro.\nPara incluir una cita en su libro, agregue una entrada bibtex a references.bib, por ejemplo:\n@article{mayor1995jupiter,\n    title={A Jupiter-mass companion to a solar-type star},\n    author={Mayor, Michel and Queloz, Didier},\n    journal={Nature},\n    volume={378},\n    number={6555},\n    pages={355--359},\n    year={1995},\n    publisher={Nature Publishing Group}\n}\n\n@article{guillot1999interiors,\n    title={Interiors of giant planets inside and outside the solar system},\n    author={Guillot, Tristan},\n    journal={Science},\n    volume={286},\n    number={5437},\n    pages={72--77},\n    year={1999},\n    publisher={American Association for the Advancement of Science}\n}\n\nNota: Consulte la documentaciÃ³n de BibTex para obtener informaciÃ³n sobre el estilo de referencia de BibTex. Google Scholar facilita la exportaciÃ³n de un formato de cita bibtex.\n\nA continuaciÃ³n, puede hacer referencia al trabajo en su libro utilizando la siguiente directiva:\n{cite}`mayor1995jupiter`\nO para mÃºltiples citas:\n{cite}`mayor1995jupiter,guillot1999interiors`\nLuego puede crear una bibliografÃ­a a partir de reference.bib usando:\n```vrvfdaunxkbvhs references.bib\n```\n\nPor ejemplo, intente agregar esto a su archivo Introduction.md:\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nSu archivo final Introduction.md deberÃ­a verse asÃ­:\n# Jupiter Book\n\nThis book contains information about the planet Jupiter - the fifth planet from the sun and the largest planet in the solar system! {numref}`jupiter-figure` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/2486_stsci-h-p1936a_1800.jpg\n---\nheight: 300px\nname: jupiter-figure\n---\nThe beautiful planet Jupiter! Source: [NASA](https://solarsystem.nasa.gov/resources/2486/hubbles-new-portrait-of-jupiter/?category=planets_jupiter).\n```\n\n## The Mass of Jupiter\n\nWe can estimate the mass of Jupiter from the period and size of an object orbiting it. For example, we can use Jupiter's moon Callisto to estimate it's mass.\n\nCallisto's period: $p_{c}=16.7 days$\n\nCallisto's orbit radius: $r_{c}=1,900,000 km$\n\nNow, using [Kepler's Law](https://solarsystem.nasa.gov/resources/310/orbits-and-keplers-laws/) we can work out the mass of Jupiter.\n\n\n```lbgrcp\n:label: eq1\nm_{j} \\approx \\frac{r_{c}}{p_{c}} \\times 7.9 \\times 10^{10}\n```\n\n```lbgrcp\n:label: eq2\nm_{j} \\approx 1.9 \\times 10^{27} kg\n```\n\n\n```txbrpgfy Did you know?\nJupiter is 11.0x larger than Earth!\n```\n\n\n```lgcccw\nNASA provides a lot more information about the physical characteristics of Jupiter [here](https://solarsystem.nasa.gov/planets/jupiter/by-the-numbers/).\n```\n\n\nThere might even be more planets out there with a similar mass to Jupiter {cite}`mayor1995jupiter,guillot1999interiors`!\n\n## Bibliography\n\n```vrvfdaunxkbvhs references.bib\n```\n\nY deberÃ­a renderizarse asÃ­: \n\n\n\nTodos los flujos de trabajo de formato y estilo que vimos en markdown tambiÃ©n se aplican a un Jupyter Notebook; simplemente agrÃ©guelos a una celda de markdown y listo.\nComencemos con lo siguiente:\n\nCree un nuevo notebook llamado great_red_spot.ipynb;\nAgregue este archivo a su _toc.yml;\nAgregue una celda de markdown con el siguiente contenido:\n\n# The Great Red Spot\n\nJupiterâ€™s iconic Great Red Spot (GRS) is actually an enormous storm that is bigger than Earth that has raged for hundreds of years! {numref}`great-red-spot` below shows an image of Jupiter captured by the Hubble Space Telescope on June 27, 2019.\n\n```gkwoiqvj https://solarsystem.nasa.gov/system/resources/detail_files/626_PIA21775.jpg\n---\nheight: 300px\nname: great-red-spot\n---\nJupiter's Great Red Spot! Source: [NASA](https://solarsystem.nasa.gov/resources/626/jupiters-great-red-spot-in-true-color/?category=planets_jupiter).\n```\n\nJupiter's GRS has been observed to be shrinking for about the last century and a half! [Here](https://github.com/UBC-DSCI/jupyterdays/tree/master/jupyterdays/sessions/beuzen/data) is some data of the length of the GRS spanning the last ~150 years which we can use to investigate this phenomenon.\n\nÂ¡Ahora intente construir su libro (jupyter-book build .) para asegurarse de que todo se vea bien! Usando la barra de contenido del lado izquierdo, navegue a la nueva pÃ¡gina â€œThe Great Red Spotâ€, que deberÃ­a verse asÃ­:\n\nÂ¡Ok genial! Ahora importemos los datos a los que hicimos referencia para que podamos crear algunos grÃ¡ficos.\nCree una nueva celda de cÃ³digo debajo de la celda de rebaja actual y agregue el siguiente cÃ³digo para leer en nuestro conjunto de datos de GRS como un marco de datos de Pandas.\nimport pandas as pd\npd.options.plotting.backend = \"plotly\"\n\nurl = \"https://raw.githubusercontent.com/UBC-DSCI/jupyterdays/master/jupyterdays/sessions/beuzen/data/GRS_data.csv\"\ndf = pd.read_csv(url)\ndf['Year'] = df['Year'].astype(int) \ndf.head()\n\nNota: Estamos imprimiendo la salida en la pantalla con el uso de df.head() y esto se mostrarÃ¡ en nuestro Jupyter Book renderizado.\n\nSi reconstruye su libro (jupyter-book build .) en este punto, verÃ¡ algo como lo siguiente:\n\nAhora, podemos usar estos datos para crear algunos grÃ¡ficos.\nLas tramas en su Jupyter Book pueden ser estÃ¡ticas (por ejemplo, matplotlib, seaborn) o interactivas (por ejemplo, altair, plotly, bokeh). Para este tutorial, crearemos algunos grÃ¡ficos de ejemplo usando Plotly (a travÃ©s del backend de Pandas).\nPrimero creemos un diagrama de dispersiÃ³n simple de nuestros datos. Cree una nueva celda de cÃ³digo en su cuaderno y agregue el siguiente cÃ³digo:\nimport plotly.io as pio\npio.renderers.default = \"notebook\"\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\", color=\"Recorder\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=650, height=400)\nfig.update_layout(title={'text': \"Great Red Spot Size\", 'x':0.5, 'y':0.92})\nfig.update_traces(marker=dict(size=7))\nYa que estamos en eso, creemos tambiÃ©n una trama animada. Cree otra celda de cÃ³digo nueva y agregue el siguiente cÃ³digo:\nfig = df.plot.scatter(x=\"Year\", y=\"GRS Length\",\n                      animation_frame=\"Year\",\n                      range_x=[1870, 2030], range_y=[10, 40],\n                      width=600, height=520)\nfig.update_layout(title={'text': \"Great Red Spot Size Animation\", 'x':0.5, 'y':0.94})\nfig.layout.updatemenus[0].buttons[0].args[1][\"frame\"][\"duration\"] = 200\nfig.update_traces(marker=dict(size=10))\n\nNota: Plotly tiene diferentes renderizadores disponibles para generar grÃ¡ficos. Es posible que deba experimentar con renderizadores para obtener el resultado que desea en su Jupyter Book. He descubierto que pio.renderers.default = \"notebook\" funciona con la versiÃ³n actual de Jupyter Book.\n\nÂ¡Ahora, reconstruyamos nuestro libro y echemos un vistazo!\n\nEs posible que desee ocultar parte del cÃ³digo en su libro, Â¡no hay problema! Eso tambiÃ©n se hace fÃ¡cilmente con Jupyter Book.\nEl que nos interesa aquÃ­ es ocultar la entrada de cÃ³digo. Podemos hacerlo fÃ¡cilmente agregando la etiqueta hide-input a la celda que deseamos ocultar. Hay varias formas de agregar etiquetas a la celda en Jupyter Notebooks. En Jupyter Lab, haga clic en el icono de engranaje en la barra lateral izquierda y luego agregue la etiqueta deseada como se muestra a continuaciÃ³n:\n\nContinÃºe y agregue las etiquetas hide-input a ambas celdas de trazado en su archivo great_red_spot.ipynb. Cuando reconstruyas el libro, verÃ¡s que la entrada del cÃ³digo estÃ¡ oculta (pero se puede alternar con el Ã­cono +):\n\n\nNota: TambiÃ©n puede almacenar el contenido de la libreta como valores, grÃ¡ficos o marcos de datos en variables que se pueden utilizar en toda su libreta mediante la herramienta glue."
  },
  {
    "objectID": "posts/2021/2021-08-11-jb.html#referencias",
    "href": "posts/2021/2021-08-11-jb.html#referencias",
    "title": "Jupyter Book",
    "section": "",
    "text": "Jupyter-Book - Documentation\nTutorial - Jupyter Book"
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html",
    "href": "posts/2021/2021-08-05-rise.html",
    "title": "RISE",
    "section": "",
    "text": "RISE es una extensiÃ³n a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.\nToda las celdas pueden editarse y ejecutarse directamente, durante la presentaciÃ³n. Esto es prÃ¡ctico si necesitas corregir un error en una celda de texto. MÃ¡s importante aÃºn, puedes ejecutar cÃ³digo directamente en el kernel. En una misma diapositiva puedes tener mÃºltiples celdas y elegir cuÃ¡l ejecutar, o corregir el texto y volver a ejecutar.\n\nAlgunas caracterÃ­sticas importantes del uso de RISE:\n\nSimplifica la generaciÃ³n de material.\nSe mantiene un archivo y no varios archivos para hablar de lo mismo.\nEs fÃ¡cil de corregir, no se necesita mucho esfuerzo (similar a una PPT).\n\nEn esta sesiÃ³n, se muestra un ejemplo de cÃ³mo crear una presentaciÃ³n con RISE.\n\nNota: Puede encontrar los cÃ³digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaciÃ³n con GitLab CI/CD.\n\n\n\n\n\n\nPara instalar RISE, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge rise\nDe lo contrario, puede instalar con pip:\npip install RISE\n\nNota: No interactuarÃ¡s directamente con RISE. En su lugar, podrÃ¡ acceder a Ã©l a travÃ©s de Jupyter Notebooks.\n\n\n\n\n\nPara crear una presentaciÃ³n, deberÃ¡ iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto despuÃ©s de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deberÃ¡ habilitar la presentaciÃ³n de diapositivas. Puede hacer esto haciendo lo siguiente:\n\nHaga clic en â€œVerâ€ en la barra de herramientas de Jupyter\nColoca el cursor sobre â€œBarra de herramientas de celdaâ€ en el menÃº â€œVerâ€\nHaga clic en â€œPresentaciÃ³n de diapositivasâ€ en el menÃº â€œBarra de herramientas de celdaâ€\n\n\n\n\nEn este punto, deberÃ­a tener una barra de herramientas de celda con un menÃº desplegable en el lado derecho: \nDeberÃ­a ver seis opciones aquÃ­. Este menÃº desplegable y sus opciones determinan cÃ³mo encaja cada celda en la presentaciÃ³n. Las opciones y sus descripciones se encuentran a continuaciÃ³n:\n\nslide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.\nsub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.\nfragment: indica que la celda seleccionada debe aparecer como una compilaciÃ³n de la diapositiva anterior.\nskip: indica que la celda seleccionada debe omitirse y no ser parte de la presentaciÃ³n de diapositivas.\nnotes: indica que la celda seleccionada debe ser solo notas del presentador.\n- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es Ãºtil cuando una celda de rebaja y una celda de cÃ³digo deben aparecer simultÃ¡neamente.\n\nCada una de estas opciones puede incluir cÃ³digo Python o cÃ³digo Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.\n\n\n\nUna vez que se han utilizado las celdas para crear material para la presentaciÃ³n, la presentaciÃ³n se puede ver directamente desde el notebook.\nHay dos opciones para ver la presentaciÃ³n de diapositivas:\n\nUsar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentaciÃ³n desde dentro de la computadora portÃ¡til\nAl hacer clic en el botÃ³n â€œModo de presentaciÃ³nâ€ de la computadora portÃ¡til, esto solo aparecerÃ¡ si ha instalado RISE.\n\n\nDespuÃ©s de ingresar al modo de presentaciÃ³n, deberÃ­a ver una pantalla similar a esta:\n\n\n\n\nSi bien puede ser tentador usar las teclas &lt;- y -&gt; para cambiar las diapositivas en la presentaciÃ³n, esto no funcionarÃ¡ por completo: omitirÃ¡ las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentaciÃ³n de diapositivas hacia adelante y MAYÃšS + ESPACIO para mover la presentaciÃ³n de diapositivas hacia atrÃ¡s.\nHay muchos otros atajos de teclado a los que se puede acceder dentro de la presentaciÃ³n haciendo clic en el signo de interrogaciÃ³n (?) en la esquina inferior izquierda.\n\n\n\nUna de las mejores cosas de RISE es que funciona en una sesiÃ³n de Python en vivo, lo que significa que puede editar y ejecutar cÃ³digo mientras se ejecuta la presentaciÃ³n.\n\n\n\n\nPuedes exportar tu presentaciÃ³n desplegando la opciÃ³n: File -&gt; Download as.\n\nNota: Para poder descargar en formato .pdf, necesita tener instalado pandoc.\n\n\n\n\n\n\nRISE - Documentation\nCreating Interactive Slideshows in Jupyter Notebooks"
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#introducciÃ³n",
    "href": "posts/2021/2021-08-05-rise.html#introducciÃ³n",
    "title": "RISE",
    "section": "",
    "text": "RISE es una extensiÃ³n a los jupyter notebooks que permite transformar tus notebooks en presentaciones interactivas.\nToda las celdas pueden editarse y ejecutarse directamente, durante la presentaciÃ³n. Esto es prÃ¡ctico si necesitas corregir un error en una celda de texto. MÃ¡s importante aÃºn, puedes ejecutar cÃ³digo directamente en el kernel. En una misma diapositiva puedes tener mÃºltiples celdas y elegir cuÃ¡l ejecutar, o corregir el texto y volver a ejecutar.\n\nAlgunas caracterÃ­sticas importantes del uso de RISE:\n\nSimplifica la generaciÃ³n de material.\nSe mantiene un archivo y no varios archivos para hablar de lo mismo.\nEs fÃ¡cil de corregir, no se necesita mucho esfuerzo (similar a una PPT).\n\nEn esta sesiÃ³n, se muestra un ejemplo de cÃ³mo crear una presentaciÃ³n con RISE.\n\nNota: Puede encontrar los cÃ³digos de este ejemplo en el siguiente repositorio. Por otro lado, puede revisar el siguente link para ver la compilaciÃ³n con GitLab CI/CD."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#primeros-pasos",
    "href": "posts/2021/2021-08-05-rise.html#primeros-pasos",
    "title": "RISE",
    "section": "",
    "text": "Para instalar RISE, necesitarÃ¡ usar la lÃ­nea de comando. Si ha instalado Anaconda, puede usar:\nconda install -c conda-forge rise\nDe lo contrario, puede instalar con pip:\npip install RISE\n\nNota: No interactuarÃ¡s directamente con RISE. En su lugar, podrÃ¡ acceder a Ã©l a travÃ©s de Jupyter Notebooks."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#habilitaciÃ³n-del-modo-de-presentaciÃ³n",
    "href": "posts/2021/2021-08-05-rise.html#habilitaciÃ³n-del-modo-de-presentaciÃ³n",
    "title": "RISE",
    "section": "",
    "text": "Para crear una presentaciÃ³n, deberÃ¡ iniciar Jupyter Notebooks y abrir un nuevo notebook (tenga en cuenta que debe hacer esto despuÃ©s de haber instalado RISE). Una vez que tenga un Jupyter Notebook nuevo, deberÃ¡ habilitar la presentaciÃ³n de diapositivas. Puede hacer esto haciendo lo siguiente:\n\nHaga clic en â€œVerâ€ en la barra de herramientas de Jupyter\nColoca el cursor sobre â€œBarra de herramientas de celdaâ€ en el menÃº â€œVerâ€\nHaga clic en â€œPresentaciÃ³n de diapositivasâ€ en el menÃº â€œBarra de herramientas de celdaâ€\n\n\n\n\nEn este punto, deberÃ­a tener una barra de herramientas de celda con un menÃº desplegable en el lado derecho: \nDeberÃ­a ver seis opciones aquÃ­. Este menÃº desplegable y sus opciones determinan cÃ³mo encaja cada celda en la presentaciÃ³n. Las opciones y sus descripciones se encuentran a continuaciÃ³n:\n\nslide: indica que la celda seleccionada debe ser el comienzo de una nueva diapositiva.\nsub-slide -: indica que la celda seleccionada debe ser el comienzo de una nueva sub-diapositiva, que aparece en un nuevo marco debajo de la diapositiva anterior.\nfragment: indica que la celda seleccionada debe aparecer como una compilaciÃ³n de la diapositiva anterior.\nskip: indica que la celda seleccionada debe omitirse y no ser parte de la presentaciÃ³n de diapositivas.\nnotes: indica que la celda seleccionada debe ser solo notas del presentador.\n- -: indica que la celda seleccionada debe seguir el comportamiento de la celda anterior, lo cual es Ãºtil cuando una celda de rebaja y una celda de cÃ³digo deben aparecer simultÃ¡neamente.\n\nCada una de estas opciones puede incluir cÃ³digo Python o cÃ³digo Markdown/HTML/LaTeX como un Jupyter Notebook tradicional.\n\n\n\nUna vez que se han utilizado las celdas para crear material para la presentaciÃ³n, la presentaciÃ³n se puede ver directamente desde el notebook.\nHay dos opciones para ver la presentaciÃ³n de diapositivas:\n\nUsar el acceso directo OPTION + R shortcut (ALT + R on Windows) para ingresar y salir del modo de presentaciÃ³n desde dentro de la computadora portÃ¡til\nAl hacer clic en el botÃ³n â€œModo de presentaciÃ³nâ€ de la computadora portÃ¡til, esto solo aparecerÃ¡ si ha instalado RISE.\n\n\nDespuÃ©s de ingresar al modo de presentaciÃ³n, deberÃ­a ver una pantalla similar a esta:\n\n\n\n\nSi bien puede ser tentador usar las teclas &lt;- y -&gt; para cambiar las diapositivas en la presentaciÃ³n, esto no funcionarÃ¡ por completo: omitirÃ¡ las celdas marcadas como sub-slides. En su lugar, se debe usar ESPACIO para mover la presentaciÃ³n de diapositivas hacia adelante y MAYÃšS + ESPACIO para mover la presentaciÃ³n de diapositivas hacia atrÃ¡s.\nHay muchos otros atajos de teclado a los que se puede acceder dentro de la presentaciÃ³n haciendo clic en el signo de interrogaciÃ³n (?) en la esquina inferior izquierda.\n\n\n\nUna de las mejores cosas de RISE es que funciona en una sesiÃ³n de Python en vivo, lo que significa que puede editar y ejecutar cÃ³digo mientras se ejecuta la presentaciÃ³n.\n\n\n\n\nPuedes exportar tu presentaciÃ³n desplegando la opciÃ³n: File -&gt; Download as.\n\nNota: Para poder descargar en formato .pdf, necesita tener instalado pandoc."
  },
  {
    "objectID": "posts/2021/2021-08-05-rise.html#referencias",
    "href": "posts/2021/2021-08-05-rise.html#referencias",
    "title": "RISE",
    "section": "",
    "text": "RISE - Documentation\nCreating Interactive Slideshows in Jupyter Notebooks"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html",
    "href": "posts/2021/2021-08-20-fastpages.html",
    "title": "Fastpages",
    "section": "",
    "text": "Fastpages es una plataforma que te permite crear y alojar un blog de forma gratuita, sin anuncios y con muchas funciones Ãºtiles, como:\n\nCree publicaciones que contengan cÃ³digo, salidas de cÃ³digo (que pueden ser interactivas), texto formateado, etc. directamente desde Jupyter Notebooks. Las publicaciones en notebooks admiten funciones como:\n\nLas visualizaciones interactivas realizadas con Altair siguen siendo interactivas.\nOcultar o mostrar la entrada y salida de la celda.\nCeldas de cÃ³digo contraÃ­bles que estÃ¡n abiertas o cerradas de forma predeterminada.\nDefina el tÃ­tulo, el resumen y otros metadatos a travÃ©s de celdas de rebajas especiales\nPosibilidad de agregar enlaces a Colab y GitHub automÃ¡ticamente.\n\nCree publicaciones, incluido el formato y las imÃ¡genes, directamente desde documentos de Microsoft Word.\nCree y edite publicaciones de Markdown completamente en lÃ­nea usando el editor de Markdown incorporado de GitHub.\nInserta tarjetas de Twitter y videos de YouTube.\nCategorizaciÃ³n de publicaciones de blog por etiquetas proporcionadas por el usuario para mayor visibilidad.\n\nEn esta secciÃ³n se enseÃ±arÃ¡ los pasos bÃ¡sicos para poder crear su propio blog con fastpages.\n\n\n\n\n\nÂ¡El proceso de configuraciÃ³n de pÃ¡ginas rÃ¡pidas tambiÃ©n estÃ¡ automatizado con GitHub Actions! Al crear un repositorio a partir de la plantilla de pÃ¡ginas rÃ¡pidas, se abrirÃ¡ automÃ¡ticamente una solicitud de extracciÃ³n (despuÃ©s de ~ 30 segundos) configurando su blog para que pueda comenzar a funcionar. La solicitud de extracciÃ³n automatizada lo recibirÃ¡ con instrucciones como esta: \nÂ¡Todo lo que tienes que hacer es seguir estas instrucciones (en el PR que recibes) y tu nuevo sitio de blogs estarÃ¡ en funcionamiento!\n\nNote: Si tienes dudas con la instalaciÃ³n, te recomiendo ver el siguiente video.\n\n\n\n\n\nEl repositorio de fastpages esta compuesto de la siguiente forma:\nâ”œâ”€â”€ .github\nâ”œâ”€â”€ _action_files\nâ”œâ”€â”€ _fastpages_docs\nâ”œâ”€â”€ images\nâ”œâ”€â”€ _includes\nâ”œâ”€â”€ _layouts\nâ”œâ”€â”€ _notebooks\nâ”œâ”€â”€ _pages\nâ”œâ”€â”€ _plugins\nâ”œâ”€â”€ _posts\nâ”œâ”€â”€ _sass\nâ””â”€â”€ _word\nâ””â”€â”€ .devcontainer.json\nâ””â”€â”€ Makefile\nâ””â”€â”€ index.html\nâ””â”€â”€ Gemfile\nâ””â”€â”€ _config.yml\nâ””â”€â”€ LICENSE\nâ””â”€â”€ docker-compose.yml\nâ””â”€â”€ Gemfile.lock\nâ””â”€â”€ README.md\nâ””â”€â”€ .gitattributes\nâ””â”€â”€ .gitignore\nDe momento nos vamos a centrar en algunos de estos archivos:\n\n_config.yml: es el archivo que funciona como el motor del proyecto. En ente archivo, podemos poner el nombre a nuestro blog, el logo, informaciÃ³n personal (github, linkedin, etc), entre otras cosas.\nindex.html: Corresponde a la primera pÃ¡gina cuando se despliega nuestro blog. por lo que es importante escribir algÃºn mensaje para especificar la motivaciÃ³n de hacer un blog.\n/_notebooks: Lugar donde se deben guardar los notebooks (.ipynb) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.ipynb**.\n/_posts: Lugar donde se deben guardar los archivos markdown (.md) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.md**.\n/_word: Lugar donde se deben guardar los archivos word (.docx) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.docx**.\n\n\nNote: fastpages usa nbdev para impulsar el proceso de conversiÃ³n de Jupyter Notebooks en publicaciones de blog. Cuando guardas un notebook en la carpeta /_notebooks de tu repositorio, GitHub Actions aplica nbdev a esos notebooks automÃ¡ticamente. El mismo proceso ocurre cuando guarda documentos de Word o markdown en el directorio _word o_posts, respectivamente.\n\n\n\n\nEn esta parte, se muestran caracterÃ­sticas especiales que fastpages proporciona para los notebooks. TambiÃ©n puede escribir las publicaciones de su blog con documentos de Word o makdown.\n\n\nLa primera celda de su Jupyter Notebook o markdown contiene informaciÃ³n preliminar. El tema principal son los metadatos que pueden activar/desactivar opciones en su Notebook. Tiene el formato siguiente:\n# Title\n&gt; Awesome summary\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\nTodas las configuraciones anteriores estÃ¡n habilitadas en esta publicaciÃ³n, Â¡para que pueda ver cÃ³mo se ven!\n\nEl campo de resumen (precedido por &gt;) se mostrarÃ¡ debajo de su tÃ­tulo y tambiÃ©n lo utilizarÃ¡n las redes sociales para mostrar la descripciÃ³n de su pÃ¡gina.\ntoc: establecer esto en true generarÃ¡ automÃ¡ticamente una tabla de contenido\nbadges: establecer esto en true mostrarÃ¡ los enlaces de Google Colab y GitHub en la publicaciÃ³n de su blog.\ncomments: establecer esto en true habilitarÃ¡ los comentarios. Consulte estas instrucciones para obtener mÃ¡s detalles.\nautor: esto mostrarÃ¡ los nombres de los autores.\ncategories: permitirÃ¡ que su publicaciÃ³n sea categorizada en una pÃ¡gina de â€œEtiquetasâ€, donde los lectores pueden navegar por su publicaciÃ³n por categorÃ­as.\n\nMarkdown Front Matters tiene un formato similar al de los notebooks. Las diferencias entre los dos se puede ver en el siguiente link.\n\n\n\ncoloque una marca # collapse-hide en la parte superior de cualquier celda si desea ocultar esa celda de forma predeterminada, pero dÃ©le al lector la opciÃ³n de mostrarla:\n\n#hide\n!pip install pandas altair\n\nRequirement already satisfied: pandas in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: altair in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (1.21.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0)\nRequirement already satisfied: jinja2 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.0.1)\nRequirement already satisfied: toolz in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: jsonschema in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jinja2-&gt;altair) (2.0.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (21.2.0)\nRequirement already satisfied: setuptools in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (57.1.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (0.18.0)\nWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\nYou should consider upgrading via the '/home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/bin/python -m pip install --upgrade pip' command.\n\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\ncoloque una marca # collapse-show en la parte superior de cualquier celda si desea mostrar esa celda de forma predeterminada, pero dÃ©le al lector la opciÃ³n de ocultarla:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nSi desea ocultar las celdas por completo (no solo contraerlas), lea estas instrucciones.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n\n\nPuede mostrar tablas de la forma habitual en su blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'IMDB_Rating']].head()\n\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nIMDB_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\n6.1\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\n6.9\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\n6.8\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nNaN\n\n\n4\nSlam\n1087521.0\n1000000.0\n3.4\n\n\n\n\n\n\n\n\n\n\n\nÂ¡Las visualizaciones interactivas realizadas con Altair siguen siendo interactivas!\nDejamos esta celda de abajo sin ocultar para que pueda disfrutar de una vista previa del resaltado de sintaxis en pÃ¡ginas rÃ¡pidas, que utiliza el tema de drÃ¡cula.\n\n#collapse-hide\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)\n\n\n\n\n\n\n\n\n\n\n\nAl escribir Le doy a esta publicaciÃ³n dos :+1:! Se mostrarÃ¡ esto:\nLe doy a esta publicaciÃ³n dos :+1:!\n\n\nPuede incluir imÃ¡genes de rebajas con leyenda (caption) como este:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")\n\nPor supuesto, la leyenda es opcional.\n\n\n\nSi escribe &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 mostrarÃ¡ esto:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20\n\n\n\n\nSi escribe &gt; youtube: https://youtu.be/XfoYk_Z5AkI mostrarÃ¡ esto:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI\n\n\n\n\nPuedes ejecutar algunas cajas con mensajes de nota, informaciÃ³n o warning!. A continuaciÃ³n se muestran algunos ejemplos:\n&gt; Warning: There will be no second warning! &gt; Warning: There will be no second warning!\n&gt; Important: Pay attention! It's important.\n\nImportant: Pay attention! Itâ€™s important.\n\n&gt; Tip: This is my tip.\n\nTip: This is my tip.\n\n&gt; Note: Take note of this.\n\nNote: Take note of this.\n\n\n\n\n\n\nRepositorio: fastpages\nnbdev: notebooks a posts\nFastai: foro"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#introducciÃ³n",
    "href": "posts/2021/2021-08-20-fastpages.html#introducciÃ³n",
    "title": "Fastpages",
    "section": "",
    "text": "Fastpages es una plataforma que te permite crear y alojar un blog de forma gratuita, sin anuncios y con muchas funciones Ãºtiles, como:\n\nCree publicaciones que contengan cÃ³digo, salidas de cÃ³digo (que pueden ser interactivas), texto formateado, etc. directamente desde Jupyter Notebooks. Las publicaciones en notebooks admiten funciones como:\n\nLas visualizaciones interactivas realizadas con Altair siguen siendo interactivas.\nOcultar o mostrar la entrada y salida de la celda.\nCeldas de cÃ³digo contraÃ­bles que estÃ¡n abiertas o cerradas de forma predeterminada.\nDefina el tÃ­tulo, el resumen y otros metadatos a travÃ©s de celdas de rebajas especiales\nPosibilidad de agregar enlaces a Colab y GitHub automÃ¡ticamente.\n\nCree publicaciones, incluido el formato y las imÃ¡genes, directamente desde documentos de Microsoft Word.\nCree y edite publicaciones de Markdown completamente en lÃ­nea usando el editor de Markdown incorporado de GitHub.\nInserta tarjetas de Twitter y videos de YouTube.\nCategorizaciÃ³n de publicaciones de blog por etiquetas proporcionadas por el usuario para mayor visibilidad.\n\nEn esta secciÃ³n se enseÃ±arÃ¡ los pasos bÃ¡sicos para poder crear su propio blog con fastpages."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#primeros-pasos",
    "href": "posts/2021/2021-08-20-fastpages.html#primeros-pasos",
    "title": "Fastpages",
    "section": "",
    "text": "Â¡El proceso de configuraciÃ³n de pÃ¡ginas rÃ¡pidas tambiÃ©n estÃ¡ automatizado con GitHub Actions! Al crear un repositorio a partir de la plantilla de pÃ¡ginas rÃ¡pidas, se abrirÃ¡ automÃ¡ticamente una solicitud de extracciÃ³n (despuÃ©s de ~ 30 segundos) configurando su blog para que pueda comenzar a funcionar. La solicitud de extracciÃ³n automatizada lo recibirÃ¡ con instrucciones como esta: \nÂ¡Todo lo que tienes que hacer es seguir estas instrucciones (en el PR que recibes) y tu nuevo sitio de blogs estarÃ¡ en funcionamiento!\n\nNote: Si tienes dudas con la instalaciÃ³n, te recomiendo ver el siguiente video."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#estructura-de-fastpages",
    "href": "posts/2021/2021-08-20-fastpages.html#estructura-de-fastpages",
    "title": "Fastpages",
    "section": "",
    "text": "El repositorio de fastpages esta compuesto de la siguiente forma:\nâ”œâ”€â”€ .github\nâ”œâ”€â”€ _action_files\nâ”œâ”€â”€ _fastpages_docs\nâ”œâ”€â”€ images\nâ”œâ”€â”€ _includes\nâ”œâ”€â”€ _layouts\nâ”œâ”€â”€ _notebooks\nâ”œâ”€â”€ _pages\nâ”œâ”€â”€ _plugins\nâ”œâ”€â”€ _posts\nâ”œâ”€â”€ _sass\nâ””â”€â”€ _word\nâ””â”€â”€ .devcontainer.json\nâ””â”€â”€ Makefile\nâ””â”€â”€ index.html\nâ””â”€â”€ Gemfile\nâ””â”€â”€ _config.yml\nâ””â”€â”€ LICENSE\nâ””â”€â”€ docker-compose.yml\nâ””â”€â”€ Gemfile.lock\nâ””â”€â”€ README.md\nâ””â”€â”€ .gitattributes\nâ””â”€â”€ .gitignore\nDe momento nos vamos a centrar en algunos de estos archivos:\n\n_config.yml: es el archivo que funciona como el motor del proyecto. En ente archivo, podemos poner el nombre a nuestro blog, el logo, informaciÃ³n personal (github, linkedin, etc), entre otras cosas.\nindex.html: Corresponde a la primera pÃ¡gina cuando se despliega nuestro blog. por lo que es importante escribir algÃºn mensaje para especificar la motivaciÃ³n de hacer un blog.\n/_notebooks: Lugar donde se deben guardar los notebooks (.ipynb) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.ipynb**.\n/_posts: Lugar donde se deben guardar los archivos markdown (.md) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.md**.\n/_word: Lugar donde se deben guardar los archivos word (.docx) con la convenciÃ³n de nomenclatura **YYYY-MM-DD-*.docx**.\n\n\nNote: fastpages usa nbdev para impulsar el proceso de conversiÃ³n de Jupyter Notebooks en publicaciones de blog. Cuando guardas un notebook en la carpeta /_notebooks de tu repositorio, GitHub Actions aplica nbdev a esos notebooks automÃ¡ticamente. El mismo proceso ocurre cuando guarda documentos de Word o markdown en el directorio _word o_posts, respectivamente."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#jupyter-notebooks-y-fastpages",
    "href": "posts/2021/2021-08-20-fastpages.html#jupyter-notebooks-y-fastpages",
    "title": "Fastpages",
    "section": "",
    "text": "En esta parte, se muestran caracterÃ­sticas especiales que fastpages proporciona para los notebooks. TambiÃ©n puede escribir las publicaciones de su blog con documentos de Word o makdown.\n\n\nLa primera celda de su Jupyter Notebook o markdown contiene informaciÃ³n preliminar. El tema principal son los metadatos que pueden activar/desactivar opciones en su Notebook. Tiene el formato siguiente:\n# Title\n&gt; Awesome summary\n\n- toc: true\n- branch: master\n- badges: true\n- comments: true\n- author: Hamel Husain & Jeremy Howard\n- categories: [fastpages, jupyter]\nTodas las configuraciones anteriores estÃ¡n habilitadas en esta publicaciÃ³n, Â¡para que pueda ver cÃ³mo se ven!\n\nEl campo de resumen (precedido por &gt;) se mostrarÃ¡ debajo de su tÃ­tulo y tambiÃ©n lo utilizarÃ¡n las redes sociales para mostrar la descripciÃ³n de su pÃ¡gina.\ntoc: establecer esto en true generarÃ¡ automÃ¡ticamente una tabla de contenido\nbadges: establecer esto en true mostrarÃ¡ los enlaces de Google Colab y GitHub en la publicaciÃ³n de su blog.\ncomments: establecer esto en true habilitarÃ¡ los comentarios. Consulte estas instrucciones para obtener mÃ¡s detalles.\nautor: esto mostrarÃ¡ los nombres de los autores.\ncategories: permitirÃ¡ que su publicaciÃ³n sea categorizada en una pÃ¡gina de â€œEtiquetasâ€, donde los lectores pueden navegar por su publicaciÃ³n por categorÃ­as.\n\nMarkdown Front Matters tiene un formato similar al de los notebooks. Las diferencias entre los dos se puede ver en el siguiente link.\n\n\n\ncoloque una marca # collapse-hide en la parte superior de cualquier celda si desea ocultar esa celda de forma predeterminada, pero dÃ©le al lector la opciÃ³n de mostrarla:\n\n#hide\n!pip install pandas altair\n\nRequirement already satisfied: pandas in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (1.3.1)\nRequirement already satisfied: altair in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (4.1.0)\nRequirement already satisfied: pytz&gt;=2017.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.17.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (1.21.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0)\nRequirement already satisfied: jinja2 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.0.1)\nRequirement already satisfied: toolz in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.11.1)\nRequirement already satisfied: entrypoints in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (0.3)\nRequirement already satisfied: jsonschema in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from altair) (3.2.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jinja2-&gt;altair) (2.0.1)\nRequirement already satisfied: attrs&gt;=17.4.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (21.2.0)\nRequirement already satisfied: setuptools in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (57.1.0)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/lib/python3.8/site-packages (from jsonschema-&gt;altair) (0.18.0)\nWARNING: You are using pip version 21.1.3; however, version 21.2.2 is available.\nYou should consider upgrading via the '/home/fralfaro/.cache/pypoetry/virtualenvs/rise-example-Z8sxMVFk-py3.8/bin/python -m pip install --upgrade pip' command.\n\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\ncoloque una marca # collapse-show en la parte superior de cualquier celda si desea mostrar esa celda de forma predeterminada, pero dÃ©le al lector la opciÃ³n de ocultarla:\n\n#collapse-show\ncars = 'https://vega.github.io/vega-datasets/data/cars.json'\nmovies = 'https://vega.github.io/vega-datasets/data/movies.json'\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\nflights = 'https://vega.github.io/vega-datasets/data/flights-5k.json'\n\nSi desea ocultar las celdas por completo (no solo contraerlas), lea estas instrucciones.\n\n# hide\ndf = pd.read_json(movies) # load movies data\ndf.columns = [x.replace(' ', '_') for x in df.columns.values]\ngenres = df['Major_Genre'].unique() # get unique field values\ngenres = list(filter(lambda d: d is not None, genres)) # filter out None values\ngenres.sort() # sort alphabetically\n\n\n\n\nPuede mostrar tablas de la forma habitual en su blog:\n\n# display table with pandas\ndf[['Title', 'Worldwide_Gross', \n    'Production_Budget', 'IMDB_Rating']].head()\n\n\n\n\n\n\n\n\n\nTitle\nWorldwide_Gross\nProduction_Budget\nIMDB_Rating\n\n\n\n\n0\nThe Land Girls\n146083.0\n8000000.0\n6.1\n\n\n1\nFirst Love, Last Rites\n10876.0\n300000.0\n6.9\n\n\n2\nI Married a Strange Person\n203134.0\n250000.0\n6.8\n\n\n3\nLet's Talk About Sex\n373615.0\n300000.0\nNaN\n\n\n4\nSlam\n1087521.0\n1000000.0\n3.4\n\n\n\n\n\n\n\n\n\n\n\nÂ¡Las visualizaciones interactivas realizadas con Altair siguen siendo interactivas!\nDejamos esta celda de abajo sin ocultar para que pueda disfrutar de una vista previa del resaltado de sintaxis en pÃ¡ginas rÃ¡pidas, que utiliza el tema de drÃ¡cula.\n\n#collapse-hide\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=500,\n    height=400\n)"
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#otras-caracterÃ­sticas",
    "href": "posts/2021/2021-08-20-fastpages.html#otras-caracterÃ­sticas",
    "title": "Fastpages",
    "section": "",
    "text": "Al escribir Le doy a esta publicaciÃ³n dos :+1:! Se mostrarÃ¡ esto:\nLe doy a esta publicaciÃ³n dos :+1:!\n\n\nPuede incluir imÃ¡genes de rebajas con leyenda (caption) como este:\n![](https://www.fast.ai/images/fastai_paper/show_batch.png \"Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/\")\n\nPor supuesto, la leyenda es opcional.\n\n\n\nSi escribe &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 mostrarÃ¡ esto:\n\ntwitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20\n\n\n\n\nSi escribe &gt; youtube: https://youtu.be/XfoYk_Z5AkI mostrarÃ¡ esto:\n\nyoutube: https://youtu.be/XfoYk_Z5AkI\n\n\n\n\nPuedes ejecutar algunas cajas con mensajes de nota, informaciÃ³n o warning!. A continuaciÃ³n se muestran algunos ejemplos:\n&gt; Warning: There will be no second warning! &gt; Warning: There will be no second warning!\n&gt; Important: Pay attention! It's important.\n\nImportant: Pay attention! Itâ€™s important.\n\n&gt; Tip: This is my tip.\n\nTip: This is my tip.\n\n&gt; Note: Take note of this.\n\nNote: Take note of this."
  },
  {
    "objectID": "posts/2021/2021-08-20-fastpages.html#referencias",
    "href": "posts/2021/2021-08-20-fastpages.html#referencias",
    "title": "Fastpages",
    "section": "",
    "text": "Repositorio: fastpages\nnbdev: notebooks a posts\nFastai: foro"
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html",
    "href": "posts/2022/2021-07-15-tdd.html",
    "title": "Test Driven Development",
    "section": "",
    "text": "Esta secciÃ³n busca dar seÃ±ales de cÃ³mo abordar el desarrollo de software para Data Science usando Test Driven Development, una tÃ©cnica ampliamente usada en otros rubros de la programaciÃ³n.\n\n\nEn palabras simples, el desarrollo guiado por pruebas pone las pruebas en el corazÃ³n de nuestro trabajo. En su forma mÃ¡s simple consiste en un proceso iterativo de 3 fases:\n\n\nRed: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla\nGreen: Escribe el cÃ³digo mÃ­nimo necesario para pasar ese test\nRefactor: Refactoriza de ser necesario\n\n\n\n\nA modo de ejemplo, vamos a testear la funciÃ³n paridad, que determina si un nÃºmero natural es par o no.\nLo primero que se debe hacer es crear el test, para ello se ocuparÃ¡ la librerÃ­a pytest.\n\nNota: No es necesario conocer previamente la librerÃ­a pytest para entender el ejemplo.\n\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\nEl test nos dice que si el input es el nÃºmero 2, la funciÃ³n paridad devuelve el output 'par'. CÃ³mo aÃºn no hemos escrito la funciÃ³n, el test fallarÃ¡ (fase red).\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\nAhora, se escribe la funciÃ³n paridad (fase green):\ndef paridad(n:int)-&gt;str:\n    \"\"\"\n    Determina si un numero natural es par o no.\n    \n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\nVolvemos a correr el test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\nHemos cometido un descuido a proposito, no hemos testeado el caso si el nÃºmero fuese impar, por lo cual reescribimos el test (fase refactor)\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\ny corremos nuevamente los test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\nListo, nuestra funciÃ³n paridad ha sido testeado correctamente!.\n\n\n\n\nExisten varias razones por las que uno deberÃ­a usar TDD. Entre ellas podemos encontrar: - Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los lÃ­mites del problema y cÃ³mo podemos resolverlo. Con el tiempo esto ayuda a obtener un diseÃ±o modular y reusable del cÃ³digo. - Escribir tests ayuda la forma en que escribimos cÃ³digo, haciÃ©ndolo mÃ¡s legible a otros. Sin embargo, no es un acto de altruismo, la mayorÃ­a de las veces ese otro es tu futuro yo. - Verifica que el cÃ³digo funciona de la manera que se espera, y lo hace de forma automÃ¡tica. - Te permite realizar refactoring con la certeza de que no has roto nada. - Los tests escritos sirven como documentaciÃ³n para otros desarrolladores. - Es una prÃ¡ctica requerida en metodologÃ­as de desarrollo de software agile.\n\n\n\nEl 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de cÃ³digo hasta las 155k. Estas son parte de sus conclusiones:\n\nTodos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.\n\nComo todo en la vida, nada es gratis:\n\nIncremento del tiempo de desarrollo varÃ­a entre un 15% a 35%.\n\nSin embargo\n\nDesde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenciÃ³n reducidos debido al incremento en calidad.\n\nAdemÃ¡s, es importante escribir tests junto con la implementaciÃ³n en pequeÃ±as iteraciones. George y Williams encontraron que escribir tests despuÃ©s de que la aplicaciÃ³n estÃ¡ mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y ademÃ¡s la aplicaciÃ³n se vuelve menos testeable. Otra conclusiÃ³n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un diseÃ±o mÃ¡s simple.\n\n\n\nNo, pero puedes usarlo casi siempre. El anÃ¡lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaciÃ³n vÃ­a testing.\n\n\n\nAcÃ¡ listamos algunas librerÃ­as de TDD en Python: - unittest: MÃ³dulo dentro de la librerÃ­a estÃ¡ndar de Python. Permite realizar tests unitarios, de integraciÃ³n y end to end. - doctest: Permite realizar test de la documentaciÃ³n del cÃ³digo (ejemplos: Numpy o Pandas). - pytest: LibrerÃ­a de testing ampliamente usada en proyectos nuevos de Python. - nose: LibrerÃ­a que extiende unittest para hacerlo mÃ¡s simple. - coverage: Herramienta para medir la cobertura de cÃ³digo de los proyectos. - tox: Herramienta para facilitar el test de una librerÃ­a en diferentes versiones e intÃ©rpretes de Python. - hypothesis: LibrerÃ­a para escribir tests vÃ­a reglas que ayuda a encontrar casos borde. - behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD.\n\n\n\n\nRealizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.\nGoogle Testing Blog: Poseen varios artÃ­culos sobre cÃ³mo abordar problemas tipo, buenas prÃ¡cticas de diseÃ±o para generar cÃ³digo testeable, entre otros. En particular destaca la serie Testing on the Toilet.\nCualquier artÃ­culo de Martin Fowler sobre testing, empezando por Ã©ste\nDesign Patterns: Los patrones de diseÃ±o de software tienen en consideraciÃ³n que el cÃ³digo sea testeable."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#introducciÃ³n",
    "href": "posts/2022/2021-07-15-tdd.html#introducciÃ³n",
    "title": "Test Driven Development",
    "section": "",
    "text": "Esta secciÃ³n busca dar seÃ±ales de cÃ³mo abordar el desarrollo de software para Data Science usando Test Driven Development, una tÃ©cnica ampliamente usada en otros rubros de la programaciÃ³n.\n\n\nEn palabras simples, el desarrollo guiado por pruebas pone las pruebas en el corazÃ³n de nuestro trabajo. En su forma mÃ¡s simple consiste en un proceso iterativo de 3 fases:\n\n\nRed: Escribe un test que ponga a prueba una nueva funcionalidad y asegurate de que el test falla\nGreen: Escribe el cÃ³digo mÃ­nimo necesario para pasar ese test\nRefactor: Refactoriza de ser necesario\n\n\n\n\nA modo de ejemplo, vamos a testear la funciÃ³n paridad, que determina si un nÃºmero natural es par o no.\nLo primero que se debe hacer es crear el test, para ello se ocuparÃ¡ la librerÃ­a pytest.\n\nNota: No es necesario conocer previamente la librerÃ­a pytest para entender el ejemplo.\n\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\nEl test nos dice que si el input es el nÃºmero 2, la funciÃ³n paridad devuelve el output 'par'. CÃ³mo aÃºn no hemos escrito la funciÃ³n, el test fallarÃ¡ (fase red).\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py F                                              [100%]\n========= 1 failed in 0.14s  ===============================================\nAhora, se escribe la funciÃ³n paridad (fase green):\ndef paridad(n:int)-&gt;str:\n    \"\"\"\n    Determina si un numero natural es par o no.\n    \n    :param n: numero entero\n    :return: 'par' si es el numero es par; 'impar' en otro caso\n    \"\"\"\n    return 'par' if n%2==0 else 'impar'\nVolvemos a correr el test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 1 item                                                                                                                                                                          \n\ntemp/test_funcion.py .                                              [100%]\n========= 1 passed in 0.06s  ===============================================\nHemos cometido un descuido a proposito, no hemos testeado el caso si el nÃºmero fuese impar, por lo cual reescribimos el test (fase refactor)\n@pytest.mark.parametrize(\n    \"number, expected\",\n    [\n        (2, 'par'),\n        (3, 'impar'),\n])\ndef test_paridad(number, expected):\n    assert paridad(number) == expected\ny corremos nuevamente los test:\n========= test session starts ============================================ \nplatform linux -- Python 3.8.10, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /home/fralfaro/PycharmProjects/ds_blog\nplugins: anyio-3.3.0\ncollected 2 items                                                                                                                                                                          \n\ntemp/test_funcion.py ..                                              [100%]\n========= 2 passed in 0.06s  ===============================================\nListo, nuestra funciÃ³n paridad ha sido testeado correctamente!."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#porquÃ©-deberÃ­a-usarlo",
    "href": "posts/2022/2021-07-15-tdd.html#porquÃ©-deberÃ­a-usarlo",
    "title": "Test Driven Development",
    "section": "",
    "text": "Existen varias razones por las que uno deberÃ­a usar TDD. Entre ellas podemos encontrar: - Formular bien nuestros pensamientos mediante la escritura de un test significativo antes de ponernos a solucionar el problema nos ayuda a clarificar los lÃ­mites del problema y cÃ³mo podemos resolverlo. Con el tiempo esto ayuda a obtener un diseÃ±o modular y reusable del cÃ³digo. - Escribir tests ayuda la forma en que escribimos cÃ³digo, haciÃ©ndolo mÃ¡s legible a otros. Sin embargo, no es un acto de altruismo, la mayorÃ­a de las veces ese otro es tu futuro yo. - Verifica que el cÃ³digo funciona de la manera que se espera, y lo hace de forma automÃ¡tica. - Te permite realizar refactoring con la certeza de que no has roto nada. - Los tests escritos sirven como documentaciÃ³n para otros desarrolladores. - Es una prÃ¡ctica requerida en metodologÃ­as de desarrollo de software agile."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#evidencia-empÃ­rica",
    "href": "posts/2022/2021-07-15-tdd.html#evidencia-empÃ­rica",
    "title": "Test Driven Development",
    "section": "",
    "text": "El 2008, Nagappan, Maximilien, Bhat y Williams publicaron el paper llamado Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, en donde estudiaron 4 equipos de trabajo (3 de Microsoft y 1 de IBM), con proyectos que variaban entre las 6000 lineas de cÃ³digo hasta las 155k. Estas son parte de sus conclusiones:\n\nTodos los equipos demostraron una baja considerable en la densidad de defectos: 40% para el equipo de IBM, y entre 60-90% para los equipos de Microsoft.\n\nComo todo en la vida, nada es gratis:\n\nIncremento del tiempo de desarrollo varÃ­a entre un 15% a 35%.\n\nSin embargo\n\nDesde un punto de vista de eficacia este incremento en tiempo de desarrollo se compensa por los costos de mantenciÃ³n reducidos debido al incremento en calidad.\n\nAdemÃ¡s, es importante escribir tests junto con la implementaciÃ³n en pequeÃ±as iteraciones. George y Williams encontraron que escribir tests despuÃ©s de que la aplicaciÃ³n estÃ¡ mas o menos lista hace que se testee menos porque los desarrolladores piensan en menos casos, y ademÃ¡s la aplicaciÃ³n se vuelve menos testeable. Otra conclusiÃ³n interesante del estudio de George y Williams es que un 79% de los desarrolladores experimentaron que el uso de TDD conlleva a un diseÃ±o mÃ¡s simple."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#puedo-usar-tdd-siempre",
    "href": "posts/2022/2021-07-15-tdd.html#puedo-usar-tdd-siempre",
    "title": "Test Driven Development",
    "section": "",
    "text": "No, pero puedes usarlo casi siempre. El anÃ¡lisis exploratorio es un caso en que el uso de TDD no hace sentido. Una vez que tenemos definido el problema a solucionar y un mejor entendimiento del problema podemos aterrizar nuestras ideas a la implementaciÃ³n vÃ­a testing."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#librerÃ­as-disponibles",
    "href": "posts/2022/2021-07-15-tdd.html#librerÃ­as-disponibles",
    "title": "Test Driven Development",
    "section": "",
    "text": "AcÃ¡ listamos algunas librerÃ­as de TDD en Python: - unittest: MÃ³dulo dentro de la librerÃ­a estÃ¡ndar de Python. Permite realizar tests unitarios, de integraciÃ³n y end to end. - doctest: Permite realizar test de la documentaciÃ³n del cÃ³digo (ejemplos: Numpy o Pandas). - pytest: LibrerÃ­a de testing ampliamente usada en proyectos nuevos de Python. - nose: LibrerÃ­a que extiende unittest para hacerlo mÃ¡s simple. - coverage: Herramienta para medir la cobertura de cÃ³digo de los proyectos. - tox: Herramienta para facilitar el test de una librerÃ­a en diferentes versiones e intÃ©rpretes de Python. - hypothesis: LibrerÃ­a para escribir tests vÃ­a reglas que ayuda a encontrar casos borde. - behave: Permite utilizar Behavior Driven Development, un proceso de desarrollo derivado del TDD."
  },
  {
    "objectID": "posts/2022/2021-07-15-tdd.html#referencias",
    "href": "posts/2022/2021-07-15-tdd.html#referencias",
    "title": "Test Driven Development",
    "section": "",
    "text": "Realizing Quality Improvement Through Test Driven Development - Results and Experiences of Four Industrial Teams, es una buena lectura, sobretodo los consejos que dan en las conclusiones.\nGoogle Testing Blog: Poseen varios artÃ­culos sobre cÃ³mo abordar problemas tipo, buenas prÃ¡cticas de diseÃ±o para generar cÃ³digo testeable, entre otros. En particular destaca la serie Testing on the Toilet.\nCualquier artÃ­culo de Martin Fowler sobre testing, empezando por Ã©ste\nDesign Patterns: Los patrones de diseÃ±o de software tienen en consideraciÃ³n que el cÃ³digo sea testeable."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html",
    "href": "posts/2022/2022-10-12-implicit.html",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendaciÃ³n que utiliza la similitud del usuario para hacer recomendaciones de productos.\nEn este tutorial, hablaremos sobre * Â¿QuÃ© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * Â¿CÃ³mo crear una matriz usuario-producto? * Â¿CÃ³mo procesar los datos para el filtrado colaborativo basado en el usuario? * Â¿CÃ³mo identificar usuarios similares? * Â¿CÃ³mo reducir el grupo de elementos? * Â¿CÃ³mo clasificar los artÃ­culos para la recomendaciÃ³n? * Â¿CÃ³mo predecir la puntuaciÃ³n de calificaciÃ³n?\n\nNota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial.\n\n\n\n\nEn primer lugar, comprendamos cÃ³mo funciona el filtrado colaborativo basado en usuarios.\nEl filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposiciÃ³n detrÃ¡s del algoritmo es que a usuarios similares les gustan productos similares.\nEl algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:\n\nEncuentre usuarios similares en funciÃ³n de las interacciones con elementos comunes.\nIdentifique los elementos con una calificaciÃ³n alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de interÃ©s.\nCalcular la puntuaciÃ³n media ponderada de cada elemento.\nClasifique los elementos segÃºn la puntuaciÃ³n y elija los n mejores elementos para recomendar.\n\n\n\n\nimage.png\n\n\nEste grÃ¡fico ilustra cÃ³mo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado. * A la Sra. Blond le gustan las manzanas, las sandÃ­as y las piÃ±as. A la Sra. Black le gusta la sandÃ­a y la piÃ±a. A la Sra. PÃºrpura le gustan las sandÃ­as y las uvas. * Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sandÃ­as como las piÃ±as, consideramos que las sandÃ­as y las piÃ±as son artÃ­culos similares. * Dado que a la Sra. PÃºrpura le gustan las sandÃ­as y aÃºn no ha estado expuesta a la piÃ±a, el sistema de recomendaciÃ³n recomienda la piÃ±a a la Sra. PÃºrpura.\n\n\n\nEn el primer paso, importaremos las bibliotecas de Python pandas, numpy y scipy.stats. Estas tres bibliotecas son para procesamiento de datos y cÃ¡lculos.\nTambiÃ©n importamos seaborn para la visualizaciÃ³n y cosine_similarity para calcular el puntaje de similitud.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n\n\n\nEste tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de pelÃ­culas.\nSeguiremos los pasos a continuaciÃ³n para obtener los conjuntos de datos: 1. Vaya a https://grouplens.org/datasets/movielens/ 2. Descargue el conjunto de datos de 100k con el nombre de archivo â€œml-latest-small.zipâ€ 3. Descomprima â€œml-latest-small.zipâ€ 4. Copie la carpeta â€œml-latest-smallâ€ en la carpeta de su proyecto\nHay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y pelÃ­culas. Ahora vamos a leer en los datos de calificaciÃ³n (ratings.csv).\n\n# Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n\n\n\n\n\n\nHay cuatro columnas en el conjunto de datos de calificaciones:\n\nuserID\nmovieID\nrating\ntimestamp\n\nEl conjunto de datos tiene mÃ¡s de 100 000 registros y no falta ningÃºn dato.\n\n# Get the dataset information\nratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n\n\nLas calificaciones de 100k son de 610 usuarios en 9724 pelÃ­culas. La calificaciÃ³n tiene diez valores Ãºnicos de 0.5 a 5.\n\n# Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n\nThe ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\nA continuaciÃ³n, leamos los datos de las pelÃ­culas para obtener los nombres de las pelÃ­culas (movies.csv).\nEl conjunto de datos de pelÃ­culas tiene: * movieId * title * genres\n\n# Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\n\nUsando movieID como clave coincidente, agregamos informaciÃ³n de la pelÃ­cula al conjunto de datos de calificaciÃ³n y lo llamamos df. Â¡AsÃ­ que ahora tenemos el tÃ­tulo de la pelÃ­cula y la calificaciÃ³n de la pelÃ­cula en el mismo conjunto de datos!\n\n# Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n5\n1\n4.0\n847434962\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n2\n7\n1\n4.5\n1106635946\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n3\n15\n1\n2.5\n1510577970\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n4\n17\n1\n4.5\n1305696483\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n\n\n\n\n\n\n\n\n\nDebemos filtrar las pelÃ­culas y mantener solo aquellas con mÃ¡s de 100 calificaciones para el anÃ¡lisis. Esto es para que el cÃ¡lculo sea manejable por la memoria de Google Colab.\nPara hacerlo, primero agrupamos las pelÃ­culas por tÃ­tulo, contamos el nÃºmero de calificaciones y mantenemos solo las pelÃ­culas con mÃ¡s de 100 calificaciones.\nLas calificaciones promedio de las pelÃ­culas tambiÃ©n se calculan.\nDesde la salida .info(), podemos ver que quedan 134 pelÃ­culas.\n\n# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100]\nagg_ratings_GT100.info()  \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n\n\nVeamos cuÃ¡les son las pelÃ­culas mÃ¡s populares y sus calificaciones.\n\n# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n\n\n\n\n\n\n\n\n\ntitle\nmean_rating\nnumber_of_ratings\n\n\n\n\n3158\nForrest Gump (1994)\n4.164134\n329\n\n\n7593\nShawshank Redemption, The (1994)\n4.429022\n317\n\n\n6865\nPulp Fiction (1994)\n4.197068\n307\n\n\n7680\nSilence of the Lambs, The (1991)\n4.161290\n279\n\n\n5512\nMatrix, The (1999)\n4.192446\n278\n\n\n\n\n\n\n\n\nA continuaciÃ³n, usemos un jointplot para verificar la correlaciÃ³n entre la calificaciÃ³n promedio y el nÃºmero de calificaciones.\nPodemos ver una tendencia ascendente en el diagrama de dispersiÃ³n, que muestra que las pelÃ­culas populares obtienen calificaciones mÃ¡s altas.\nLa distribuciÃ³n de calificaciÃ³n promedio muestra que la mayorÃ­a de las pelÃ­culas en el conjunto de datos tienen una calificaciÃ³n promedio de alrededor de 4.\nEl nÃºmero de distribuciÃ³n de calificaciones muestra que la mayorÃ­a de las pelÃ­culas tienen menos de 150 calificaciones.\n\n# Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n\n\n\n\n\n\n\n\nPara mantener solo las 134 pelÃ­culas con mÃ¡s de 100 calificaciones, debemos unir la pelÃ­cula con el dataframe del nivel de calificaciÃ³n del usuario.\nhow='inner' y on='title' aseguran que solo se incluyan las pelÃ­culas con mÃ¡s de 100 calificaciones.\n\n# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDespuÃ©s de filtrar las pelÃ­culas con mÃ¡s de 100 calificaciones, tenemos 597 usuarios que calificaron 134 pelÃ­culas.\n\n# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n\nThe ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\n\n\n\nTransformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son pelÃ­culas. El valor de la matriz es la calificaciÃ³n de usuario de la pelÃ­cula si hay una calificaciÃ³n. De lo contrario, muestra NaN.\n\n# Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\n5.0\n5.0\nNaN\n4.0\n...\nNaN\nNaN\nNaN\n3.0\nNaN\n5.0\nNaN\nNaN\n5.0\n5.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\n\n\n5\nNaN\n3.0\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows Ã— 134 columns\n\n\n\n\n\n\n\nDado que algunas personas tienden a dar una calificaciÃ³n mÃ¡s alta que otras, normalizamos la calificaciÃ³n extrayendo la calificaciÃ³n promedio de cada usuario.\nDespuÃ©s de la normalizaciÃ³n, las pelÃ­culas con una calificaciÃ³n inferior a la calificaciÃ³n promedio del usuario obtienen un valor negativo y las pelÃ­culas con una calificaciÃ³n superior a la calificaciÃ³n promedio del usuario obtienen un valor positivo.\n\n# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n-0.392857\nNaN\nNaN\n0.607143\n0.607143\nNaN\n-0.392857\n...\nNaN\nNaN\nNaN\n-1.392857\nNaN\n0.607143\nNaN\nNaN\n0.607143\n0.607143\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n0.617647\nNaN\nNaN\nNaN\n1.617647\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n0.617647\nNaN\n\n\n5\nNaN\n-0.461538\n0.538462\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n-1.461538\nNaN\nNaN\nNaN\nNaN\n0.538462\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows Ã— 134 columns\n\n\n\n\n\n\n\nHay diferentes formas de medir las similitudes. La correlaciÃ³n de Pearson y la similitud del coseno son dos mÃ©todos ampliamente utilizados.\nEn este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlaciÃ³n de Pearson.\n\n# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1.000000\nNaN\nNaN\n0.391797\n0.180151\n-0.439941\n-0.029894\n0.464277\n1.0\n-0.037987\n...\n0.091574\n0.254514\n0.101482\n-0.500000\n0.780020\n0.303854\n-0.012077\n0.242309\n-0.175412\n0.071553\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n\n\n5 rows Ã— 597 columns\n\n\n\n\nAquellos que estÃ©n interesados en usar la similitud del coseno pueden consultar este cÃ³digo. Dado que cosine_similarity no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del cÃ¡lculo.\n\n# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n\narray([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])\n\n\nAhora usemos el ID de usuario 1 como ejemplo para ilustrar cÃ³mo encontrar usuarios similares.\nPrimero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el nÃºmero de usuarios similares.\n\n# Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n6\n-0.439941\nNaN\nNaN\n0.421927\n-0.006888\n1.000000\n0.000000\n-0.127385\nNaN\n0.957427\n...\n-0.292770\n-0.030599\n-0.123983\n-0.176327\n0.063861\n-0.468008\n0.541386\n-0.337129\n0.158255\n-0.030567\n\n\n\n\n5 rows Ã— 597 columns\n\n\n\n\nEn la matriz de similitud del usuario, los valores varÃ­an de -1 a 1, donde -1 significa la preferencia de pelÃ­cula opuesta y 1 significa la misma preferencia de pelÃ­cula.\nn = 10 significa que nos gustarÃ­a elegir los 10 usuarios mÃ¡s similares para el ID de usuario 1.\nEl filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. AquÃ­ configuramos user_similarity_threshold en 0,3, lo que significa que un usuario debe tener un coeficiente de correlaciÃ³n de Pearson de al menos 0,3 para ser considerado como un usuario similar.\nDespuÃ©s de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del mÃ¡s alto al mÃ¡s bajo, luego imprimimos la ID de los usuarios mÃ¡s similares y el valor de correlaciÃ³n de Pearson.\n\n# Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n\nThe similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64\n\n\n\n\n\nReduciremos el grupo de artÃ­culos haciendo lo siguiente:\n\nElimine las pelÃ­culas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).\nGuarde solo las pelÃ­culas que otros usuarios similares hayan visto.\n\nPara eliminar las pelÃ­culas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.\n\n# Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n\n\n\n\n\n\n\n\ntitle\nAlien (1979)\nAmerican Beauty (1999)\nAmerican History X (1998)\nApocalypse Now (1979)\nBack to the Future (1985)\nBatman (1989)\nBig Lebowski, The (1998)\nBraveheart (1995)\nClear and Present Danger (1994)\nClerks (1994)\n...\nStar Wars: Episode IV - A New Hope (1977)\nStar Wars: Episode V - The Empire Strikes Back (1980)\nStar Wars: Episode VI - Return of the Jedi (1983)\nStargate (1994)\nTerminator, The (1984)\nToy Story (1995)\nTwister (1996)\nUsual Suspects, The (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n-0.392857\n0.607143\n0.607143\n-0.392857\n0.607143\n-0.392857\n0.607143\n-0.392857\n-0.392857\n-1.392857\n...\n0.607143\n0.607143\n0.607143\n-1.392857\n0.607143\n-0.392857\n-1.392857\n0.607143\n0.607143\n0.607143\n\n\n\n\n1 rows Ã— 56 columns\n\n\n\n\nPara mantener solo las pelÃ­culas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la pelÃ­cula con todos los valores faltantes. Todo valor faltante para una pelÃ­cula significa que ninguno de los usuarios similares ha visto la pelÃ­cula.\n\n# Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAlien (1979)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nBack to the Future (1985)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBraveheart (1995)\n...\nShrek (2001)\nSilence of the Lambs, The (1991)\nSpider-Man (2002)\nStar Wars: Episode I - The Phantom Menace (1999)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nToy Story (1995)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\n0.333333\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\nNaN\n0.466667\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\nNaN\n...\nNaN\nNaN\n0.466667\nNaN\nNaN\n-0.533333\nNaN\nNaN\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\nNaN\n\n\n366\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\nNaN\nNaN\nNaN\nNaN\n0.117647\n0.617647\nNaN\n0.617647\n\n\n502\nNaN\n-0.375\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.153846\n-0.653846\nNaN\nNaN\nNaN\n-0.153846\nNaN\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n0.222222\nNaN\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n-0.333333\nNaN\nNaN\nNaN\nNaN\n0.666667\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\n...\n-2.111111\n-2.611111\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows Ã— 62 columns\n\n\n\n\nA continuaciÃ³n, eliminaremos las pelÃ­culas que el usuario ID 1 vio de la lista de pelÃ­culas de usuarios similares. errors='ignore' elimina columnas si existen sin dar un mensaje de error.\n\n# Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBreakfast Club, The (1985)\nCatch Me If You Can (2002)\nDark Knight, The (2008)\n...\nMonsters, Inc. (2001)\nOcean's Eleven (2001)\nPirates of the Caribbean: The Curse of the Black Pearl (2003)\nShawshank Redemption, The (1994)\nShrek (2001)\nSpider-Man (2002)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nUp (2009)\nWALLÂ·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\n-0.533333\n0.466667\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.466667\nNaN\n-0.533333\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\n\n\n366\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\n0.117647\nNaN\n0.117647\nNaN\nNaN\nNaN\n0.617647\n0.617647\n\n\n502\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.125000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.346154\nNaN\n-1.153846\nNaN\nNaN\n-0.153846\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n-0.277778\n...\nNaN\nNaN\nNaN\n0.222222\nNaN\nNaN\nNaN\nNaN\n0.222222\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\nNaN\nNaN\n...\nNaN\n0.888889\nNaN\nNaN\n-2.111111\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows Ã— 38 columns\n\n\n\n\n\n\n\nDecidiremos quÃ© pelÃ­cula recomendar al usuario objetivo. Los elementos recomendados estÃ¡n determinados por el promedio ponderado del puntaje de similitud del usuario y la calificaciÃ³n de la pelÃ­cula. Las calificaciones de las pelÃ­culas estÃ¡n ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderaciÃ³n.\nEste cÃ³digo recorre los elementos y los usuarios para obtener la puntuaciÃ³n del elemento, clasificar la puntuaciÃ³n de mayor a menor y elegir las 10 mejores pelÃ­culas para recomendar al ID de usuario 1.\n\n# A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n\n\n6\nBourne Identity, The (2002)\n0.888889\n\n\n29\nOcean's Eleven (2001)\n0.888889\n\n\n18\nInception (2010)\n0.587491\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n\n\n5\nBlade Runner (1982)\n0.466667\n\n\n12\nDonnie Darko (2001)\n0.466667\n\n\n10\nDeparted, The (2006)\n0.256727\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n\n\n\n\n\n\n\n\n\n\n\nSi el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificaciÃ³n del usuario, debemos sumar la calificaciÃ³n promedio de la pelÃ­cula del usuario a la calificaciÃ³n de la pelÃ­cula.\n\n# Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n\nThe average movie rating for user 1 is 4.39\n\n\nLa calificaciÃ³n promedio de la pelÃ­cula para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificaciÃ³n de la pelÃ­cula.\n\n# Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\npredicted_rating\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n6.281746\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n6.281746\n\n\n6\nBourne Identity, The (2002)\n0.888889\n5.281746\n\n\n29\nOcean's Eleven (2001)\n0.888889\n5.281746\n\n\n18\nInception (2010)\n0.587491\n4.980348\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n4.859524\n\n\n5\nBlade Runner (1982)\n0.466667\n4.859524\n\n\n12\nDonnie Darko (2001)\n0.466667\n4.859524\n\n\n10\nDeparted, The (2006)\n0.256727\n4.649584\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n4.615423\n\n\n\n\n\n\n\n\nPodemos ver que las 10 mejores pelÃ­culas recomendadas tienen calificaciones pronosticadas superiores a 4.5.\n\n\n\nEn este tutorial, analizamos cÃ³mo crear un sistema de recomendaciÃ³n de filtrado colaborativo basado en el usuario. Aprendiste * Â¿QuÃ© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * Â¿CÃ³mo crear una matriz usuario-producto? * Â¿CÃ³mo procesar los datos para el filtrado colaborativo basado en el usuario? * Â¿CÃ³mo identificar usuarios similares? * Â¿CÃ³mo reducir el grupo de elementos? * Â¿CÃ³mo clasificar los artÃ­culos para la recomendaciÃ³n? * Â¿CÃ³mo predecir la puntuaciÃ³n de calificaciÃ³n?\n\n\n\n\nUser-Based Collaborative Filtering.\nUser-Based Collaborative Filtering In Python | Machine Learning.\nCollaborative Filtering : Data Science Concepts."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#introducciÃ³n",
    "href": "posts/2022/2022-10-12-implicit.html#introducciÃ³n",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Recommendation System: User-Based Collaborative Filtering o Filtrado Colaborativo Basado en el Usuario es un tipo de algoritmo de sistema de recomendaciÃ³n que utiliza la similitud del usuario para hacer recomendaciones de productos.\nEn este tutorial, hablaremos sobre * Â¿QuÃ© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * Â¿CÃ³mo crear una matriz usuario-producto? * Â¿CÃ³mo procesar los datos para el filtrado colaborativo basado en el usuario? * Â¿CÃ³mo identificar usuarios similares? * Â¿CÃ³mo reducir el grupo de elementos? * Â¿CÃ³mo clasificar los artÃ­culos para la recomendaciÃ³n? * Â¿CÃ³mo predecir la puntuaciÃ³n de calificaciÃ³n?\n\nNota: Para entender a profundidad este algoritmo, se recomienda leer el documento oficial."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#algoritmo",
    "href": "posts/2022/2022-10-12-implicit.html#algoritmo",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En primer lugar, comprendamos cÃ³mo funciona el filtrado colaborativo basado en usuarios.\nEl filtrado colaborativo basado en el usuario hace recomendaciones basadas en las interacciones del usuario con el producto en el pasado. La suposiciÃ³n detrÃ¡s del algoritmo es que a usuarios similares les gustan productos similares.\nEl algoritmo de filtrado colaborativo basado en el usuario generalmente tiene los siguientes pasos:\n\nEncuentre usuarios similares en funciÃ³n de las interacciones con elementos comunes.\nIdentifique los elementos con una calificaciÃ³n alta por parte de usuarios similares pero que no han sido expuestos al usuario activo de interÃ©s.\nCalcular la puntuaciÃ³n media ponderada de cada elemento.\nClasifique los elementos segÃºn la puntuaciÃ³n y elija los n mejores elementos para recomendar.\n\n\n\n\nimage.png\n\n\nEste grÃ¡fico ilustra cÃ³mo funciona el filtrado colaborativo basado en elementos mediante un ejemplo simplificado. * A la Sra. Blond le gustan las manzanas, las sandÃ­as y las piÃ±as. A la Sra. Black le gusta la sandÃ­a y la piÃ±a. A la Sra. PÃºrpura le gustan las sandÃ­as y las uvas. * Debido a que a la Sra. Black y la Sra. Purple les gustan tanto las sandÃ­as como las piÃ±as, consideramos que las sandÃ­as y las piÃ±as son artÃ­culos similares. * Dado que a la Sra. PÃºrpura le gustan las sandÃ­as y aÃºn no ha estado expuesta a la piÃ±a, el sistema de recomendaciÃ³n recomienda la piÃ±a a la Sra. PÃºrpura."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#importar-librerÃ­as",
    "href": "posts/2022/2022-10-12-implicit.html#importar-librerÃ­as",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En el primer paso, importaremos las bibliotecas de Python pandas, numpy y scipy.stats. Estas tres bibliotecas son para procesamiento de datos y cÃ¡lculos.\nTambiÃ©n importamos seaborn para la visualizaciÃ³n y cosine_similarity para calcular el puntaje de similitud.\n\n# Data processing\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\n\n# Visualization\nimport seaborn as sns\n\n# Similarity\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport warnings\nwarnings.simplefilter('ignore')"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#descargar-y-leer-datos",
    "href": "posts/2022/2022-10-12-implicit.html#descargar-y-leer-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Este tutorial utiliza el conjunto de datos movielens. Este conjunto de datos contiene calificaciones reales de usuarios de pelÃ­culas.\nSeguiremos los pasos a continuaciÃ³n para obtener los conjuntos de datos: 1. Vaya a https://grouplens.org/datasets/movielens/ 2. Descargue el conjunto de datos de 100k con el nombre de archivo â€œml-latest-small.zipâ€ 3. Descomprima â€œml-latest-small.zipâ€ 4. Copie la carpeta â€œml-latest-smallâ€ en la carpeta de su proyecto\nHay varios conjuntos de datos en la carpeta 100k movielens. Para este tutorial, utilizaremos dos clasificaciones y pelÃ­culas. Ahora vamos a leer en los datos de calificaciÃ³n (ratings.csv).\n\n# Read in data\nratings=pd.read_csv('data/ml-latest-small/ratings.csv')\n\n# Take a look at the data\nratings.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\n\n\n\n\n0\n1\n1\n4.0\n964982703\n\n\n1\n1\n3\n4.0\n964981247\n\n\n2\n1\n6\n4.0\n964982224\n\n\n3\n1\n47\n5.0\n964983815\n\n\n4\n1\n50\n5.0\n964982931\n\n\n\n\n\n\n\n\nHay cuatro columnas en el conjunto de datos de calificaciones:\n\nuserID\nmovieID\nrating\ntimestamp\n\nEl conjunto de datos tiene mÃ¡s de 100 000 registros y no falta ningÃºn dato.\n\n# Get the dataset information\nratings.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 100836 entries, 0 to 100835\nData columns (total 4 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   userId     100836 non-null  int64  \n 1   movieId    100836 non-null  int64  \n 2   rating     100836 non-null  float64\n 3   timestamp  100836 non-null  int64  \ndtypes: float64(1), int64(3)\nmemory usage: 3.1 MB\n\n\nLas calificaciones de 100k son de 610 usuarios en 9724 pelÃ­culas. La calificaciÃ³n tiene diez valores Ãºnicos de 0.5 a 5.\n\n# Number of users\nprint('The ratings dataset has', ratings['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', ratings['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', ratings['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(ratings['rating'].unique()))\n\nThe ratings dataset has 610 unique users\nThe ratings dataset has 9724 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]\n\n\nA continuaciÃ³n, leamos los datos de las pelÃ­culas para obtener los nombres de las pelÃ­culas (movies.csv).\nEl conjunto de datos de pelÃ­culas tiene: * movieId * title * genres\n\n# Read in data\nmovies = pd.read_csv('data/ml-latest-small/movies.csv')\n\n# Take a look at the data\nmovies.head()\n\n\n\n\n\n\n\n\n\nmovieId\ntitle\ngenres\n\n\n\n\n0\n1\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n2\nJumanji (1995)\nAdventure|Children|Fantasy\n\n\n2\n3\nGrumpier Old Men (1995)\nComedy|Romance\n\n\n3\n4\nWaiting to Exhale (1995)\nComedy|Drama|Romance\n\n\n4\n5\nFather of the Bride Part II (1995)\nComedy\n\n\n\n\n\n\n\n\nUsando movieID como clave coincidente, agregamos informaciÃ³n de la pelÃ­cula al conjunto de datos de calificaciÃ³n y lo llamamos df. Â¡AsÃ­ que ahora tenemos el tÃ­tulo de la pelÃ­cula y la calificaciÃ³n de la pelÃ­cula en el mismo conjunto de datos!\n\n# Merge ratings and movies datasets\ndf = pd.merge(ratings, movies, on='movieId', how='inner')\n\n# Take a look at the data\ndf.head()\n\n\n\n\n\n\n\n\n\nuserId\nmovieId\nrating\ntimestamp\ntitle\ngenres\n\n\n\n\n0\n1\n1\n4.0\n964982703\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n1\n5\n1\n4.0\n847434962\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n2\n7\n1\n4.5\n1106635946\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n3\n15\n1\n2.5\n1510577970\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy\n\n\n4\n17\n1\n4.5\n1305696483\nToy Story (1995)\nAdventure|Animation|Children|Comedy|Fantasy"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#anÃ¡lisis-exploratorio-de-datos",
    "href": "posts/2022/2022-10-12-implicit.html#anÃ¡lisis-exploratorio-de-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Debemos filtrar las pelÃ­culas y mantener solo aquellas con mÃ¡s de 100 calificaciones para el anÃ¡lisis. Esto es para que el cÃ¡lculo sea manejable por la memoria de Google Colab.\nPara hacerlo, primero agrupamos las pelÃ­culas por tÃ­tulo, contamos el nÃºmero de calificaciones y mantenemos solo las pelÃ­culas con mÃ¡s de 100 calificaciones.\nLas calificaciones promedio de las pelÃ­culas tambiÃ©n se calculan.\nDesde la salida .info(), podemos ver que quedan 134 pelÃ­culas.\n\n# Aggregate by movie\nagg_ratings = df.groupby('title').agg(mean_rating = ('rating', 'mean'),\n                                                number_of_ratings = ('rating', 'count')).reset_index()\n\n# Keep the movies with over 100 ratings\nagg_ratings_GT100 = agg_ratings[agg_ratings['number_of_ratings']&gt;100]\nagg_ratings_GT100.info()  \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 134 entries, 74 to 9615\nData columns (total 3 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   title              134 non-null    object \n 1   mean_rating        134 non-null    float64\n 2   number_of_ratings  134 non-null    int64  \ndtypes: float64(1), int64(1), object(1)\nmemory usage: 4.2+ KB\n\n\nVeamos cuÃ¡les son las pelÃ­culas mÃ¡s populares y sus calificaciones.\n\n# Check popular movies\nagg_ratings_GT100.sort_values(by='number_of_ratings', ascending=False).head()\n\n\n\n\n\n\n\n\n\ntitle\nmean_rating\nnumber_of_ratings\n\n\n\n\n3158\nForrest Gump (1994)\n4.164134\n329\n\n\n7593\nShawshank Redemption, The (1994)\n4.429022\n317\n\n\n6865\nPulp Fiction (1994)\n4.197068\n307\n\n\n7680\nSilence of the Lambs, The (1991)\n4.161290\n279\n\n\n5512\nMatrix, The (1999)\n4.192446\n278\n\n\n\n\n\n\n\n\nA continuaciÃ³n, usemos un jointplot para verificar la correlaciÃ³n entre la calificaciÃ³n promedio y el nÃºmero de calificaciones.\nPodemos ver una tendencia ascendente en el diagrama de dispersiÃ³n, que muestra que las pelÃ­culas populares obtienen calificaciones mÃ¡s altas.\nLa distribuciÃ³n de calificaciÃ³n promedio muestra que la mayorÃ­a de las pelÃ­culas en el conjunto de datos tienen una calificaciÃ³n promedio de alrededor de 4.\nEl nÃºmero de distribuciÃ³n de calificaciones muestra que la mayorÃ­a de las pelÃ­culas tienen menos de 150 calificaciones.\n\n# Visulization\nsns.jointplot(\n    x='mean_rating',\n    y='number_of_ratings',\n    data=agg_ratings_GT100,\n    color='gray'\n)\n\n\n\n\n\n\n\n\nPara mantener solo las 134 pelÃ­culas con mÃ¡s de 100 calificaciones, debemos unir la pelÃ­cula con el dataframe del nivel de calificaciÃ³n del usuario.\nhow='inner' y on='title' aseguran que solo se incluyan las pelÃ­culas con mÃ¡s de 100 calificaciones.\n\n# Merge data\ndf_GT100 = pd.merge(df, agg_ratings_GT100[['title']], on='title', how='inner')\ndf_GT100.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 19788 entries, 0 to 19787\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   userId     19788 non-null  int64  \n 1   movieId    19788 non-null  int64  \n 2   rating     19788 non-null  float64\n 3   timestamp  19788 non-null  int64  \n 4   title      19788 non-null  object \n 5   genres     19788 non-null  object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 1.1+ MB\n\n\nDespuÃ©s de filtrar las pelÃ­culas con mÃ¡s de 100 calificaciones, tenemos 597 usuarios que calificaron 134 pelÃ­culas.\n\n# Number of users\nprint('The ratings dataset has', df_GT100['userId'].nunique(), 'unique users')\n\n# Number of movies\nprint('The ratings dataset has', df_GT100['movieId'].nunique(), 'unique movies')\n\n# Number of ratings\nprint('The ratings dataset has', df_GT100['rating'].nunique(), 'unique ratings')\n\n# List of unique ratings\nprint('The unique ratings are', sorted(df_GT100['rating'].unique()))\n\nThe ratings dataset has 597 unique users\nThe ratings dataset has 134 unique movies\nThe ratings dataset has 10 unique ratings\nThe unique ratings are [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0]"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#matriz-pelÃ­culas-de-usuario",
    "href": "posts/2022/2022-10-12-implicit.html#matriz-pelÃ­culas-de-usuario",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Transformaremos el conjunto de datos en un formato de matriz. Las filas de la matriz son usuarios y las columnas de la matriz son pelÃ­culas. El valor de la matriz es la calificaciÃ³n de usuario de la pelÃ­cula si hay una calificaciÃ³n. De lo contrario, muestra NaN.\n\n# Create user-item matrix\nmatrix = df_GT100.pivot_table(index='userId', columns='title', values='rating')\nmatrix.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\n5.0\n5.0\nNaN\n4.0\n...\nNaN\nNaN\nNaN\n3.0\nNaN\n5.0\nNaN\nNaN\n5.0\n5.0\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\n5.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n2.0\nNaN\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\n\n\n5\nNaN\n3.0\n4.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n2.0\nNaN\nNaN\nNaN\nNaN\n4.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows Ã— 134 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#normalizaciÃ³n-de-datos",
    "href": "posts/2022/2022-10-12-implicit.html#normalizaciÃ³n-de-datos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Dado que algunas personas tienden a dar una calificaciÃ³n mÃ¡s alta que otras, normalizamos la calificaciÃ³n extrayendo la calificaciÃ³n promedio de cada usuario.\nDespuÃ©s de la normalizaciÃ³n, las pelÃ­culas con una calificaciÃ³n inferior a la calificaciÃ³n promedio del usuario obtienen un valor negativo y las pelÃ­culas con una calificaciÃ³n superior a la calificaciÃ³n promedio del usuario obtienen un valor positivo.\n\n# Normalize user-item matrix\nmatrix_norm = matrix.subtract(matrix.mean(axis=1), axis = 'rows')\nmatrix_norm.head()\n\n\n\n\n\n\n\n\ntitle\n2001: A Space Odyssey (1968)\nAce Ventura: Pet Detective (1994)\nAladdin (1992)\nAlien (1979)\nAliens (1986)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nAmerican Beauty (1999)\nAmerican History X (1998)\nAmerican Pie (1999)\nApocalypse Now (1979)\n...\nTrue Lies (1994)\nTruman Show, The (1998)\nTwelve Monkeys (a.k.a. 12 Monkeys) (1995)\nTwister (1996)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\nWaterworld (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\nNaN\nNaN\nNaN\n-0.392857\nNaN\nNaN\n0.607143\n0.607143\nNaN\n-0.392857\n...\nNaN\nNaN\nNaN\n-1.392857\nNaN\n0.607143\nNaN\nNaN\n0.607143\n0.607143\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\n0.617647\nNaN\nNaN\nNaN\n1.617647\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n0.617647\nNaN\n\n\n5\nNaN\n-0.461538\n0.538462\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n-1.461538\nNaN\nNaN\nNaN\nNaN\n0.538462\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows Ã— 134 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#identifique-usuarios-similares",
    "href": "posts/2022/2022-10-12-implicit.html#identifique-usuarios-similares",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Hay diferentes formas de medir las similitudes. La correlaciÃ³n de Pearson y la similitud del coseno son dos mÃ©todos ampliamente utilizados.\nEn este tutorial, calcularemos la matriz de similitud del usuario utilizando la correlaciÃ³n de Pearson.\n\n# User similarity matrix using Pearson correlation\nuser_similarity = matrix_norm.T.corr()\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n1.000000\nNaN\nNaN\n0.391797\n0.180151\n-0.439941\n-0.029894\n0.464277\n1.0\n-0.037987\n...\n0.091574\n0.254514\n0.101482\n-0.500000\n0.780020\n0.303854\n-0.012077\n0.242309\n-0.175412\n0.071553\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n\n\n5 rows Ã— 597 columns\n\n\n\n\nAquellos que estÃ©n interesados en usar la similitud del coseno pueden consultar este cÃ³digo. Dado que cosine_similarity no toma valores perdidos, necesitamos imputar los valores perdidos con 0 antes del cÃ¡lculo.\n\n# User similarity matrix using cosine similarity\nuser_similarity_cosine = cosine_similarity(matrix_norm.fillna(0))\nuser_similarity_cosine\n\narray([[ 1.        ,  0.        ,  0.        , ...,  0.14893867,\n        -0.06003146,  0.04528224],\n       [ 0.        ,  1.        ,  0.        , ..., -0.04485403,\n        -0.25197632,  0.18886414],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.14893867, -0.04485403,  0.        , ...,  1.        ,\n         0.14734568,  0.07931015],\n       [-0.06003146, -0.25197632,  0.        , ...,  0.14734568,\n         1.        , -0.14276787],\n       [ 0.04528224,  0.18886414,  0.        , ...,  0.07931015,\n        -0.14276787,  1.        ]])\n\n\nAhora usemos el ID de usuario 1 como ejemplo para ilustrar cÃ³mo encontrar usuarios similares.\nPrimero debemos excluir el ID de usuario 1 de la lista de usuarios similares y decidir el nÃºmero de usuarios similares.\n\n# Pick a user ID\npicked_userid = 1\n\n# Remove picked user ID from the candidate list\nuser_similarity.drop(index=picked_userid, inplace=True)\n\n# Take a look at the data\nuser_similarity.head()\n\n\n\n\n\n\n\n\nuserId\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n...\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.000000\n...\n-0.583333\nNaN\n-1.000000\nNaN\nNaN\n0.583333\nNaN\n-0.229416\nNaN\n0.765641\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n0.391797\nNaN\nNaN\n1.000000\n-0.394823\n0.421927\n0.704669\n0.055442\nNaN\n0.360399\n...\n-0.239325\n0.562500\n0.162301\n-0.158114\n0.905134\n0.021898\n-0.020659\n-0.286872\nNaN\n-0.050868\n\n\n5\n0.180151\nNaN\nNaN\n-0.394823\n1.000000\n-0.006888\n0.328889\n0.030168\nNaN\n-0.777714\n...\n0.000000\n0.231642\n0.131108\n0.068621\n-0.245026\n0.377341\n0.228218\n0.263139\n0.384111\n0.040582\n\n\n6\n-0.439941\nNaN\nNaN\n0.421927\n-0.006888\n1.000000\n0.000000\n-0.127385\nNaN\n0.957427\n...\n-0.292770\n-0.030599\n-0.123983\n-0.176327\n0.063861\n-0.468008\n0.541386\n-0.337129\n0.158255\n-0.030567\n\n\n\n\n5 rows Ã— 597 columns\n\n\n\n\nEn la matriz de similitud del usuario, los valores varÃ­an de -1 a 1, donde -1 significa la preferencia de pelÃ­cula opuesta y 1 significa la misma preferencia de pelÃ­cula.\nn = 10 significa que nos gustarÃ­a elegir los 10 usuarios mÃ¡s similares para el ID de usuario 1.\nEl filtrado colaborativo basado en usuarios hace recomendaciones basadas en usuarios con gustos similares, por lo que debemos establecer un umbral positivo. AquÃ­ configuramos user_similarity_threshold en 0,3, lo que significa que un usuario debe tener un coeficiente de correlaciÃ³n de Pearson de al menos 0,3 para ser considerado como un usuario similar.\nDespuÃ©s de establecer la cantidad de usuarios similares y el umbral de similitud, clasificamos el valor de similitud del usuario del mÃ¡s alto al mÃ¡s bajo, luego imprimimos la ID de los usuarios mÃ¡s similares y el valor de correlaciÃ³n de Pearson.\n\n# Number of similar users\nn = 10\n\n# User similarity threashold\nuser_similarity_threshold = 0.3\n\n# Get top n similar users\nsimilar_users = user_similarity[user_similarity[picked_userid]&gt;user_similarity_threshold][picked_userid].sort_values(ascending=False)[:n]\n\n# Print out top n similar users\nprint(f'The similar users for user {picked_userid} are', similar_users)\n\nThe similar users for user 1 are userId\n108    1.000000\n9      1.000000\n550    1.000000\n598    1.000000\n502    1.000000\n401    0.942809\n511    0.925820\n366    0.872872\n154    0.866025\n595    0.866025\nName: 1, dtype: float64"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#restringir-el-grupo-de-artÃ­culos",
    "href": "posts/2022/2022-10-12-implicit.html#restringir-el-grupo-de-artÃ­culos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Reduciremos el grupo de artÃ­culos haciendo lo siguiente:\n\nElimine las pelÃ­culas que ha visto el usuario de destino (ID de usuario 1 en este ejemplo).\nGuarde solo las pelÃ­culas que otros usuarios similares hayan visto.\n\nPara eliminar las pelÃ­culas vistas por el usuario objetivo, mantenemos solo la fila para userId=1 en la matriz de elementos de usuario y eliminamos los elementos con valores faltantes.\n\n# Movies that the target user has watched\npicked_userid_watched = matrix_norm[matrix_norm.index == picked_userid].dropna(axis=1, how='all')\npicked_userid_watched\n\n\n\n\n\n\n\n\ntitle\nAlien (1979)\nAmerican Beauty (1999)\nAmerican History X (1998)\nApocalypse Now (1979)\nBack to the Future (1985)\nBatman (1989)\nBig Lebowski, The (1998)\nBraveheart (1995)\nClear and Present Danger (1994)\nClerks (1994)\n...\nStar Wars: Episode IV - A New Hope (1977)\nStar Wars: Episode V - The Empire Strikes Back (1980)\nStar Wars: Episode VI - Return of the Jedi (1983)\nStargate (1994)\nTerminator, The (1984)\nToy Story (1995)\nTwister (1996)\nUsual Suspects, The (1995)\nWilly Wonka & the Chocolate Factory (1971)\nX-Men (2000)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n-0.392857\n0.607143\n0.607143\n-0.392857\n0.607143\n-0.392857\n0.607143\n-0.392857\n-0.392857\n-1.392857\n...\n0.607143\n0.607143\n0.607143\n-1.392857\n0.607143\n-0.392857\n-1.392857\n0.607143\n0.607143\n0.607143\n\n\n\n\n1 rows Ã— 56 columns\n\n\n\n\nPara mantener solo las pelÃ­culas de los usuarios similares, mantenemos los ID de usuario en las 10 listas de usuarios similares principales y eliminamos la pelÃ­cula con todos los valores faltantes. Todo valor faltante para una pelÃ­cula significa que ninguno de los usuarios similares ha visto la pelÃ­cula.\n\n# Movies that similar users watched. Remove movies that none of the similar users have watched\nsimilar_user_movies = matrix_norm[matrix_norm.index.isin(similar_users.index)].dropna(axis=1, how='all')\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAlien (1979)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nBack to the Future (1985)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBraveheart (1995)\n...\nShrek (2001)\nSilence of the Lambs, The (1991)\nSpider-Man (2002)\nStar Wars: Episode I - The Phantom Menace (1999)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nToy Story (1995)\nUp (2009)\nUsual Suspects, The (1995)\nWALLÂ·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\n0.333333\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\nNaN\n0.466667\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\nNaN\n...\nNaN\nNaN\n0.466667\nNaN\nNaN\n-0.533333\nNaN\nNaN\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\nNaN\n\n\n366\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\nNaN\nNaN\nNaN\nNaN\n0.117647\n0.617647\nNaN\n0.617647\n\n\n502\nNaN\n-0.375\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n-1.153846\n-0.653846\nNaN\nNaN\nNaN\n-0.153846\nNaN\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n0.222222\nNaN\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n-0.333333\nNaN\nNaN\nNaN\nNaN\n0.666667\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\n...\n-2.111111\n-2.611111\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows Ã— 62 columns\n\n\n\n\nA continuaciÃ³n, eliminaremos las pelÃ­culas que el usuario ID 1 vio de la lista de pelÃ­culas de usuarios similares. errors='ignore' elimina columnas si existen sin dar un mensaje de error.\n\n# Remove the watched movie from the movie list\nsimilar_user_movies.drop(picked_userid_watched.columns,axis=1, inplace=True, errors='ignore')\n\n# Take a look at the data\nsimilar_user_movies\n\n\n\n\n\n\n\n\ntitle\nAladdin (1992)\nAmelie (Fabuleux destin d'AmÃ©lie Poulain, Le) (2001)\nBatman Begins (2005)\nBeautiful Mind, A (2001)\nBeauty and the Beast (1991)\nBlade Runner (1982)\nBourne Identity, The (2002)\nBreakfast Club, The (1985)\nCatch Me If You Can (2002)\nDark Knight, The (2008)\n...\nMonsters, Inc. (2001)\nOcean's Eleven (2001)\nPirates of the Caribbean: The Curse of the Black Pearl (2003)\nShawshank Redemption, The (1994)\nShrek (2001)\nSpider-Man (2002)\nTerminator 2: Judgment Day (1991)\nTitanic (1997)\nUp (2009)\nWALLÂ·E (2008)\n\n\nuserId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n108\nNaN\n0.466667\nNaN\n0.466667\nNaN\n0.466667\nNaN\n-0.533333\n0.466667\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n0.466667\nNaN\n-0.533333\nNaN\nNaN\n\n\n154\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.214286\nNaN\n\n\n366\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.205882\n...\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n-0.205882\nNaN\nNaN\nNaN\n\n\n401\n-0.382353\nNaN\nNaN\nNaN\n-0.382353\nNaN\nNaN\nNaN\nNaN\nNaN\n...\n0.117647\nNaN\n0.117647\nNaN\n0.117647\nNaN\nNaN\nNaN\n0.617647\n0.617647\n\n\n502\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.125000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n511\nNaN\n-0.653846\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\n0.346154\nNaN\n-1.153846\nNaN\nNaN\n-0.153846\nNaN\n\n\n550\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n-0.277778\n-0.277778\n...\nNaN\nNaN\nNaN\n0.222222\nNaN\nNaN\nNaN\nNaN\n0.222222\n-0.277778\n\n\n595\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n598\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.888889\nNaN\nNaN\nNaN\n...\nNaN\n0.888889\nNaN\nNaN\n-2.111111\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n10 rows Ã— 38 columns"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#recomendar-artÃ­culos",
    "href": "posts/2022/2022-10-12-implicit.html#recomendar-artÃ­culos",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Decidiremos quÃ© pelÃ­cula recomendar al usuario objetivo. Los elementos recomendados estÃ¡n determinados por el promedio ponderado del puntaje de similitud del usuario y la calificaciÃ³n de la pelÃ­cula. Las calificaciones de las pelÃ­culas estÃ¡n ponderadas por las puntuaciones de similitud, por lo que los usuarios con mayor similitud obtienen una mayor ponderaciÃ³n.\nEste cÃ³digo recorre los elementos y los usuarios para obtener la puntuaciÃ³n del elemento, clasificar la puntuaciÃ³n de mayor a menor y elegir las 10 mejores pelÃ­culas para recomendar al ID de usuario 1.\n\n# A dictionary to store item scores\nitem_score = {}\n\n# Loop through items\nfor i in similar_user_movies.columns:\n  # Get the ratings for movie i\n  movie_rating = similar_user_movies[i]\n  # Create a variable to store the score\n  total = 0\n  # Create a variable to store the number of scores\n  count = 0\n  # Loop through similar users\n  for u in similar_users.index:\n    # If the movie has rating\n    if pd.isna(movie_rating[u]) == False:\n      # Score is the sum of user similarity score multiply by the movie rating\n      score = similar_users[u] * movie_rating[u]\n      # Add the score to the total score for the movie so far\n      total += score\n      # Add 1 to the count\n      count +=1\n  # Get the average score for the item\n  item_score[i] = total / count\n\n# Convert dictionary to pandas dataframe\nitem_score = pd.DataFrame(item_score.items(), columns=['movie', 'movie_score'])\n    \n# Sort the movies by score\nranked_item_score = item_score.sort_values(by='movie_score', ascending=False)\n\n# Select top m movies\nm = 10\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n\n\n6\nBourne Identity, The (2002)\n0.888889\n\n\n29\nOcean's Eleven (2001)\n0.888889\n\n\n18\nInception (2010)\n0.587491\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n\n\n5\nBlade Runner (1982)\n0.466667\n\n\n12\nDonnie Darko (2001)\n0.466667\n\n\n10\nDeparted, The (2006)\n0.256727\n\n\n31\nShawshank Redemption, The (1994)\n0.222566"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#predecir-puntuaciones-opcional",
    "href": "posts/2022/2022-10-12-implicit.html#predecir-puntuaciones-opcional",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "Si el objetivo es elegir los elementos recomendados, basta con tener el rango de los elementos. Sin embargo, si el objetivo es predecir la calificaciÃ³n del usuario, debemos sumar la calificaciÃ³n promedio de la pelÃ­cula del usuario a la calificaciÃ³n de la pelÃ­cula.\n\n# Average rating for the picked user\navg_rating = matrix[matrix.index == picked_userid].T.mean()[picked_userid]\n\n# Print the average movie rating for user 1\nprint(f'The average movie rating for user {picked_userid} is {avg_rating:.2f}')\n\nThe average movie rating for user 1 is 4.39\n\n\nLa calificaciÃ³n promedio de la pelÃ­cula para el usuario 1 es 4.39, por lo que agregamos 4.39 nuevamente a la calificaciÃ³n de la pelÃ­cula.\n\n# Calcuate the predicted rating\nranked_item_score['predicted_rating'] = ranked_item_score['movie_score'] + avg_rating\n\n# Take a look at the data\nranked_item_score.head(m)\n\n\n\n\n\n\n\n\n\nmovie\nmovie_score\npredicted_rating\n\n\n\n\n16\nHarry Potter and the Chamber of Secrets (2002)\n1.888889\n6.281746\n\n\n13\nEternal Sunshine of the Spotless Mind (2004)\n1.888889\n6.281746\n\n\n6\nBourne Identity, The (2002)\n0.888889\n5.281746\n\n\n29\nOcean's Eleven (2001)\n0.888889\n5.281746\n\n\n18\nInception (2010)\n0.587491\n4.980348\n\n\n3\nBeautiful Mind, A (2001)\n0.466667\n4.859524\n\n\n5\nBlade Runner (1982)\n0.466667\n4.859524\n\n\n12\nDonnie Darko (2001)\n0.466667\n4.859524\n\n\n10\nDeparted, The (2006)\n0.256727\n4.649584\n\n\n31\nShawshank Redemption, The (1994)\n0.222566\n4.615423\n\n\n\n\n\n\n\n\nPodemos ver que las 10 mejores pelÃ­culas recomendadas tienen calificaciones pronosticadas superiores a 4.5."
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#resumen",
    "href": "posts/2022/2022-10-12-implicit.html#resumen",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "En este tutorial, analizamos cÃ³mo crear un sistema de recomendaciÃ³n de filtrado colaborativo basado en el usuario. Aprendiste * Â¿QuÃ© es el filtrado colaborativo basado en usuarios (usuario-usuario)? * Â¿CÃ³mo crear una matriz usuario-producto? * Â¿CÃ³mo procesar los datos para el filtrado colaborativo basado en el usuario? * Â¿CÃ³mo identificar usuarios similares? * Â¿CÃ³mo reducir el grupo de elementos? * Â¿CÃ³mo clasificar los artÃ­culos para la recomendaciÃ³n? * Â¿CÃ³mo predecir la puntuaciÃ³n de calificaciÃ³n?"
  },
  {
    "objectID": "posts/2022/2022-10-12-implicit.html#referencias",
    "href": "posts/2022/2022-10-12-implicit.html#referencias",
    "title": "Collaborative Filtering",
    "section": "",
    "text": "User-Based Collaborative Filtering.\nUser-Based Collaborative Filtering In Python | Machine Learning.\nCollaborative Filtering : Data Science Concepts."
  }
]